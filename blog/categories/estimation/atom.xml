<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Estimation | Polyglot]]></title>
  <link href="http://markfussell.emenar.com/blog/categories/estimation/atom.xml" rel="self"/>
  <link href="http://markfussell.emenar.com/"/>
  <updated>2015-12-16T10:36:24-08:00</updated>
  <id>http://markfussell.emenar.com/</id>
  <author>
    <name><![CDATA[Mark Fussell]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[READ: Scaled Agile]]></title>
    <link href="http://markfussell.emenar.com/blog/read-2/"/>
    <updated>2015-11-20T01:00:00-08:00</updated>
    <id>http://markfussell.emenar.com/blog/read-2</id>
    <content type="html"><![CDATA[<p>This is the fourth series describing the ADD: a radically more productive software development and delivery
environment.  This series is on estimation, and the first part is here: <a href="/blog/read-1">ADD: Estimation</a>.</p>

<h2>Scaled Agile</h2>

<p>As mentioned at the end of the first part, the extreme reaction to waterfall was to toss Analysis and Design
away, and go from very-loose requirements into actual implementation.  I believe this was a complete mistake
and has caused these "extreme" agile projects to be much <em>slower</em> and <em>unstable</em> compared to another approach.</p>

<p>The alternative is "Scaled Agile", and by this I mean both:</p>

<ul>
<li>It is better designed for larger projects</li>
<li>It is achieved by scaling the activities of software development projects to be appropriate for the scale and needs of the project

<ul>
<li>Analysis &amp; Design efforts, deliverables, and precision can be smaller or larger</li>
<li>Increments of delivery can be smaller or larger</li>
<li>Allowance for iteration (repeating something to refine it) can be smaller or larger</li>
<li>Overlap of activities can be smaller or larger</li>
</ul>
</li>
</ul>


<p>The second item was tossed out with the "Waterfall", at least by many in the industry.  A Waterfall is an extreme version of Agile (basically
no during-build agility), but going directly from verbal or spread-sheet based requirements is a similarly extreme version of Agile
and is unlikely to be as effective as properly-scaled versions of Agile.</p>

<h3>Retail Aspect / Evant / XP / Scaled Agile</h3>

<p>In 2003, while at Evant, we took an XP-based product (an advanced retail planning system) and worked on scaling the velocity and
agility of that project:</p>

<ul>
<li>To a bigger and non-XP team</li>
<li>Through multiple time-zones, including to India</li>
<li>Outside the development team into Product Management, Technical Sales, Marketing, and Support</li>
<li>Into a bigger portfolio of products that had inter-dependent deliverables</li>
<li>Across the portfolio to other product lines and teams</li>
</ul>


<!--more-->


<p>Evant was eventually acquired and folded into a larger company, so much of the memory of what worked well and not about
this was lost.  Except the person who had the 'APT' charter for Evant is still alive and cognizant.  And maybe a bit
opinionated, but will try to be objective.</p>

<p>Ultimately, I think the scaling "worked in principle" but having a pure XP team shift to Scaled Agile is asking them
to give up an almost religious desire to "Not Analyze", "Not Design", and "Not be Objectively Measured!".  If the
software developers really think they can program without being measured around productivity of delivering value to the
customer, you really have to let them go.  Maybe you have a wizard or two that makes crazy amazing things happen and you
don't bother putting them into the normal project teams, but beyond that, you need to know if your products, projects,
and programs are <em>delivering a good ROI</em> and how you could make them deliver a better ROI.</p>

<p>To do that, you need to measure.  And returning to COSMIC, it is a way to measure the scale of software functionality.
Which isn't the value to the customer of that system, but does reasonable express the complexity in providing the
value to the customer.</p>

<h3>COSMIC Measurement: Analysis and Design</h3>

<p>The COSMIC process is clearly doing Analysis and Design, but in a relatively light and tunable way.  You can control:</p>

<ul>
<li>How much you decompose a system</li>
<li>The detail level and decomposition of the functional users of the system</li>
<li>How detailed you get with "Objects of Interest" and their corresponding "Data Groups"</li>
<li>What scope of the system you are going to measure</li>
<li>The granularity and formality of the functional user requirements (FURs)</li>
<li>Details around the non-FURs (indirect FURs, non-functional requirements, and other constraints)</li>
</ul>


<p>Doing the COSMIC measurement process is basically doing a minimal Analysis and Design on a system, so you have a good
basis for measuring its size and correlating that to development costs.  But it doesn't just support the measurement,
it starts building a model of the user concepts (Object of Interests), their events, the system actions, and the
system components (based on the decomposition level).</p>

<h3>The Hub: Additional (or Not) Analysis and Design</h3>

<p>The COSMIC analysis can be a basis to additional investment in Analysis and Design, but that is optional.  That
is a choice of <em>delivery</em>.  You could have extreme delivery that ignored the COSMIC scope model completely.  Or
you could have extreme delivery that went into Waterfall after the COSMIC model was produced.  Or more sensibly,
you could start working <em>outward</em> from that hub:</p>

<ul>
<li>Does the minimal user concepts start describing a good logical data model?</li>
<li>Does the decomposition work with the implementation frameworks?  Should these be more fully designed or just implemented and noted how they are different.</li>
<li>If there is a lot of data-reading by different functional processes, are these from a single database or many or cached?</li>
</ul>


<p>Note that these 'expansions' are not meant to alter the Hub: The measurement was done, and you can leave it alone
but leverage it for subsequent activities</p>

<h3>The Hub: Drilling down with COSMIC</h3>

<p>Alternatively, as you do analysis and design, you can produce a new granularity and scope that you want to now
measure again.  This could be done only when things seem to have become <em>very</em> different from original expectations,
or consistently to understand development 'hidden requirements' or really 'derived requirements' growth.  The
measurement should still ignore NFRs, but the deeper granularity and the conversion of some NFRs to FURs would
cause a different estimate of 'Joints' because we have a different (more detailed) view of the system.</p>

<h3>Scaled Agile Manifesto</h3>

<p>I believe the Agile Manifesto was reactionary and extreme.  It had good ideas in it, but was missing core premises
that customers and development teams should expect from a good development project:</p>

<p>Manifesto:</p>

<ul>
<li>The Customer is always right about what they want.  The delivery team should always be honest and informative about what they can deliver and how they are doing it.

<ul>
<li>The Customer can change what they want, change the delivery team they use, but they can't interfere within the delivery team</li>
<li>The delivery team should always deliver a functional-requirements-based estimate of a project independent of the technology and other delivery choices, so the customer knows what they are asking for (compared to other projects) and getting (given the delivery approach)

<ul>
<li>Unless the customer clearly says "I don't care", in which case the delivery team can choose whether to do the work for their own internal benefit</li>
</ul>
</li>
</ul>
</li>
<li>Unless it helps a customer figure out what they want, requirement-fulfilling working systems are the only true progress of a project</li>
<li>A customer can desire to go forward to later stages (analysis, design, and implementation) before they are sure what they want

<ul>
<li>The cost of responding to change should be minimized while still supporting maximal ROI and velocity if there are no changes</li>
</ul>
</li>
<li>Processes and tools are to support the Customer and Ddelivery team's needs, and not ends of themselves.

<ul>
<li>A default or customary usage of processes and tools is appropriate, but should be evaluated if there appears to be no need or even clear counter-benefit</li>
</ul>
</li>
</ul>


<p>Principles:</p>

<ul>
<li>Our highest priority is to satisfy our current customers

<ul>
<li>Through techniques that enable the customer to make good decisions, see verifiable progress toward their goals, and determine if they are getting a good ROI</li>
</ul>
</li>
<li>Our second highest priority is to satisfy future customers even better

<ul>
<li>Through evaluation of what went well and poorly, and looking for ways to give even higher satisfaction (whether ROI, predictability, or in other ways) to customers</li>
</ul>
</li>
<li>Both sides should care about each other:

<ul>
<li>A happy development team that cares about the customer is usually better at satisfying the customer</li>
<li>A customer that truly cares about the development team is usually better at inspiring the team to deliver its best</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ADD: Estimation]]></title>
    <link href="http://markfussell.emenar.com/blog/read-1/"/>
    <updated>2015-10-23T01:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/read-1</id>
    <content type="html"><![CDATA[<p>This is the fourth series describing the ADD: a radically more productive software development and delivery
environment.  The first series starts here: <a href="/blog/add-1/">Advanced Development and Delivery</a> and described the core four ingredients
of the ADD.  The second series starts here: <a href="/blog/addstack-1/">ADD Stack</a> and described the application stack and
system components associated with the ADD.  Some of these were Java / Grails specific, and some were more general
components and capabilities.  Together these went through a huge amount of functionality, flexibility, reliability,
and scalability.  Both for the IT infrastructure (whether on EC2 or on bare hardware) and for the applications
on top of that IT infrastructure. The third series starts here: <a href="/blog/add2-1">Advanced Development and Delivery Summary</a>
and gave an executive summary of the ADD.</p>

<p>This series is about project estimation, and is basically <em>orthogonal</em> to the ADD itself.  You can have good
 estimates or bad estimates.  The ADD delivers exceptional productivity.  How much you need to estimate is
 quite independent of productivity.  But by using Estimation and Feedback, you can figure out (a) whether
 to do a project and (b) whether the people you have are up-to-snuff.  If you <em>know</em> you have to do
 a project and you <em>have</em> certain people: don't worry about the estimate.  It is irrelevant.  Just "wing it".
 But if you have choices about either (a) whether to do a project or (b) what people to use or even (c)
 figuring out how good your team is after you do a project, then estimations are the core benchmark to
 give you feedback.</p>

<h2>Estimation is crazy-hard</h2>

<p>The problem with estimating is you can't be good at it unless you have done something very similar before.  But
most workers in an industry are relatively new to it: less than ten years.  And if each project takes a couple
years to complete, you may have only a couple experiences in your first five years of work.  Change technologies,
change roles, or change organization, and your estimates are likely poorly based.</p>

<p>When something is too hard for someone to do easily, you need to decompose it to make it (a) easier and (b) separable.</p>

<!--more-->


<p>Doing two or more pieces sequentially is commonly easier than doing the whole task at one time.  By easier, I don't
mean faster.  It may take more time, especially if there is some interrelationship between the pieces.  But there
is commonly a limit to how much a human brain can do at one time until they are trained.  Over time, the training
in the pieces enables them to be done simultaneously.  Master chefs and similar tend to be trained in all pieces of
  their craft until they can do it all so easily that they can do it simultaneously.</p>

<p>By making the pieces separable, you can have different people work on each of the pieces, and develop the skills
   needed for that piece alone.  Note that this isn't "slicing" a large piece of work into a smaller piece of work which
   requires all the same skills.  That would be like a chef doing part of the dinner service.  Instead it is
   separating the skills needed for the task: one chef "Plates" and just plates, another cooks meats, another cooks vegetables,
   another makes sauces, and so on.</p>

<p>The critical question is then how to decompose?  First we need to define and decompose what we mean by estimation.  And to do
that, we can put estimation into the macro solution-delivery process of software.</p>

<h2>A Framework: READ-OR</h2>

<p>I have been reading and writing software for about 35,000 hours.  Mostly reading, which I believe is the core
to being a great programmer: read great programmers' code and you will learn from it.  It may be hard to find
great code to read, but you can always start with Smalltalk or find some epic system close to the
kinds of systems you build.</p>

<p>Working through possible acronyms for the bigger software development process, I stumbled upon my favorite word: "READ".
All software is developed through this simple approach and acronym:</p>

<ul>
<li>Requirements of Solution – Given the problem, and what is your solution to that problem?  What does that solution require to solve the problem?</li>
<li>Estimation of Size – What is the estimate of the <em>size</em> of the solution?  Note this is independent of the architecture and technology.  It is the functional size estimate and not the delivery estimate</li>
<li>Architecture – What is the architecture and technology you want to use to deliver this solution</li>
<li>Delivery – Now deliver or consider delivering.

<ul>
<li>Estimate the delivery timeline and costs, and decide if the solution is worth doing.  If not, change architecture or change solution.</li>
<li>If this is a waterfall, deliver once.  If incremental, deliver in increments.</li>
</ul>
</li>
</ul>


<p>And bonus pieces</p>

<ul>
<li>Over – Now the project is either done or the process starts over until the solution solves the real problem.

<ul>
<li>This requires "Delta" size estimation.  How much is being added to or modified about a system?</li>
</ul>
</li>
<li>Review – Review the size of what was built (not time, but solution functional size)

<ul>
<li>Determine what you would likely want to improve in the solution, architecture, or delivery</li>
</ul>
</li>
</ul>


<p>So now that we have Esitmation</p>

<h3>Estimation of Solution Size</h3>

<p>The hard part is doing Estimation of Solution Size without getting pulled into the details of architecture and delivery.  We are not
yet good at this as an industry.  We need a body of knowledge and a system to estimate the size of a solution
 independent of the architecture chosen and the team that delivers.  If we have this, we can finally <em>measure</em>
 something useful.  Actually we can measure <em>a lot</em> of useful things.</p>

<ul>
<li>What is the ROI of the solution?  If the solution is really big, there are very few architectures and delivery teams that are going to dramatically reduce the cost</li>
<li>How well do certain architectures do against a particular solution?  For a given delivery team, you can see what they are good at or not.  Going across delivery teams, you may be able to identify <em>better</em> architectures vs. <em>more familiar</em> to the team.

<ul>
<li>What are the differences between the architectures that cause these effects?</li>
<li>Should we change architectures or train more in an architecture?</li>
</ul>
</li>
<li>How well do the delivery teams deliver?  Especially if the solution is about the same size, and the architecture is the same, you should start seeing better and worse results.

<ul>
<li>What are the differences between the teams that cause this effect?</li>
<li>Can we improve the numbers?</li>
</ul>
</li>
</ul>


<p>Estimation of solution size has been attempted with Function Points and more recently with COSMIC Function Points. I
think using the term "point" is not quite right, so I am going to recommend being more creative although just a
simple twist on the word.  I propose 'joint' to capture complexity.  Things with more joints are usually more
complex, and it is relatively linear or even somewhat exponential as you increase the joint count.</p>

<h3>Joints / Moves / COSMIC</h3>

<p>So what should constitute a 'joint'?  With very little effort in evaluating it (will be starting soon), I believe COSMIC
is a valid starting methodology.  So a COSMIC Point is a Joint.  Not much difference, but at least it splits the
  two issues: How big is it (1000 joints)?  How long is it expected to take to build it (10,000 points)?
  Given COSMIC measure data movements, a joint could be a synonym for a 'move', but if we augment the system with
  additions beyond 'moves' as source of size, then the term would become a misnomer.</p>

<p>More information about COSMIC is here: <a href="http://cosmic-sizing.org/">http://cosmic-sizing.org/</a> and elsewhere online.</p>

<h3>Analysis and Design</h3>

<p>When working through COSMIC terminology and examples, it flashed me back a couple decades to a time that we built software very differently:</p>

<ul>
<li>Software was built from requirements through analysis through design and then into implementation</li>
</ul>


<p>This was called a 'Waterfall' model because each stage tended to block doing anything for the next stage until
the stage was 'complete'.  Modern Agile proponents, especially the extremely agile, have basically dropped the stages.
But the stages are useful going both <em>forward</em> (analyse what your system is supposed to do before coding it) and <em>backward</em>
does the analysis seem to plausibly match the requirements?  The analysis and design work was actually not the problem,
it was just the amount of work (if you an analyst, you want to analyze) and the 'blocking' nature of the work that was
a problem.</p>

<p>A great book on this analysis and designe work was Object-Oriented Analysis and Design (1st edition) which was in several different programming
 languages and dealt with realtime, server application, and other types of software systems.  The second generation can
  be found here: <a href="http://www.sphoorthyengg.com/MCAupload/upload/object-oriented-analysis-and-design-with-applications-2nd-edition.pdf">http://www.sphoorthyengg.com/MCAupload/upload/object-oriented-analysis-and-design-with-applications-2nd-edition.pdf</a>
These methods became parts of UML, which is  again commonly used for <em>heavy</em> analysis and <em>heavy</em> design, preventing the
customer from seeing the actual product early on.</p>

<p>The solution isn't to drop the steps, but to scale and time them properly</p>
]]></content>
  </entry>
  
</feed>
