<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ADD | Polyglot]]></title>
  <link href="http://markfussell.emenar.com/blog/categories/add/atom.xml" rel="self"/>
  <link href="http://markfussell.emenar.com/"/>
  <updated>2015-10-01T16:35:24-07:00</updated>
  <id>http://markfussell.emenar.com/</id>
  <author>
    <name><![CDATA[Mark Fussell]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-4]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-4/"/>
    <updated>2015-10-01T03:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-4</id>
    <content type="html"><![CDATA[<p>This is the fourth installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>.  In the previous parts I described the big picture and the Vagrant and EC2 bootstrap.</p>

<h2>Node initialization</h2>

<p>The previous parts described getting Vagrant and EC2 to have an operational node.  For Vagrant it leverages 'host' virtual
disk access to configure and bootstrap itself.  For EC2, it leverages CloudFormation to configure and bootstrap itself.
In both cases the very last thing the node does in the bootstrap is:</p>

<p><code>bash
cd /root/gitrepo/`cat /root/nodeinfo/initgitrepo.txt`
include () { if [[ -f \"$1\" ]]; then source \"$1\"; else echo \"Skipped missing: $1\"; fi }
include it/nodeinit/common/init.sh
</code></p>

<!-- more -->


<p>It is an 'include/source' to make sure it is at the same level as the initial bootstrap script.  For EC2 this affects
logging, so continual sourcing is preferred.  In other cases, the 'source' enables sub-scripts to set values for subsequent
scripts where subshells are more isolated.</p>

<h3>init.sh</h3>

<p>The init script first figures out where it is and sets up some important paths.</p>

<p>```bash</p>

<h1>!/bin/bash</h1>

<h1>===================================================</h1>

<h1>=== Want DIR to be root of the 'nodeinit' directory</h1>

<h1>===================================================</h1>

<p>export DIR="$( cd -P "$( dirname "${BASH_SOURCE[0]}" )" &amp;&amp; pwd )/../"
export RESOURCE=${DIR}/resource
export COMMON=${DIR}/common
```</p>

<h3>cron_1m.sh</h3>

<p>It then gets some AWS resources, sets up a shared 'cron', and so on.  I like a single 'cron' job running every minute
so it is easy to understand what is going on.  This is the 'heartbeat' of the server configuration infrastructure: a
server can want to change any 'minute'.  They look every minute for something that makes them want to change and
then they launch an activity.  The look need to be fast: take about a second or two per 'look' and not cause much load.
But the 'change' does not have to be fast: it could take minutes to reconfigure based on the change.  So while
changing, the 'looking' is disabled.  For example, deploying a new WAR can take a while.  The server stops looking
for new WARs when deploying a WAR.  Then starts looking again when it is back online.</p>

<p>At scale (say 100 servers) with servers all on NTP this one-minute rhythm can cause resource rushing.  To
counter that we need to 'jitter' the servers so they work on a different
second of the minute, or even as much as minutes later at super-scale (1000 servers).
That is done within the cron_1m.sh script after the look has established something needs to be done.</p>

<p>```bash
mkdir -p /root/bin
cp ${RESOURCE}/cron_1m.sh /root/bin/cron_1m.sh
chmod +x /root/bin/cron_1m.sh</p>

<p>cat &lt;<EOS > /var/spool/cron/root
MAILTO=""</p>

<ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li>/root/bin/cron_1m.sh
EOS
```</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>


<h3>More specific initialization</h3>

<p>The above activities are done for any node.  They all need to have heartbeats and some other common resources.
But beyond that, it depends on the type of node and the type of stack what should be put on a particular node.
This is done by simple 'includes' with the 'nodeinfo' that came from the configuration.</p>

<p>```bash</p>

<p>include ${DIR}part/<code>cat /root/nodeinfo/nodepart.txt</code>/init.sh
include ${DIR}stacktype/<code>cat /root/nodeinfo/stacktype.txt</code>/init.sh
include ${DIR}stacktype/<code>cat /root/nodeinfo/stacktype.txt</code>/part/<code>cat /root/nodeinfo/nodepart.txt</code>/init.sh</p>

<p>```</p>

<p>You can see the layout in the initial directory picture</p>

<p><img src="http://markfussell.emenar.com/images/add-2/vag1_20151001b.png" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-3]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-3/"/>
    <updated>2015-10-01T02:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-3</id>
    <content type="html"><![CDATA[<p>This is the third installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>.  In the previous parts I described the big picture and the
first part of the Vagrant bootstrap.</p>

<h2>EC2</h2>

<p>The Vagrant bootstrap occurred through 'bash' files that shaped (put shape information into files) and
the 'init' itself to get access to the repo (repo2) that contains the true configuration.  For EC2
the same thing happens within a CloudFormation template.  The code of the 'init' is almost identical, but because
it is in a JSON file there is a lot of noise as the string gets concatenated together.</p>

<!-- more -->


<h3>Shaping</h3>

<p>```json</p>

<pre><code>"TemplateConstant" : {
  "stacktype" : { "value" : "ControlServer1" },
  "initgitrepo" : { "value" : "repo2_petulant-cyril" },
  "nodepart" : { "value" : "controlnode" }
},
</code></pre>

<p>```</p>

<p>```json</p>

<pre><code>        "files" : {
          "/root/nodeinfo/stacktype.txt" : {
            "content" : { "Fn::Join" : ["", [
              { "Fn::FindInMap" : [ "TemplateConstant", "stacktype", "value" ] },
              ""
            ]]},
            "mode"  : "000700",
            "owner" : "root",
            "group" : "root"
          },
          "/root/nodeinfo/initgitrepo.txt" : {
            "content" : { "Fn::Join" : ["", [
              { "Fn::FindInMap" : [ "TemplateConstant", "initgitrepo", "value" ] },
              ""
            ]]},
            "mode"  : "000700",
            "owner" : "root",
            "group" : "root"
          },
</code></pre>

<p>```</p>

<h3>Init</h3>

<p>```json</p>

<pre><code>    "UserData"       : { "Fn::Base64" : { "Fn::Join" : ["", [
      "#!/bin/bash -v\n",
      "yum update -y aws-cfn-bootstrap\n",

      "# Helper function\n",
      "function error_exit\n",
      "{\n",
      "  /opt/aws/bin/cfn-signal -e 1 -r \"$1\" '", { "Ref" : "WaitHandle" }, "'\n",
      "  exit 1\n",
      "}\n",

      "# Install LAMP packages\n",
      "/opt/aws/bin/cfn-init -s ", { "Ref" : "AWS::StackName" }, " -r PrimaryLaunchConfig ",
      "    --access-key ",  { "Ref" : "HostKeys" },
      "    --secret-key ", {"Fn::GetAtt": ["HostKeys", "SecretAccessKey"]},
      "    --region ", { "Ref" : "AWS::Region" }, " || error_exit 'Failed to run cfn-init'\n",

      "yum -y install git \n",

      "echo 'Fetch s3cmd to get credentials' \n",
      "mkdir /root/download/ \n",
      "pushd /root/download/ \n",
      "git clone git://github.com/s3tools/s3cmd.git \n",
</code></pre>

<p>```</p>

<h3>Launching and Clusters</h3>

<p>A CloudFormation can provision a single server, but it is used more for clusters.  Instead of creating a server, we create
a server definition and then say how many servers we want.  The 'PrimaryServerGroup' defines this:</p>

<p>```json</p>

<pre><code>"PrimaryServerGroup" : {
  "Type" : "AWS::AutoScaling::AutoScalingGroup",
  "Properties" : {
    "Tags": [
      { "Key": "add:stacktype", "Value": { "Fn::FindInMap" : [ "TemplateConstant", "stacktype", "value" ] }, "PropagateAtLaunch" : "true" },
      { "Key": "add:nodepart", "Value": { "Fn::FindInMap" : [ "TemplateConstant", "nodepart", "value" ] }, "PropagateAtLaunch" : "true" }
    ],
    "AvailabilityZones" : { "Fn::GetAZs" : "" },
    "LaunchConfigurationName" : { "Ref" : "PrimaryLaunchConfig" },
    "MinSize" : "1",
    "MaxSize" : "1",
    "DesiredCapacity" : "1"
  }
},
</code></pre>

<p>```</p>

<p>Note that it ends with '1', '1', '1' meaning this is just a single server.  But those numbers can be changed at any time.</p>

<p>Given the DesiredCapacity is '1', if you ever kill a server, a new one will be spun up.</p>

<h3>EC2 Keypairs</h3>

<p>Another difference of the EC2 model is that EC2 holds onto the keypair that is used for logging into it.  So that information
doesn't need to be exposed.  And further, the EC2 version creates a special 'IAM' agent for the machine.</p>

<h3>EC2 Dashboard</h3>

<p>The dashboard shows some standard EC2 properties along with the add:stacktype and add:nodepart.  The nodepart shows the
kind of node it is (say a load-balancer vs. a game server) and is used within the bootstrap to put the right software
onto the machine.  The 'nodepart' and the 'stacktype' are the core DNA switches of a server.  Later we will add in
'federation' which primarily configures the size of the node (e.g. no 'small' in production)</p>

<p><img src="http://markfussell.emenar.com/images/add-3/add3_ec2_cv1.png" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-2]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-2/"/>
    <updated>2015-10-01T01:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-2</id>
    <content type="html"><![CDATA[<p>This is the second installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>, but to summarize: since the 1970s, the most productive way
to develop and deliver software was present in Smalltalk, Lisp, and other languages (Mesa/Cedar at Xerox)
by using a very simple and powerful model.  You take a computer with a fully running environment, you tweak it,
and then you clone that.  This way you: (a) minimize what could go wrong, and (b) maximize what will continue to work.
It is very tangible and very instructive (you have full source for everything that is running).  You tweak other
people's masterpieces until they do what you want, and you learn from their masterpieces to create your own.</p>

<h2>ADD: How Better?</h2>

<p>As described before, ADD has four ingredients:</p>

<ul>
<li>Amazon EC2 <a href="http://aws.amazon.com/">http://aws.amazon.com/</a></li>
<li>Amazon S3  <a href="http://aws.amazon.com/">http://aws.amazon.com/</a></li>
<li>GitHub.com <a href="http://github.com/">http://github.com/</a></li>
<li>HipChat    <a href="http://HipChat.com/">http://HipChat.com/</a></li>
</ul>


<p>And these are hooked together to enable 'Changers', 'Watchders', and 'Machines' to be super-productive.  How
is the ADD <em>more productive</em> than the tweak and clone model?  It is because it solves the core problems
of the clone model:</p>

<ul>
<li>How do we clone to <em>different</em> environments?  Different hardware or configuration changes?</li>
<li>How do we reduce the amount of information we have to clone?</li>
<li>How do we reduce the time it takes to transport the clone?</li>
<li>How do we know what version of the clone is on any machine?</li>
<li>How do we create thousands of clones?</li>
<li>How do we know what is different about the different clones?</li>
<li>...</li>
</ul>


<!-- more -->


<p>The first three ingredients are the most powerful and enables a fantastic improvement to the tweak and clone model.
The last ingredient is mainly for people to be able to enjoy the ADD more easily.  It is like the 'salt-to-taste'
and how much you integrate HipChat in with the rest of the ecosystem is up to your team.  But the more it is integrated,
the more your team will know what is going on, the more easily your team will solve problems/issues, and the more easily
you will onboard new people.</p>

<h2>ADD: GitHub</h2>

<p>GitHub will become your primary resource for <em>everything</em> related to 'information'.  Human notes... go into GitHub
in Markdown format (like this blog).  Meeting notes.  GitHub.  Images to go with Meeting notes.  GitHub.  Your first,
most important, repository will be called 'repo1' and will be all the notes you want the team to see.  No more
arguing about the best Wiki, blogger, file store, etc.  The answer will always be the same.  It is in GitHub.  Because
<em>everything</em> is in GitHub.</p>

<p>Why?  Because it is simple.  It is accessible.  It is powerful.  Keeps history.  Takes almost no space.
It works offline (on an airplane).  And it works with multiple writers.  And if GitHub dies...
you have a complete copy of everything you need to bring up your own 'Git'.  Asking 'Why?' is silly.  First
move to GitHub for all of this, and then ask 'Why?' to everything else.</p>

<h2>ADD: S3 : Annexed Repositories</h2>

<p>Git and GitHub are not good with large binary assets.  They get stored in a notably raw way and just make the
repository huge for no benefit.  So don't store large binary assets in GitHub.  Instead store a reference to
the binary object up in S3.  Retrieve it as needed.  See <a href="/blog/git-about-everything-annex">Annex</a></p>

<h2>ADD: EC2 and Vagrant and GitHub and S3</h2>

<p>Using EC2 and Vagrant with a 'PushMe-PullYou' model (see <a href="/blog/git-about-everything-it-automation-2">PushMePullYou</a>)
solves a host of development, delivery, and operations issues.  The benefits are:</p>

<ul>
<li>Complete version control of machines â€“ both operations and developers' machines (or part of a developers' machines)</li>
<li>A very simple model that enables machines to be provisioned rapidly and to change their state every minute (if needed)</li>
<li>An impressive fan-out of activity</li>
<li>An ability to work offline (say GitHub goes down) or to have complete redundancy (use both GitHub and BitBucket to avoid SPOF)</li>
<li>Inherently no SPOF</li>
<li>Dependent on <em>nothing</em> - Not EC2, Not Vagrant, not GitHub, not S3.  These may be gold standards, but they can all be swapped out</li>
</ul>


<p>This is where the ADD just shoots through the roof.  The ADD uses particular technology to show "How it is done" and
get you doing it right.  But it is not dependent on those technologies.  No Chef.  Unless you want it (and I recommend 'Solo').
No Linux unless you want it.  No Grails or Groovy unless you want it.  Use Google Compute if you want to.  Or even your
own Big Iron.  The ADD is a set of tools and methods that work well together and is most easily seen with the Gold Standard.
But it is beyond them: like a mathematical formula (the Golden Ratio) that can be present in many forms.</p>

<h2>Demo or Die!</h2>

<p>The core demo for this article will walk through bringing up a server on EC2 <em>and</em> Vagrant.  If you are not familiar
with EC2 and Vagrant, please read some of my other articles or meeting notes, or look to the web for resources.</p>

<h3>Vagrant</h3>

<p>The demo in Vagrant is slightly simpler than in EC2 because you are dealing with a machine at a time.  On EC2 you
should be thinking 'Clusters' of machines that work together in 'Federations', and the technology to do that is
more complicated and more EC2-centric.</p>

<p>In Vagrant, you have a 'Box' definition and then an actual virtual instance.  To provision an instance you have
to 'init' it, bring it up, and then configure it.  Except you don't.  As long as the instance knows how to
bootstrap itself.  Demo:</p>

<h4>Vagrantfile</h4>

<p>```ruby
  config.vm.provision "shell", inline: &lt;&lt;-SHELL</p>

<pre><code> su root
 source /vagrant/resource/centos7a_cf2_ControlServer1.sh
 source /vagrant/resource/centos7a_cred_bot1.sh
 source /vagrant/resource/centos7a_boot1.sh
</code></pre>

<p>  SHELL
```</p>

<p>As the last step of the Vagrantfile, the (linux) server does three things:</p>

<ul>
<li>It 'shapes' itself to be a ControlServer</li>
<li>It 'shapes' itself to be 'bot1' for it's credentials</li>
<li>It configures itself with a boot script</li>
</ul>


<p>By the end of the boot script it will be fully alive and running.  Watching for changes to repositories that
indicate it should do something.  You should never have to SSH into the machine... ever.  You can to look around
(like the Magic Schoolbus) but you should treat it like it is a living creature and <em>never</em> touch anything inside
it.  If you have to touch something, fix the 'DNA' (that boot script), kill the server, and launch a new one.</p>

<p>Given this is a Vagrant file on the developer's machine, they can certainly feel free to fiddle with things.  But that is
to learn to <em>understand</em> the server.  Some EC2 servers may even be for 'fiddling'.  But QA and production servers should
never be touched and should only be looked at if they are confusing people (who already understand the fiddling and Vagrant servers).</p>

<p>Both 'ControlServer1.sh' and 'cred_bot1.sh' simply put information into files under '/root/nodeinfo/'.  This is an
amazingly flexible approach that works very simply for Vagrant and EC2.</p>

<h4>ControlServer1.sh</h4>

<p>```bash</p>

<h1>!/bin/bash -v</h1>

<p>export stacktype="ControlServer1"
export initgitrepo="repo2_petulant-cyril"
export nodepart="controlnode"</p>

<p>mkdir /root/nodeinfo
echo $stacktype > /root/nodeinfo/stacktype.txt
echo $initgitrepo > /root/nodeinfo/initgitrepo.txt
echo $nodepart > /root/nodeinfo/nodepart.txt
```</p>

<h4>cred_bot1.sh</h4>

<p>The actual version of this would contain real credential information.  The actual version would be developer-specific and not in version control.</p>

<p>```bash</p>

<h1>!/bin/bash -v</h1>

<p>export access_key="access_key"
export secret_key="secret_key"
export keyname="keyname"</p>

<p>echo $access_key > /root/nodeinfo/access_key.txt
echo $secret_key > /root/nodeinfo/secret_key.txt
echo $keyname > /root/nodeinfo/keyname.txt</p>

<p>cat >> /root/.s3cfg &lt;&lt;EOS
[default]
access_key = $access_key
secret_key = $secret_key</p>

<p>EOS
```</p>

<h4>centos7a_boot1.sh</h4>

<p>This script mirrors most of how EC2 works: we need this machine to be able to checkout a repository from GitHub but
we only have Amazon credentials.  So we put the full credentials into S3 and check them out.  Then we can clone
the 'repo2' provisioning repo and go from there.</p>

<p>```bash</p>

<h1>!/bin/bash -v</h1>

<p>yum -y install git</p>

<p>echo 'Fetch s3cmd to get credentials'
mkdir /root/download/
pushd /root/download/
git clone git://github.com/s3tools/s3cmd.git
cd s3cmd
git checkout a91c40fcd14772fa48297e676c8c6efa1aabc3c0
python --version
python setup.py install
mkdir /root/bin/
mv /root/download/s3cmd /root/bin/s3cmd
popd</p>

<p>echo 'Retrieve SSH keys for Github'
pushd /root/.ssh
/root/bin/s3cmd/s3cmd --config /root/.s3cfg get s3://gapshaklee/it/key/shakbot1key2/<em>
chmod 600 id</em>
cd /root
ssh -v -o StrictHostKeyChecking=no -T git@github.com</p>

<p>mkdir /root/gitrepo
cd /root/gitrepo
git clone git@github.com:shaklee/<code>cat /root/nodeinfo/initgitrepo.txt</code>.git</p>

<p>cd /root/gitrepo/<code>cat /root/nodeinfo/initgitrepo.txt</code>
include () { if [[ -f \"$1\" ]]; then source \"$1\"; else echo \"Skipped missing: $1\"; fi }
include it/nodeinit/common/init.sh</p>

<h1>Zzzzz....</h1>

<p>```</p>

<h4>Directory Layout</h4>

<p>An image of the directory structure is below.  The little-meaning but organized name 'repo2' is augmented with a human suffix 'petulant-cyril'
to make it unique and memorable. 'repo2' is always the first operations repo and 'repo3' is always the first development repo.
The suffix is generated by GitHub or other name generators.</p>

<p>The layout of the directory contains a few things:</p>

<ul>
<li> A 'bin' that contains scripts that can be run within this repository.  The 'deflateAll.sh' script is important enough to be put in the root, but the rest are inside 'bin'.</li>
<li> All things other than the README and deflatAll should be in consistent subdirectories.  The 's3info' is for the annex.  And 'it' is for everything related to being it.  'src' and 'test' are meaningless at the root level and should not be checked in.</li>
<li> You can see the 'node' folders.  A 'node' is a virtual server (Chef and others terminology).  'nodeaws' is for aws related node configuration.  'nodeinit' is common.  'nodevag' is for vagrant.  'resource' contains resourceds in general if under 'it' and for something more specific if lower</li>
<li> folder names are never capitalized or pluralized to avoid inter-operating-system issues.  File names can be any format, but I use augmented CamelCase (with snakes) or snake_case depending on the situation.</li>
<li> You can see the annexed files in the '/it/resource' folder.  They are all '50 bytes'</li>
</ul>


<p><img src="http://markfussell.emenar.com/images/add-2/vag1_20151001b.png" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD)]]></title>
    <link href="http://markfussell.emenar.com/blog/add-1/"/>
    <updated>2015-09-24T01:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-1</id>
    <content type="html"><![CDATA[<p>I have been paid to develop and deliver software since about 1980.  That is 35 years of professional experience.
When I started in 1980 there were a lot of 'old timers' who had been around since 1965 or so.  They were 15 years
ahead of me, and even after finishing college, I had less than ten years to their twenty or so.  This was both
intimidating but also very helpful: after college my main programming language was Smalltalk (ParcPlace, Digitalk, etc.),
which included full source to everything.  So the 'masters' would write masterpieces of code, and I would read them.  And then
try to write my own beautiful things leveraging the masterpieces.  I was late to the party, but could learn quickly.</p>

<p>I also have one other unusual advantage: I do startups.  Lots of startups (<a href="http://SlumsOfPaloAlto.com/">http://SlumsOfPaloAlto.com/</a>).  A total of ten software startups over a period of
a bit more than a decade.  Each of these startups failed for one reason or another, but each one <em>hugely</em> progressed in
how good my development team ran.  Eight, nine, and ten were <em>crazy</em> productive: I would run production servers for the
whole company at the same time that I built out the product.  Alone.  And generally way faster than the product management
team could keep up.  At PeerCase the product team actually asked me to <em>slow down</em> delivery so they could ponder what
they wanted for longer.  I literally went to Disney World during ASH (a medical conference) to prevent myself from
releasing new features I knew they wanted.  I was paid to <em>not work</em> (well, I was contracting at the time, so I stopped the
hourly billing clock, but my project bonus was the same).</p>

<h2>10x Productivity</h2>

<p>Besides doing startups, I also consult for companies.  I try to help them improve their development methods, usually by at least 4x if not 10x.
A lot of times, people don't believe you can improve things to '10x' the productivity of the current team using new
development and delivery techniques.  At one company, the CIO and a number of other executives believed me, but I had to convince
a lot more stakeholders.  So two amigos and I sent me into the trenches.  I started taking projects estimated as two-developers, six-months,
and doing them in one month.  Part time.  That is more than 12x productivity.  Realistically it was likely about 20x the productivity
because the teams tend to miss their estimates (they go over).</p>

<!-- more -->


<p>Then for fun, I was sent into the trenches again but this time had to use <em>some</em> of their development methods.  Still way faster,
but back down to 10x or a bit better.  As the trenches became "less mine" and "more theirs" I would slow down more and more.
Eventually everyone tired of each other and the experiments stopped.</p>

<p>Bizarrely, this company that saw the 20x continues to use the slow, unreliable, method of development.  The workers may have
been scared that the company would only need one in ten of them if they changed to a better method.</p>

<h2>Development Stagnation</h2>

<p>Again, I have been doing this for 35 years.  My skill as a developer has improved over that time, and I now view myself as an '8'
where each increase from '0' represents a doubling of business productivity.  So I am 256 times as productive as a '0', and 16 times
as productive as a '4'.  Stephen Wolfram, Bill Joy, and others are above me, but there are not a lot of people up there anymore.</p>

<p>Getting to be a '4' involves understanding how to program.  Getting to a '6' involves understanding business needs.  But getting to an '8'
involved an incredible / revolutionary change to how software is developed and delivered.  For most of my career, getting to an '8'
was not possible.  Or at least the scale had to be different (i.e. add 50% for each number vs. doubling).</p>

<p>The problem was that software development had made <em>absolutely no progress</em> for 40+ years.  "You lie!" people claim.  "We build software very
differently then we used to!" they say.  Yes, <em>some people</em> have made progress.  But it was people doing it wrong for 40+ years.  The
people who did it right (Xerox PARC, MIT's Lisp group, Xerox El Segundo, Tektronics, etc.) were buzzing along happily with a 4-10x speed
of the rest of the industry.</p>

<p>What is this amazing way to build software?  Well, for 40+ years, the best way to write software was to take a working computer and tweak it.
Then clone that. Voila: you have a new capability on all your computers.  Testing is trivial.  Demoing is trivial.  Fixing
is trivial.  Tweak.  Clone.  Repeat.  Smalltalk, Lisp Machines, Xerox Stars, and so on all used this model.  And they were
blazingly fast to develop on.  And to learn how to develop on.</p>

<p>If you wrote software any way different from that, you were just punishing yourself.  Your software would regress because
you touched too much and broke things.  Your software would take too long to write and be buggy because you wrote it
from scratch instead of tweaking something that worked.  Your software would not do what the business wanted because
you spent months writing it instead of hours tweaking something that was close.  Pain... pain... pain...</p>

<p>When I shifted to Java, I dropped to a '6' from an '8' in Smalltalk.  Lots of developers hated leaving Smalltalk
because of that.  But in my case, I cared about the libraries and people who were moving into Java.  It is not
a great language, but it had a lot of potential.  And Smalltalk imploded when Java was released for free.</p>

<h2>Automated Testing</h2>

<p>So I am in this crappy new language, dealing with jars (usually with source), deploying to containers, building out
linux servers in data centers, and trying to make this whole thing scale to millions of users and international
development teams who like to break my code.  It was fun and hell at the same time.  To avoid my going down to a '4'
because other people broke my code, I leveraged XP practices that were entrenched in startup #3 (Evant / Retail Aspect).
This was one of the first full-bore XP companies with Kent Beck and Rob Mee at the helm of the development team.</p>

<p>XP is very much oriented to "Write tests first".  That is stupid.  You need to figure out what you are doing and
writing tests is not a good way to figure out how to do something.  You <em>do it</em> to figure out how to do it.  But
after you <em>do it</em>, you should write tests to make sure the code still does it tomorrow.  Unfortunately, people
usually forget to write tests after, hence the XP maxim.  I lucked out and a whole bunch of tests existed
when I showed up.  I needed to scale the product to be 1-million times bigger and 1-thousands times faster.
And not break any existing test (unless we changed functionality).</p>

<p>That sold me on automated testing.  It made me slower (say a '5') but it protected me from other people breaking
my working code.  Eventually we used Excel for the automated tests, the business people wrote them directly, and
I was back to a '6'.</p>

<h2>Grails and Opinionated Frameworks</h2>

<p>I became an '8' again when I started using <a href="http://grails.org/">http://grails.org/</a>.  The language (Groovy and Annotated Java)
was now approaching Smalltalk if a bit uglier.  And the automated binding to the database, plugin model,
and other great features in Grails made it so I simply didn't have to worry about a lot of stuff.  I
was getting different benefits from Smalltalk, but they netted out.  And I had the same benefit as
Smalltalk in onboarding others: (1) This is the Grails way, (2) This is my 'tweaks' to the Grails way...
now start asking for features and build them.</p>

<h2>Flex and Angular</h2>

<p>On the client side, I had shifted to using Flex very early on for a company called Winster.  It was a bit
bleading edge at the time, but Flex was very powerful and very productive.  It was basically Smalltalk
on the client.  Eventually Flex/Flash became non-viable because of the iPhone issue, but then Angular
jumped in to replace it.  I tried others (e.g. YUI, Sencha, etc.) and they have pros-and-cons, but Angular
is very good and very Flex-like.</p>

<h2>Advanced Development and Delivery (ADD)</h2>

<p>OK, so above describes a lot of the stack I tend to use, but that isn't where the '256x' comes from.  Above I claimed
I was an '8' but that was on the old '50%' or maybe '70%' more scale.  On the new scale, I am a '6' with the stack above
<em>until</em> you add in the ADD: The Advanced Development and Delivery environment.</p>

<p>The ADD came to me incrementally from Evant, through my own failed attempt at productizing it (Velidom), and fully
germinated with my last software startup: Rumble.  "Amusingly", Velidom was killed by two of the four ingredients,
tried to work around problems with SVN that is replaced by one of the four ingredients, and included as part
of the product one of the four ingredients.  If we had just focused on that <em>one ingredient</em> and not the whole
software factory, Velidom would now be owned by Atlassian.</p>

<p>The four ingredients are:</p>

<ul>
<li>Amazon EC2 <a href="http://aws.amazon.com/">http://aws.amazon.com/</a></li>
<li>Amazon S3  <a href="http://aws.amazon.com/">http://aws.amazon.com/</a></li>
<li>GitHub.com <a href="http://github.com/">http://github.com/</a></li>
<li>HipChat    <a href="http://HipChat.com/">http://HipChat.com/</a></li>
</ul>


<p>These four ingredients, when baked properly together, give individuals and teams 4x or more productivity.  No, you
can not substitute the ingredients.  Not until you understand what they are and how they work together.  Yes, <em>I</em>
can substitute ingredients for the above, but the above are the current gold standard.  And hosting your own version
of any of these products is also a 'substitution' and not equivalent to using them as a service.</p>

<h3>Four Ingredients, Three Roles</h3>

<p><img width="432" height="414" src="http://markfussell.emenar.com/images/add-1/ADD_FourIngredients_ThreeRoles_mlf1a1.png" /></p>

<p>For more details or questions, please contact me.</p>
]]></content>
  </entry>
  
</feed>
