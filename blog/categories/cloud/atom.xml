<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Cloud | Polyglot]]></title>
  <link href="http://markfussell.emenar.com/blog/categories/cloud/atom.xml" rel="self"/>
  <link href="http://markfussell.emenar.com/"/>
  <updated>2015-10-14T09:01:27-07:00</updated>
  <id>http://markfussell.emenar.com/</id>
  <author>
    <name><![CDATA[Mark Fussell]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-11]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-11/"/>
    <updated>2015-10-14T01:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-11</id>
    <content type="html"><![CDATA[<p>This is the elevent installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>.  In the previous parts I described the big picture, Vagrant and EC2,
node initialization, the one minute configuration HeartBeat, HipChat integration, presence, EC2, configuring nodes, inter-machine presence,
and configuring an application server and its' stack.</p>

<h2>It's Alive!</h2>

<p>We are close to having a viable semi-production environment running the latest version of our code, and with an incredible
capability of configuring, controlling, and being informed of that environment.  With the simple line:</p>

<p>```bash
mkdir -p /root/app/
cd /root/app</p>

<p>git clone git@github.com:grails-samples/grails-petclinic.git
cd grails-petclinic
./gradlew run
```</p>

<p>We got:</p>

<p><img src="http://markfussell.emenar.com/images/add-10/add10_grails2.png" /></p>

<!-- more -->


<p>Which is almost what we want... except.</p>

<ul>
<li> We need that IP address to be a stable domain name</li>
<li> We need the 'clone' to be augmented with subsequent 'git pull' in case there are subsequent updates</li>
</ul>


<p>In reverse order</p>

<h2>Pulling the Application</h2>

<p>Going back to the one-minute heartbeat, we had this 'cron' job:</p>

<p>```bash
mkdir -p /root/bin
cp ${RESOURCE}/cron_1m.sh /root/bin/cron_1m.sh
chmod +x /root/bin/cron_1m.sh</p>

<p>cat &lt;<EOS > /var/spool/cron/root
MAILTO=""</p>

<ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li>/root/bin/cron_1m.sh
EOS
```</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>Where part of the cron was described as:</p>

<p>```bash</p>

<pre><code>        bash_ifexist bin/nodework/common/work.sh
        bash_ifexist bin/nodework/part/`cat /root/nodeinfo/nodepart.txt`/work.sh
        bash_ifexist bin/nodework/stacktype/`cat /root/nodeinfo/stacktype.txt`/work.sh
        bash_ifexist bin/nodework/stacktype/`cat /root/nodeinfo/stacktype.txt`/part/`cat /root/nodeinfo/nodepart.txt`/work.sh
</code></pre>

<p>```</p>

<p>That is both too much code and too little to understand what is going on.  It is too much because we don't need to put anything but
the first line in the 'cron_1b.sh'.  It is "fine" to do so, but if we add some new concept we have to rebuild machines (or
dynamically copy in new 'cron_1m.sh') vs. the simpler option of just having the first 'work' cal the rest of the 'work'.</p>

<p>The part that is too little is the part where a bunch of git repositories are getting updated.  The full 'cron_1m.sh' looks
like this:</p>

<h4>cron_1m.sh</h4>

<p>```bash</p>

<h1>! /bin/bash</h1>

<h1>================================</h1>

<h1>=== Simple worker example</h1>

<h1>================================</h1>

<p>export ME=<code>basename $0</code>
export LOG=/root/log/${ME}<em>log.txt
export ERROR=/root/log/${ME}</em>error.txt
export START_TSS="<code>date +%Y%m%d-%H%M%S</code>"</p>

<p>export CURRENT_ACTION_FILE=./.temp/nodeinfo/currentaction.txt</p>

<p>bash_ifexist () { if [[ -f "$1" ]]; then bash "$1"; else echo "Skipped missing: $1"; fi }</p>

<p>mkdir -p /root/log/</p>

<p>exec 1>> ${LOG}
exec 2>> ${ERROR}</p>

<p>echo "${ME}: Start  ${START_TSS}" >> ${LOG}</p>

<p>export REPOS=<code>find /root/gitrepo/ -maxdepth 1 -mindepth 1</code>
for REPO in ${REPOS}; do</p>

<pre><code>pushd ${REPO}
    git pull
popd
</code></pre>

<p>done</p>

<p>for REPO in ${REPOS}; do</p>

<pre><code>pushd ${REPO}
    if [[ -e ${CURRENT_ACTION_FILE} ]]; then
        : #Don't do anything until the current action completes
    else
        bash_ifexist bin/nodework/common/work.sh
    fi
popd
</code></pre>

<p>done</p>

<p>export FINISH_TSS="<code>date +%Y%m%d-%H%M%S</code>"
echo "${ME}: Finish ${FINISH_TSS}" >> ${LOG}
```</p>

<p>The reason for the double loop is to make sure inter-repository interactions are all "at the same time".  After the set
of 'pulls', everything is stable.  We could even slow down the tempo of the system by simply pulling less often and
could jitter before the set of 'pulls' which would jitter the whole system.</p>

<h3>Where to work?</h3>

<p>So if we have our 'IT' repo as 'repo2_petulant-cyril' and our application repo as 'repo3_miniature-ironman', where should
we do our 'work.sh'?  The answer is unfortunatly simple: "where ever you want".  The more you do work in 'repo2' the
more 'functional' your system.  Like classic functional / imperative programming.  The more you do work in 'repo3' the
more 'object-oriented' your system.  The more 'repo3' is alive vs. being acted-upon.  On the other hand, the more you
do work in 'repo2', the more you can leverage similarities in the work (like knowing directory structures, git versions,
etc.) and the less 'repo3' is complicated by these things.  Ultimately it would be ideal to be 'aspect-oriented' and
add a 'trait' to 'repo3'.  But for the moment, I will keep things simple and do all the work in 'repo2' where depending
on the kind of node you are, you <em>know about</em> 'repo3' vs. 'repo3' knowing it could be alive.</p>

<h3>'repo3:miniature-ironman'</h3>

<p>Why does 'repo3' have two names?  The first is a simple identifier: it is a 'repo' and it is the third of its kind.
This simplicity makes sure things like 'find' produce a simple in-order answer.  If we have a lot of repositories, we
might start with '101' to make sure sorting works for a few hundred repositories.  The second part is a human-memorable
name, and to help associate the repository with its purpose.  You shouldn't name a repository <em>after</em> its purpose, because
it's purpose could change.  Outright change or simply grow.  Or another repo could have the same purpose and now we
can't figure out which is right.  It would be like naming a kid 'toddler'.  At first it might
not even be a toddler yet, but more importantly, eventually it will clearly no longer be a toddler.  And we would have
  a lot of 'toddler's so we can't figure out which is what.</p>

<p>So instead, each repo just has a unique name.  We have to look at it and interact with it to figure out what it is.  Or
each has a README that we can try to keep current.</p>

<h3>app1/work.sh</h3>

<p>We can fetch 'repo3' from 'repo2' within the work of an application server:</p>

<p>```bash</p>

<h1>====================================================</h1>

<h1>=== lad1/app1</h1>

<h1>====================================================</h1>

<p>echo "Doing App1 work"</p>

<h1>====================================================</h1>

<h1>=== checkout the application repository</h1>

<h1>=== if it doesn't exist</h1>

<h1>====================================================</h1>

<p>export APP_REPO=repo3_miniature-ironman
export APP_NAME=app2
export APP_PATH=$REPO_ROOT/$APP_NAME</p>

<p>if [ ! -d "$APP_PATH" ]; then
  echo "git clone git@github.com:shaklee/${APP_REPO}.git $APP_PATH"
  git clone git@github.com:shaklee/${APP_REPO}.git $APP_PATH
fi</p>

<p>```</p>

<p>The rest of the script looks identical to our main 'work.sh' except our directory has switched from 'common' to
something more specific.</p>

<p>```bash</p>

<h1>====================================================</h1>

<h1>=== Temporary files location</h1>

<h1>=== Override the 'TEMP' directory to be for this part</h1>

<h1>====================================================</h1>

<p>export ADD_TEMP=${GIT_ROOT}/.temp/add/lad1_app1b/
export MY_DIR="$( cd -P "$( dirname "${BASH_SOURCE[0]}" )" &amp;&amp; pwd )"</p>

<p>```</p>

<p>And we have to push/pop to get in the right directory for the 'GIT' commands to work.</p>

<p>```bash</p>

<pre><code>pushd $APP_PATH
export GIT_VERSION=`git rev-parse HEAD`
export DETECT_GIT_CHANGE=`git log --pretty=oneline ${PREV_WORK_VERSION}..  -- ${WORK_WATCH_VALUE} | awk '{print $1}'`
popd
</code></pre>

<p>```</p>

<h3>Start / restart</h3>

<p>Ultimately we will run from a Tomcat or similar container, but currently we just have to:</p>

<p><code>bash
pushd $APP_PATH
./gradlew run &amp;
popd
</code></p>

<p>Except for a couple issues</p>

<ul>
<li>We would like the error and standard output to go somewhere in case something goes wrong</li>
<li>We need to not collide with an previous 'run' because they hold onto the same port</li>
</ul>


<p>Solving these is pretty trivial if we are allowed to be 'brutal' and accept downtime.  Ultimately this won't be
a problem because we will first 'shutdown' and let the load balancer pull us out of rotation before we actually
upgrade versions.</p>

<h2>DNS Stability</h2>

<p>The DNS stability is a trivial thing on EC2 and basically any modern DNS system.  Trivial as long as we understand
one fatal flaw:</p>

<ul>
<li>Everyone caches</li>
</ul>


<p>We can have stability so long as we accept that everyone is trying to cache the DNS to IP address for as long as
possible.  This can be minutes or even longer <em>in spite</em> of our DNS records saying otherwise.  This is actually
 one of the reason load balancers are so important.  Load balancing is a very simple concept.  Your load balancers
 should be simple enough to 'keep running' and be rarely flipped.  So their IP addresses will be stable: like the person
 who is always hanging out on their front porch.  You can always find them there and ask them to ask someone else
 a question.  They will take care of the rest.</p>

<p>So ultimately we don't want the application server obtaining a domain-name, we want it to register with a load
balancer.  But for the moment (and for some useful internal capabilities) we will have each application server
take-a-name.</p>

<h3>How to take a name?</h3>

<p>Before the cloud, the common ways to take a name were:</p>

<ul>
<li>To assign a name via the console of the domain name server</li>
<li>To dynamically attach an IP address to a name via various protocols</li>
</ul>


<p>Pre-cloud, these work fairly well.  If you DN server is smart enough, it can take failed servers out of rotation
with a health check.  And even early versions of cloud computing used a similar model where servers took
possession of a pre-allocated IP address that the DN server knew about.  But these models don't scale as
well as a much simpler model.  Route-53.</p>

<p>Route-53 takes the 'dynamically' to a whole new level.  You can dynamically add / change / delete most anything
and the common latency for the change to take affect is very short.  The rest of the world <em>still caches</em> but
at least you know the DN servers are up to date.  In the following, you can see the 'gaps2c' project I created
a while ago as a demonstration of this ability.  The main domain registry for 'gaps2c.com' is elsewhere, but
the subdomain of 'aws.gaps2c.com' is managed by route53.  The last entry works and says &lt;google.aws.gaps2c.com>
knows where the google servers are.</p>

<p><img src="http://markfussell.emenar.com/images/add-11/add11_route53_1.png" /></p>

<h3>Registering with Route-53</h3>

<p>There are a lot of ways to register with Route-53:</p>

<ul>
<li> Via the console</li>
<li> Via an XML-based payload update</li>
<li> Via settings in a CloudFormation</li>
</ul>


<p>The first is easy, but not scalable.  The second is not hard, but a bit 'peculiar' in how the payload works.  The
third is trivial and scalable.</p>

<p>So lets start with the third</p>

<h4>Registering via a CloudFormation template</h4>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-10]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-10/"/>
    <updated>2015-10-10T02:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-10</id>
    <content type="html"><![CDATA[<p>This is the tenth installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>.  In the previous parts I described the big picture, Vagrant and EC2,
node initialization, the one minute configuration HeartBeat, HipChat integration, presence, EC2, configuring nodes, and inter-machine presence.</p>

<h2>A good, opinionated, framework</h2>

<p>Back in 1972, Smalltalk became the first Object-Oriented Programming Language (Simula was Object-Based but there is a difference).
For decades this kind of language was 'esoteric'.  It was like LISP or Prolog or APL: somehow exotic and inaccessible.
I was lucky: I had access to Smalltalk at Caltech.  I had access to lots of crazy expensive things at Caltech and that
made the exotic (e.g. making your own chips) into the mundane (e.g. made lots of chips, they were commonly broken,
had to be re-fabricated, and I eventually got bored with all that and moved a level up).</p>

<!-- more -->


<p>But back to the language of Smalltalk.  The problem with Smalltalk is that it appears to be a language when it is
actually a computer. 'C' was a language.  It made programs. 'Pascal', 'Lisp' (sans Machine), 'Fortan', and so on...
they were all languages.  Smalltalk <em>contains</em> a language.  It is named Smalltalk (darn).  But Smalltalk-80 was not
<em>just</em> a language, it was an <em>entire running operating system with applications and full source</em>  It could boot on
most any machine that you made the 'bootstrap' code work on.  To make a new Smalltalk-80 machine, you cloned either
the primordial Smalltalk-80 'image' from PARC, or you cloned your own modified 'image'.  And by 'image' this is
basically the same concept as cloning a disk... bit by bit identical copy that happens to be on a different disk / computer.</p>

<p>Eventually OOP became mainstream with Java, C++, Objective-C, Ruby, Python, and the like.  So people thought they
were getting the "Smalltalk" (or LISP Machine) benefits.  But they left out the 'computer' that went with the language.</p>

<h3>Why opinionated?</h3>

<p>The Smalltalk computer was quite functional and thoroughly opinionated.  It <em>already did</em> a bunch of things and showed
you how it did them.  It wasn't opinionated like a human usually uses the term: "You should build that house out of bricks, not straw".
It was opinionated like the planet is: "I have already created lots of flora and fauna... please use them wisely".  Even
how humans on the planet are: "We have already created plenty of roads... please use them instead of driving through yards"</p>

<p>Opinionated is basically a synonym for "Working".  Smalltalk computers "worked" so don't break it.  They work, so you
should probably copy them for anything similar.  And they work, so you might want to study how they work even if you
are going to be creative later.</p>

<h3>Modern 'working' frameworks</h3>

<p>Early frameworks (say for Java) 'kind-of-worked'.  They didn't fully work, but you could 'configure' them to work.
That is like getting all 'IKEA' furniture for your house.  You could easily build it wrong.  It could not work
together.  Yes, you get to 'tweak' it, but if someone simply offered "a furnished house" you would save a lot of
time and leverage their full sense of design.  Or you could go to a different furnished house that more closely matched your tastes.</p>

<p>The later fully-working / opinionated frameworks (like Ruby/Rails) truly worked out of the box.  They would come up with a UI, Business/Domain
layer, and a Database layer.  You could add things to the UI and it would go down the whole stack.  Add things to the database
or Domain, and it would bubble up/down.  For the framework to do these things it had to have a model for what software (in its full form)
looks like.  These frameworks had patterns/templates/rules for building things at command.  This isn't quite as good as Smalltalk ("it already exists")
but it is getting close, especially with sample applications available.
It also gets rid of the one problem / hurdle with Smalltalk full-computers: you had to strip them
of things you didn't want customers to see / use / clone.</p>

<h3>Languages</h3>

<p>There are many modern languages.  They are mostly quite similar and boring in the language themselves.  The community around the
language makes much more of a difference, and the libraries / frameworks that exist based on that community's interest.</p>

<p>I mentioned that I switched to Java pretty early on, which cost me productivity.  But I wanted the community and their
libraries.  Java was popular enough that it had multiple communities associated with it.  Some were crazy stupid and
created things (even tried to mandate use of things) that were completely stupid.  But other communities continued
to plug along and evolve libraries and frameworks that are better than you can get in other languages.  On the whole,
I believe the Java ecosystem is by far the best 'hub' to build most custom development and to pair
with other tools/components in other languages.  And by Java, I mean Java, Groovy, and potentially other JVM-targeting
languages.  The less Java-like the language, the less likely I would consider it acceptably an 'other'.</p>

<h3>Framework</h3>

<p>I believe the best (general) application framework in Java is Grails, which lives on top of the Spring stack.  It is
very mature and has good minds in the drivers seat.  It gets simpler and more powerful every generation.  If Spring
does something right, Grails simply uses it.  If not, Grails augments it.  Very rational.  Very powerful.</p>

<h2>The first application</h2>

<p>The first application will simply be the default applications with a grails 'create-app'.  To get the application we
need to get grails on the 'app1' nodes, create the application, and then run it.</p>

<p>Grails needs Java, but ec2 instances automatically have that.  In other environments we would use something like:</p>

<h3>installJava.sh</h3>

<p>```bash</p>

<h1>!/bin/bash</h1>

<h1>================================================</h1>

<h1>=== Java</h1>

<h1>================================================</h1>

<p>export JAVA_VERSION=7u79
export JAVA_FULL_VERSION=jdk-${JAVA_VERSION}-linux-x64</p>

<p>mkdir -p .temp
cp it/resource/${JAVA_FULL_VERSION}.rpm .temp/</p>

<p>./bin/inflatePaths.sh .temp/${JAVA_FULL_VERSION}.rpm</p>

<p>rpm -i .temp/${JAVA_FULL_VERSION}.rpm</p>

<p>```</p>

<p>The advantage of storing the RPM within our own system is speed of access and reliability.  EC2 to S3 communication
is very fast.  And S3 has never been down (AFAIK) at all, let alone when EC2 is running.  We also lock down on the
version we want vs. using 'yum' without an explicit version.</p>

<h3>installGrails3_x.sh</h3>

<p>For grails we will get the latest version</p>

<p>```bash</p>

<h1>!/bin/bash</h1>

<h1>==========================================================</h1>

<h1>=== Install Grails 3.x</h1>

<h1>==========================================================</h1>

<p>curl -s get.sdkman.io | bash
source "$HOME/.sdkman/bin/sdkman-init.sh"
yes | sdk install grails</p>

<p>source ~/.bashrc
grails -version</p>

<p>```</p>

<p>We need to source '~/.bashrc' so we get the additions to our path.</p>

<h3>Create 'test' application</h3>

<p>At this point we have grails on the machine and can simply
```bash
mkdir -p /root/app/
cd /root/app</p>

<p>grails create-app test
cd test
grails run-app
```</p>

<p>The application will come up at 'localhost:8080' and if you wget/curl it, it returns the generated 'index.html' file:</p>

<p>```html
&lt;!doctype html>
<html lang="en" class="no-js"></p>

<pre><code>&lt;head&gt;
    &lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt;
    &lt;meta http-equiv="X-UA-Compatible" content="IE=edge"&gt;
    &lt;title&gt;Welcome to Grails&lt;/title&gt;
    &lt;meta name="viewport" content="width=device-width, initial-scale=1"&gt;
</code></pre>

<p>...</p>

<pre><code>    &lt;div id="page-body" role="main"&gt;
        &lt;h1&gt;Welcome to Grails&lt;/h1&gt;
        &lt;p&gt;Congratulations, you have successfully started your first Grails application! At the moment
           this is the default page, feel free to modify it to either redirect to a controller or display whatever
           content you may choose. Below is a list of controllers that are currently deployed in this application,
           click on each to execute its default action:&lt;/p&gt;

        &lt;div id="controller-list" role="navigation"&gt;
            &lt;h2&gt;Available Controllers:&lt;/h2&gt;
            &lt;ul&gt;

            &lt;/ul&gt;
        &lt;/div&gt;
    &lt;/div&gt;

    &lt;div class="footer" role="contentinfo"&gt;&lt;/div&gt;
    &lt;div id="spinner" class="spinner" style="display:none;"&gt;Loading&amp;hellip;&lt;/div&gt;
&lt;/body&gt;
</code></pre>

<p></html>
```</p>

<h3>Firewalls</h3>

<p>Unfortunately, you won't be able to test this app from the outside unless you open the '8080' port.</p>

<p>If we open that port either initially or through commands:
```json
{</p>

<pre><code>"IpProtocol":"tcp",
"FromPort":"8080",
"ToPort":"8080",
"CidrIp":"0.0.0.0/0"
</code></pre>

<p>}
```</p>

<p>We get the latest and greatest 'Grails' application (3.0.8 as of this writing)</p>

<p><img src="http://markfussell.emenar.com/images/add-10/add10_grails1.png" /></p>

<h3>Demo2: PetClinic</h3>

<p>There is a PetClinic demo at: https://github.com/grails-samples/grails-petclinic .  Doing the same simple launch procedure you would get something like this:</p>

<p>```bash
mkdir -p /root/app/
cd /root/app</p>

<p>git clone git@github.com:grails-samples/grails-petclinic.git
cd grails-petclinic
./gradlew run
```</p>

<p>And you get:</p>

<p><img src="http://markfussell.emenar.com/images/add-10/add10_grails2.png" /></p>

<p>But an important difference between the two: the second is now <em>wired</em> into the git repository and can 'pull' or 'push' to it as needed.
We now have a live server with both alive 'IT' and alive 'APP'!  It will happily and automatically bend to our will and needs.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-9]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-9/"/>
    <updated>2015-10-08T02:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-9</id>
    <content type="html"><![CDATA[<p>This is the ninth installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>.  In the previous parts I described the big picture, Vagrant and EC2,
node initialization, the one minute configuration HeartBeat, HipChat integration, presence, EC2, and configuring nodes.</p>

<h2>Using Presence for configuration</h2>

<p>So far, presence has been just information that 'humans' consume.  It shows up on dashboards, in chat rooms, and so on,
but nothing has acted upon it.  Until now!</p>

<p>We have 'app' and 'db' nodes.  Clearly the 'app' nodes need to find the 'db' nodes or the app is not going to be
able to persist much.  The 'db' here happens to be 'Maria' but it could be anything from a single DB node to a
cluster of Riak nodes.  At the moment, I just want to get the information that a 'db' node knows ("I exist!", "My IP is this!")
over to the 'app' nodes so they can process it.</p>

<h3>It is already there?</h3>

<p>But wait?  All nodes have 'repo4' for writing?  Don't they already have everything in 'repo4' for reading as well?</p>

<p>And so they do.  The presence information is already there waiting patiently for somebody or someboty to read it.</p>

<!-- more -->


<p>As of this writing, repo4 looks like this:</p>

<p><img src="http://markfussell.emenar.com/images/add-9/add9_all.png" /></p>

<p>And the live hipchat still looks like this:</p>

<p><img src="http://markfussell.emenar.com/images/add-8/add8_demo1.png" /></p>

<p>So the DB server is definitely there:</p>

<p><code>
i-4e7cb59a:fed1/db1: Launched!
</code></p>

<p>And the information is there:
```json
{</p>

<pre><code>"filetype": "nodepresence",
"value": {
    "a": "a",
    "deployment": "fed1",
    "initgitrepo": "repo2_petulant-cyril",
    "instance-id": "i-4e7cb59a",
    "nodepart": "db1",
    "stacktype": "lad1",
    "state": "done",
    "statelog": "setup:20151008-174646;initdone:20151008-174830;",
    "statetss": "20151008-174830"
}
</code></pre>

<p>}
```</p>

<p>So the only issue is for nodes to 'find their partners'</p>

<h3>Finding the partners</h3>

<p>By the HipChat there are only three nodes alive, so problem one is finding live nodes vs. dead nodes.  There are
two levels to that:</p>

<ul>
<li> Finding plausibly live nodes</li>
<li> Finding truly awake nodes</li>
</ul>


<p>The simplest approach to the first is to make sure there is some kind of heartbeat within 'statetss'.  Every-minute
is clearly possible, but a bit noisy if done in the main part of repo4.  It would be nice to not to see the heartbeat
block out the actual state change information that is already there. An interesting alternative is to 'flatten time'
and have an alternative branch that stores information as 'it/presence/flattime/timestamp'.  Or given we are storing
the information differently, we could use the main branch and just change the comment to mention 'flattime'.
Yes, the information in the files would not change very often.  But Git stores files separate from paths,
so there is very little overhead to adding new paths to identical files.</p>

<h4>Time flattened</h4>

<p>repo4 has folder under 'presence' called 'flattime' which contains the presence information where every checkin is a new path.
To keep things manageable the timestamp is complete 'YYYYMMDDHHMM', but year/month/day/hour/ is used to organize it.
Seconds are not used because we want everything to be together and there is no guarantee the seconds would match
between machines.</p>

<p>Looking at a few minutes of time, we get something like this:</p>

<p><img src="http://markfussell.emenar.com/images/add-9/add9_flattime1.png" /></p>

<p>The files change a bit because the machines are switching states and at the 'capture' moment could be in almost any state
of their 'state machine'.</p>

<h4>How precise?</h4>

<p>So this is quite precise in time and you could certainly slow it down.  Sometimes providers get annoyed if you use
git this way, but the software itself is thoroughly comfortable with it.  You can get some impressive merge graphs
as the machine count goes up.</p>

<p><img src="http://markfussell.emenar.com/images/add-9/add9_merge1.png" /></p>

<p>So making things less often and more jittered will help alleviate some stress.  Since
this is only the first stage of presence (what exists and is plausibly alive), we will deal with stale data in the
second stage.</p>

<h3>Who is what?</h3>

<p>So we have a directory of JSON files and we want to find certain kinds of partners.  It is simplest to just
run through all the files and see what is inside them.  The files can be loaded one-by-one or concatenated together
into a working temporary file.</p>

<p>A basic python script could look like this:</p>

<h4>buildLiveServerJson.py</h4>

<p>```python</p>

<h1>!/usr/bin/env python</h1>

<h1>========================================================</h1>

<h1>=== buildLiveServerJson.py</h1>

<h1>=== This builds a JSON structure of server presence data</h1>

<h1>=== from a directory of presence data</h1>

<h1>=========================================================</h1>

<p>import json
import string
import os
import sys
from optparse import OptionParser
from datetime import datetime,timedelta</p>

<p>now = datetime.utcnow()
tss = now.strftime("%Y%m%d%H%M%S")</p>

<p>backminute = timedelta(minutes=-1)
before = now + backminute</p>

<p>tsm = before.strftime("%Y%m%d%H%M")
datepath = before.strftime("%Y/%m/%d/%H")</p>

<p>USAGE_STRING = "usage: %prog [options]"</p>

<p>parser = OptionParser(usage=USAGE_STRING)
parser.set_defaults(verbose=True)
parser.add_option("--source", action="store", type="string", dest="source", help="The source directories of the presence files.  Use a comma to separate multiple source directories.")
parser.add_option("--adddatepath", action="store_true", default=False, dest="adddatepath", help="Whether to add the current datetime path to the source")
parser.add_option("--suffix", action="store", type="string", dest="suffix", help="A suffix to add at the end of the source")
(options, args) = parser.parse_args()</p>

<p>source = options.source.strip()
adddatepath = options.adddatepath
suffix = options.suffix</p>

<p>if not source:</p>

<pre><code>parser.print_help()
exit()
</code></pre>

<p>full_source = source</p>

<p>if adddatepath:
   full_source = full_source + datepath + '/' + tsm</p>

<p>if suffix:
   full_source = full_source + suffix</p>

<h1>=========================================================</h1>

<h1>=========================================================</h1>

<h1>=========================================================</h1>

<p>sys.stdout.write('{"source":"'+source+'","suffix":"'+suffix+'","full_source":"'+full_source+'","tss":"'+tss+'","tsm":"'+tsm+'","datepath":"'+datepath+'","nodes":[')</p>

<p>isfirst = True
for f in os.listdir(full_source):
  f_json = json.load(open(full_source+f))</p>

<p>  if isfirst:</p>

<pre><code> isfirst = False
</code></pre>

<p>  else:</p>

<pre><code> sys.stdout.write(",")
</code></pre>

<p>  sys.stdout.write(json.dumps(f_json))
sys.stdout.write("]}")</p>

<p>sys.stdout.flush()</p>

<p>```</p>

<p>Our first stage is to get a list of plausible nodes and store that:</p>

<p><code>bash
python buildLiveServerJson.py --source=/root/gitrepo/repo4_sagacious-adventure/it/presence/flattime/ --adddatepath --suffix /all/ | python -mjson.tool &gt; /root/nodeinfo/liveserver.json
</code></p>

<p><img src="http://markfussell.emenar.com/images/add-9/add9_json1.png" /></p>

<p>Next we can filter out the ones that don't respond to our 'awake' check.  That leaves
us with one or more remaining.  Depending on the kind of system you may actually want to know a bunch of nodes vs.
just one (e.g. ZooKeeper).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-8]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-8/"/>
    <updated>2015-10-08T01:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-8</id>
    <content type="html"><![CDATA[<p>This is the eighth installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>.  In the previous parts I described the big picture, Vagrant and EC2,
node initialization, the one minute configuration HeartBeat, HipChat integration, presence, and EC2 integration.</p>

<h2>Configuring Node Parts and even fuller Presence and visibility</h2>

<p>So far our node was a generic 'ControlNode'.  A ControlNode is a node that is in your data center
(to be near the other nodes), similar to your actual nodes, and configured to be able to do interesting tasks.
It is not a critical part of anything, so you can fiddle with it and just throw it away.  Say you want to
start configuring a database node.  You launch and login to a ControlNode and then try to install a database
(for example Maria).</p>

<!-- more -->


<p>```bash
cat > /etc/yum.repos.d/mariadb10.repo &lt;&lt;EOS</p>

<h1>MariaDB 10.0 CentOS repository list - created 2015-10-08 15:16 UTC</h1>

<h1>http://mariadb.org/mariadb/repositories/</h1>

<p>[mariadb]
name = MariaDB
baseurl = http://yum.mariadb.org/10.0/centos7-amd64
gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB
gpgcheck=1
EOS</p>

<p>yum install MariaDB-server MariaDB-client
```</p>

<p>Assuming you changed to root for the instance:</p>

<p><code>bash
sudo bash
</code></p>

<p>This works!  After confirming some things along the way...</p>

<p>OK, so now we want to have our database servers install MariaDB.  We first need an 'installMaria' script with the above
in it.  Say</p>

<h3>installMaria10.sh</h3>

<p>```bash</p>

<h1>!/bin/bash</h1>

<h1>================================================</h1>

<h1>=== Java</h1>

<h1>================================================</h1>

<p>cat > /etc/yum.repos.d/mariadb10.repo &lt;&lt;EOS</p>

<h1>MariaDB 10.0 CentOS repository list - created 2015-10-08 15:16 UTC</h1>

<h1>http://mariadb.org/mariadb/repositories/</h1>

<p>[mariadb]
name = MariaDB
baseurl = http://yum.mariadb.org/10.0/centos7-amd64
gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB
gpgcheck=1
EOS</p>

<p>yum install MariaDB-server MariaDB-client
```</p>

<h3>adding init</h3>

<p>And unless you want every node to install Maria, you need to add an 'init' for the type of node we are dealing with.
In our case, the StackType is 'lad1' (Load,App,Db,1) and the node type / part is 'db1'.</p>

<p>So we get this hierarchy:</p>

<p><img src="http://markfussell.emenar.com/images/add-8/add8_hierarchy1.png" /></p>

<p>And the content of the 'init.sh' file is simply:
```bash</p>

<h1>!/bin/bash</h1>

<h1>====================================================</h1>

<h1>=== fed1/db1</h1>

<h1>====================================================</h1>

<p>bash ${COMMON}/installMaria10.sh</p>

<p>```</p>

<h3>CloudFormation</h3>

<p>We have a new CloudFormation named 'awscf4_Fed1Db1' with the simple change of the nodepart being 'db1'</p>

<p>```json</p>

<pre><code>"TemplateConstant" : {
  "stacktype" : { "value" : "lad1" },
  "initgitrepo" : { "value" : "repo2_petulant-cyril" },
  "nodepart" : { "value" : "db1" },
  "state" : { "value" : "presetup" },
  "statetss" : { "value" : "" },
  "statelog" : { "value" : "" },
  "deployment" : { "value" : "fed1" }
},
</code></pre>

<p>```</p>

<h3>Demo or Die!</h3>

<p>Since the previous presence description, the presence system has upped itself and now nodes check-in with
more information than before.  After launching two 'app' stack and one 'db' stack we get this:</p>

<p><img src="http://markfussell.emenar.com/images/add-8/add8_demo1.png" /></p>

<p>So clearly our nodes know who they are, what deployment they are in, what the 'part' in that deployment is, and how to
"do work". But are they actually what they say they are?</p>

<h2>Being the 'user', fixing the 'user'</h2>

<p>If you use the installMaria script above, it will not work.  Because I said: "This works!  After confirming some things along the way..."
During 'boot' there is no user to confirm anything.  So the 'yum' part fails although the yum repository is there
(the user is 'root' so it has permission).  It ran the script but it didn't work under 'automation'.
The three annoying issues in automation:</p>

<ul>
<li> The user could be different from the user you think it is (say 'ec2' vs. 'root')</li>
<li> The user is not interactive</li>
<li> The user did not "launch a login shell" and so some launch things that happen for you did not happen for them.</li>
</ul>


<p>There are trivial and super-effective solutions to all of these, but until you get them right, you can bang your head
quite a bit.</p>

<h3>Who is the user?  'root'</h3>

<p>The user provisioning a machine should always be 'root'.  Yes, 'root' is dangerous.  Because 'root' is powerful.  And
given (a) if you mess up you simply kill the machine, and (b) everything is from version-controlled source files...
you can handle that power.  So don't add silly hoops to jump through.  On a ControlNode, immediately 'sudo bash'.
And if for some reason the default user isn't 'root' in a launch or cron script, 'su root' or 'sudo bash' to fix that.</p>

<h3>Is the user interactive? 'no'</h3>

<p>We are doing production automation that is designed to scale into thousands of machines.  No one is going to answer
questions for thousands of machines.  That is not 'scalable' or at all valuable.  You should always know all the
answers when a machine launches.  So script any UI that really requires some value put in, or use the variant of
a command that has default answers.</p>

<p>The true script for 'installMaria' has this line.</p>

<p><code>bash
yes | yum -y install MariaDB-server MariaDB-client
</code></p>

<p>The 'yum -y' <em>should</em> never ask any questions.  But just in case it does, I give it a 'y' for every answer.  If you
are testing something and get a question, check if it has a flag like '-y'</p>

<p><img src="http://markfussell.emenar.com/images/add-8/add8_yes.png" /></p>

<h3>Has the user launch as a login shel? 'yes'</h3>

<p>Since getting on a machine without 'logging in' is quite a bit more painful than 'ssh' into the machine, just make sure
any 'init' and 'work' script has read the files a login shell would automatically read.</p>

<p>A simple 'source /root/.bashrc' fixes the problem immediately</p>

<p>```bash</p>

<h1>========================================</h1>

<h1>=== Do the work for this repository</h1>

<h1>========================================</h1>

<p>source /root/.bashrc
...
```</p>

<h2>Node Work</h2>

<p>Just like the 'init' hierarchy, the 'db1' nodes can have their own work items to do regularly by simply adding the 'work.sh'
into the hierarchy:</p>

<p><img src="http://markfussell.emenar.com/images/add-8/add8_hierarchy2.png" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-7]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-7/"/>
    <updated>2015-10-04T01:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-7</id>
    <content type="html"><![CDATA[<p>This is the seventh installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>.  In the previous parts I described the big picture, Vagrant and EC2,
node initialization, the one minute configuration HeartBeat, HipChat integration, and presence.</p>

<h2>Fuller Presence and EC2 Integration</h2>

<p>In the previous part, I went through a very simple but powerful model of 'presence' using simply GitHub repositories.
The content of those presence statements was enough to figure out what nodes exist, but not much more about them.
The second level of presence is to update the state of the node as it changes.  For example, a node goes through
a few bootstrap steps:</p>

<ul>
<li> presetup – The node before any updates are possible (no ability to change status)</li>
<li> setup – The beginning of the 'setup' phase where the node is alive enough to change it's status</li>
<li> initdone – The time a node is done are initialization and can start doing 'work' as the 'nodepart' it is</li>
</ul>


<p>A node getting to 'setup' is pretty important: before that it may be a zombie!  And we don't want zombie's in
our federation!</p>

<!-- more -->


<p>So far for the ADD we now have four resources within which track node states:</p>

<ul>
<li> On the node (say '/root/log' or '/root/nodeinfo/state.txt)</li>
<li> Within the presence system</li>
<li> Within HipChat</li>
<li> On EC2 itself</li>
</ul>


<p>I recommend using <em>all</em> of them.</p>

<h3>On node</h3>

<p> On the node is very helpful in that it is isolated from any other failures.  You
 can 'tail' the logs or 'cat' the state file.  This tangibility helps understand things and debug if there is failure.</p>

<h3>Within Presence</h3>

<p> Within the presence system is the most powerful and flexible.  It is easy to see history and all the activity of your
 nodegrid.  And the nodegrid can use the presence system to figure out what nodes are present and in full 'working'
 mode.</p>

<h3>HipChat</h3>

<p> Within HipChat lets everyone see and talk about the changes.  It can get noisy though, so you need
 to separate the 'chatty' state changes from the 'critical' ones.  An example of 'critical' is when a machine realizes
 it is broken.  It is running the cron job, but something is wrong and it can tell that the 'work' is not completable.
 I call this being 'wedged'.  If a machine is 'wedged', it should tell people and then we can work on improving its
 DNA so it can unwedge itself in the future.  And then kill the machine.</p>

<h3>EC2</h3>

<p> By using EC2 tags you can leverage the EC2 dashboard.  I view 'tags' as read-only because the ADD should not get
 attached (be dependent) on EC2, but it is helpful for visibility.</p>

<h3>Examples</h3>

<p>The following show two machines initializing through Presence, HipChat, and EC2.  The only trigger for this was
killing the existing two instances: the AutoScalingGroup automatically replaced them.</p>

<h4>Launching viewed within EC2 Dashboard</h4>

<p>Nicely the 'add:' prefix makes all the properties that are most important appear on the left.  Some of the names
of concepts are intentionally alphabetically 'sorted' so they appear in the correct column.</p>

<p><img src="http://markfussell.emenar.com/images/add-7/add7_ec2_cv1b.png" /></p>

<p>Click here: <a target="add7_ec2_cv1b" href="http://markfussell.emenar.com/images/add-7/add7_ec2_cv1b.png" >add7_ec2_cv1b</a> to expand.</p>

<p>The ‘stacktype’ of ‘lad1’ in the capture is short for a stack of</p>

<ul>
<li>Load Balancer</li>
<li>Application</li>
<li>Database</li>
</ul>


<h4>Working viewed within HipChat</h4>

<p><img src="http://markfussell.emenar.com/images/add-7/add7_hipchat_cv1.png" /></p>

<h4>Working viewed within Presence / SourceTree</h4>

<p><img src="http://markfussell.emenar.com/images/add-7/add7_presence_cv1.png" /></p>
]]></content>
  </entry>
  
</feed>
