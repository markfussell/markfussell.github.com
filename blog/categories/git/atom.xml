<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: git | Foo]]></title>
  <link href="http://markfussell.github.com/blog/categories/git/atom.xml" rel="self"/>
  <link href="http://markfussell.github.com/"/>
  <updated>2013-01-18T21:33:42-08:00</updated>
  <id>http://markfussell.github.com/</id>
  <author>
    <name><![CDATA[Mark Fussell]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Being a git about everything (Annexing)]]></title>
    <link href="http://markfussell.github.com/blog/2013/01/13/git-about-everything-annex/"/>
    <updated>2013-01-13T18:16:00-08:00</updated>
    <id>http://markfussell.github.com/blog/2013/01/13/git-about-everything-annex</id>
    <content type="html"><![CDATA[<p>This is the second in a series of using git as part of interesting solutions to problems.</p>

<p>The first is here: <a href="/blog/2013/01/11/git-about-everything-intro/">Intro</a></p>

<h2>Dealing with Binary Files</h2>

<p>As mentioned in the first posting, git and similar DVCS have issues with binary files.  Adding 100s of 10MB files, or 100s of
versions of 10MB files will produce gigabytes worth of data that must be cloned by everyone using the repository.  How do we avoid this?</p>

<p>There are a number of solutions in this space out there with differing characteristics, but the core approach is usually a similar
"Don't store it in git".  Instead we want to record enough information to retrieve the binary files from somewhere else.</p>

<!-- more -->


<h2>Record Enough of the Binary File</h2>

<p>What is
enough information?  How about 160 bits!  Using SHA1 or any similar hash, we can identify the contents of any file.  To add
a bit of consistency and readability, we will make this hex-based, so we get 40 characters.  And to make it a little clearer what
hash was used and what this string is, we add 'sha1_' to the front and '.blob' to the end.  Now at 50 to 52 bytes depending on
human-entered white spaces.  For example:</p>

<ul>
<li> sha1_8ac02ee34b94461fed19320d789f251e6a2a6796.blob</li>
<li> SHA1-Hash = 8ac02ee34b94461fed19320d789f251e6a2a6796</li>
<li> Google test: <a href="http://www.google.com/search?q=8ac02ee34b94461fed19320d789f251e6a2a6796">http://www.google.com/search?q=8ac02ee34b94461fed19320d789f251e6a2a6796</a></li>
</ul>


<p>And we have confirmed that the hash is enough to identify that file's content (and the file itself if it has a unique name).</p>

<p>Storing tens of thousands of 50Byte files is still under a few megabytes, so that part is good.</p>

<h2>Put the content into the Cloud (or similar)</h2>

<p>Where should we store the actual content?  Pretty much anywhere we want if things are simple (only a few files at a time), but
if we want to store and retrieve thousands of files rapidly things get nastier.  For example, there are projects that try to
hide the content being moved elsewhere using git smudge/clean filters.  These make the process all of: sequential, heavy, and
perpetual.  We would have similar major issues if we moved the content via some other sequential approach that was not super-fast.  And
git is meant to be highly distributed, so our approach needs to work for all the 'clones' of a repository.  And finally, we don't
want to break the bank over this.</p>

<p>The solution?  S3 or similar.  S3 is:</p>

<ul>
<li> Relatively inexpensive for what it does</li>
<li> Highly distributed</li>
<li> Reasonably fast in network transfers</li>
<li> Incredibly fast as you throw more workers at the network transfers</li>
<li> Blazingly fast if you throw many workers from EC2 at it</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Being a git about everything (Intro)]]></title>
    <link href="http://markfussell.github.com/blog/2013/01/11/git-about-everything-intro/"/>
    <updated>2013-01-11T18:16:00-08:00</updated>
    <id>http://markfussell.github.com/blog/2013/01/11/git-about-everything-intro</id>
    <content type="html"><![CDATA[<p>There are times when a new technology comes along, that at first appears to be pretty similar to
existing technology, but certain characteristics make for radically different or just nicely new solutions.
A recent example of this is 'git' and similar distributed version control systems (DVCS).  They may
at first appear to be an interesting version of centralized version/content management systems, but
they are really much more... a core piece of technology useful for many things.</p>

<p>This is a series about how to use git to solve many different problems, some obvious and some more unusual.
I hope a few of them are interesting to readers.</p>

<!-- more -->


<p>There are alternative DVCSs but I am not going to compare or translate examples... at least not on a first pass.
Git was created by Linus Torvalds in 2005.  It was initially quite 'raw' and still maintains much of that rawness,
but other tools (e.g. github) and general improvements have made it more accessible.  For source-code control, git has some big wins
in collaboration and offline work compared to SVN, Perforce, and the like.  Whether these are worth some trade-offs depends on your team
and company... but that is a different topic.</p>

<h2>Git Syntax / Overview</h2>

<p>You can learn more about git somewhere like github:</p>

<ul>
<li> <a href="http://learn.github.com/p/intro.html">http://learn.github.com/p/intro.html</a></li>
</ul>


<p>Git has a lot of commands, but around seven are core to standard git flows:</p>

<ul>
<li> clone &mdash; copy a repository from a remote location</li>
<li> fetch &mdash; get updates from a remote repository</li>
<li> merge &mdash; merge changes from one branch into the current branch</li>
<li> pull &mdash; 'fetch' and then 'merge'</li>
<li> add &mdash; add changes to the commit stage of the current branch</li>
<li> commit &mdash; commit the changes into the current branch</li>
<li> push &mdash; try to make a remote branch look like the current branch</li>
</ul>


<p>Actually of those seven 'pull' basically replaces/combines two of them, so you get
down to about five with a core loop like this:</p>

<ul>
<li> clone

<ul>
<li>pull</li>
<li>make changes (if needed)

<ul>
<li>add</li>
<li>commit</li>
<li>push</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>Of these commands only 'push' can fail due to timing.  'push' is transactional
and you can be contending with other people or machines that pushed to the same branch
as you between when you pulled and you pushed.  Merging can fail, but that
represents some actual file-level conflict vs. a timing issue (someone beat you to the 'push').
A proper 'commit' can't fail because it is local.</p>

<h3>Repositories and branches</h3>

<p>At least to begin, all these solutions will use a standard centralized repository approach
and generally not use branches.  So you don't have to worry about git's ability to
deal with many remotes and which branch a given git repository is using.  By default
at any current moment there is only</p>

<ul>
<li> local: 'master' <-> remote: origin/master</li>
</ul>


<p>where things get interesting because there could be 100 different machines each with their own 'local'.</p>

<h3>Problems with git / DVCS</h3>

<p>The biggest issue with git and similar DVCSs is that their model doesn't work well for large amounts of
binary assets.  Large amounts of binary assets could occur either because there are a large number of
assets available at any time but only a subset are needed (so with Perforce or SVN many people would not
check out those directories) or more commonly, a modest number of binary assets are frequently
changing.  Because a git repository contains all assets throughout time and to work with a git repository
you clone the whole thing, having a large amount of binary assets punishes everyone.</p>

<p>What is a large amount?  That depends on the circumstances, but generally passing 100MB can start to
become painful depending on the purpose git is being used for.  Having 1GB of textual files in
a single git repository (even over time) is an unusual thing.  GBs of images is common.</p>

<p>So our first problems and solution is going to have to deal with this critical issue.</p>

<p>Enter the annex: <a href="/blog/2013/01/13/git-about-everything-annex/">Annex</a></p>
]]></content>
  </entry>
  
</feed>
