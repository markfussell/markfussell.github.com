<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Git | Polyglot]]></title>
  <link href="http://markfussell.emenar.com/blog/categories/git/atom.xml" rel="self"/>
  <link href="http://markfussell.emenar.com/"/>
  <updated>2013-02-19T15:25:24-08:00</updated>
  <id>http://markfussell.emenar.com/</id>
  <author>
    <name><![CDATA[Mark Fussell]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Being a git about everything (IT Automation)]]></title>
    <link href="http://markfussell.emenar.com/blog/git-about-everything-it-automation/"/>
    <updated>2013-02-16T18:16:00-08:00</updated>
    <id>http://markfussell.emenar.com/blog/git-about-everything-it-automation</id>
    <content type="html"><![CDATA[<p>This is the fourth in a series of using git as part of interesting solutions to problems.</p>

<p>The first is here: <a href="/blog/git-about-everything-intro/">Intro</a></p>

<h2>Leveraging git to help enable automated IT</h2>

<p>Doing IT for computers involves installing software, configuring things, doing backups, updates, etc.
The ultimate IT is one that <em>'simply works'</em> and involves almost no human interaction
even in failure situations.  Ideally IT should be equivalent to
a macro-level program that does everything that does not require touching
a physical machine.</p>

<p>This IT-as-program has become easier and easier over the last many years with
better and more standardized operating systems, free software that does not
require annoying human interaction during installation, and virtualization on top of physical
hardware that makes provisioning and reprovisioning easier.  With cloud computing,
IT-as-program becomes almost a necessity as hundreds of virtual computers are created, updated, failed,
migrated, and decommissioned.</p>

<p>Git alone doesn't enable IT-as-program but it can be a core component in many areas.  Among these are:</p>

<ul>
<li> Easy 'Live IT' servers</li>
<li> A Push-Me-Pull-You model for continual deployment</li>
<li> Server presence</li>
</ul>


<p>Having git as a core piece of IT infrastructure enables thousands of machines to very rapidly react (within a minute or two)
without needing a heavy infrastructure.  You simply need one or two (for redundancy) git servers, of which one can be GitHub
or a similar free or inexpensive service.  Other technologies in this space have significantly more complicated servers,
are more likely to be SPOFs (Single points of failures) or bottlenecks, and are much more expensive as a service.</p>

<!-- more -->


<h2>Easy 'Live IT' servers</h2>

<p>A 'Live IT' server is one that can automatically do new things when something about the IT world changes.  This
is not referring to how sophisticated the applications on a server are, but whether the server itself can
manage upgrades or other configuration changes to itself.  Examples are:</p>

<ul>
<li> Deploying new version of a Java war or a Rails app</li>
<li> Doing database backups and offloads</li>
<li> Offloading or deleting logs (that are harder than logrotate)</li>
<li> Reacting to simple configuration changes</li>
<li> Reacting to server presence changes</li>
</ul>


<p>There are a number of ways to do the activities listed above, from manually interacting with machines, through cron jobs,
mass 'push' model interactions (e.g. Capistrano), and finally puppetry via Chef and similar.  I have found almost all of
these to be lacking in many ways, including:</p>

<ul>
<li> Lack of documented change-to-server</li>
<li> Difficulty in rolling back changes</li>
<li> Not scaling nicely (one client hitting many servers, or many servers doing queries against another server)</li>
<li> Lack of flexibility</li>
<li> Slowness or non-responsiveness (delays) of applying changes</li>
<li> Differences from 'bootstrap' of cloud servers</li>
</ul>


<h3>Pull Model</h3>

<p>A different approach leveraging Git (or any other DVCS) seems to produce much simpler and more powerful
solutions.  The approach is composed of:</p>

<ul>
<li> A git repository that has working scripts (in any language people like, including Chef-solo)</li>
<li> A simple bootstrap script that clones the repository and calls an <code>init.sh</code> script in it</li>
<li> A cron job that is set up by the <code>init.sh</code> script.  This cron job executes every minute

<ul>
<li>Goes into the working git repository</li>
<li>Does a pull to get the latest version of scripts</li>
<li>Then calls into a <code>work.sh</code> script</li>
</ul>
</li>
</ul>


<p>This flow enables activity at every minute, using the latest version of the git repository, and with very little overhead
for the core behavior.  Advantages are:</p>

<ul>
<li> All changes to a server are caused by one or more git repositories changing.  Servers can even publish there status by showing the git revision they are on.</li>
<li> Rolling back changes is simply reversing a commit</li>
<li> The only centralized activity is the <code>git fetch</code> which is very simple and fast.</li>
<li> So far the only constraint is the time is every minute, and that could be sub-minute but needing that is rare</li>
<li> Delays are at most a minute, and again that could easily become less (but not sub-second)</li>
<li> The behavior is actually the same as bootstrapping a server.  A bootstrap is just the first minute of work.</li>
</ul>


<h3>Example-1</h3>

<p>Example-1 is the initial example of this model.  The repository is</p>

<ul>
<li> <a href="https://github.com/markfussell/giteveryrepo3/">https://github.com/markfussell/giteveryrepo3/</a></li>
</ul>


<p>with the CloudFormation template being:</p>

<ul>
<li> <a href="https://github.com/markfussell/giteveryrepo3/blob/master/it/aws/cloudformation/GitEverythingServer3.template">https://github.com/markfussell/giteveryrepo3/blob/master/it/aws/cloudformation/GitEverythingServer3.template</a></li>
</ul>


<p>This has a UserData section which does the initial bootstrap of cloning the repository and
calling an init script inside it.</p>

<p>```json</p>

<pre><code>      "yum -y install git \n",
      "mkdir /root/gitrepo \n",
      "cd /root/gitrepo \n",
      "git clone git://github.com/markfussell/giteveryrepo3.git  \n",

      "cd /root/gitrepo/giteveryrepo3 \n",
      "source bin/init/common/init.sh \n",
</code></pre>

<p>```</p>

<p>The <code>init.sh</code> script simply sets up a cron job that calls the <code>cron_1m.sh</code> script
in <code>/root/bin/</code>.  I prefer to have crontab files that are very simple (e.g. one line)
and call into /root/bin/ scripts so (a) it is more visible what crons are running
(b) if there are any inter-cron issues they can be managed, and (c) it is easy to
disable a cron by doing a rename.</p>

<p>The <code>init.sh</code> file:</p>

<h1>```bash</h1>

<h1>=== Have a preference that crons</h1>

<h1>=== all go through a single file</h1>

<h1>================================</h1>

<p>mkdir -p /root/bin
cp ./bin/init/common/cron_1m.sh /root/bin/cron_1m.sh
chmod +x /root/bin/cron_1m.sh</p>

<p>cat &lt;<EOS > /var/spool/cron/root
MAILTO=""</p>

<ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li>/root/bin/cron_1m.sh
EOS</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>```</p>

<p>And the <code>cron_1m.sh</code> file:</p>

<p>```bash</p>

<h1>! /bin/bash</h1>

<h1>================================</h1>

<h1>=== Simple worker example</h1>

<h1>================================</h1>

<p>export ME=<code>basename $0</code>
export LOG=/root/log/${ME}<em>log.txt
export ERROR=/root/log/${ME}</em>error.txt
export START_TSS="<code>date +%Y%m%d-%H%M%S</code>"</p>

<p>mkdir -p /root/log/</p>

<p>exec 1>> ${LOG}
exec 2>> ${ERROR}</p>

<p>echo "${ME}: Start  ${START_TSS}" >> ${LOG}</p>

<p>export REPOS=<code>find /root/gitrepo/ -maxdepth 1 -mindepth 1</code>
for REPO in ${REPOS}; do</p>

<pre><code>pushd ${REPO}
    git pull

    source bin/work/common/work.sh
popd
</code></pre>

<p>done</p>

<p>export FINISH_TSS="<code>date +%Y%m%d-%H%M%S</code>"
echo "${ME}: Finish ${FINISH_TSS}" >> ${LOG}</p>

<p>```</p>

<p>As you can see the <code>cron_1m.sh</code> script is "repo flexible".
It will do any and all <code>work.sh</code> file it finds in the repositories under
<code>/root/gitrepo/</code>.  You might want to be more restrictive than
that (say only 'active' repos), but this at least shows the power of the
generalization.</p>

<p>If you login to the server, you will find it doing some kind of work:</p>

<p>```bash
[root@ip-10-120-174-182 ~]# tail -f /root/log/*
==> /root/log/cron_1m.sh_error.txt &lt;==</p>

<p>==> /root/log/cron_1m.sh_log.txt &lt;==
Already up-to-date.
We are doing some kind of work
~
cron_1m.sh: Finish 20130219-050202
cron_1m.sh: Start  20130219-050301
~/gitrepo/giteveryrepo3 ~
Already up-to-date.
We are doing some kind of work
~
cron_1m.sh: Finish 20130219-050301</p>

<p>```</p>

<p>As you can see, the total time to do a "Hello World" is under a second.  Very fast!</p>

<h4>Inherent overhead of approach</h4>

<p>The amount of overhead associated associated with this approach is less than a second.  The fetch itself:</p>

<p>```bash
[root@ip-10-120-174-182 giteveryrepo3]# time git fetch</p>

<p>real    0m0.087s
user    0m0.002s
sys 0m0.006s
```</p>

<p>is 87 milliseconds and the system overhead is 8 milliseconds on an m1.small (that isn't doing anything else).
With a busier server the 'real' time goes up a bit, but the system overhead is still tens of milliseconds at
most.</p>

<p>Although fetching is useful on its own, we will initially always merge as well, so let us time that:</p>

<p>```bash
[root@ip-10-120-174-182 giteveryrepo3]# time git pull
Already up-to-date.</p>

<p>real    0m0.105s
user    0m0.007s
sys 0m0.053s
```</p>

<p>Pulling requires a bit more effort to do a no-op merge but things are still in the tens of milliseconds of 'effort'
and a clock time well under a second.</p>

<h2>Push-Me-Pull-You</h2>

<div style="float:right">
<img width="244" height="191" src="http://markfussell.emenar.com/images/git-about-everything-it-automation/Pushmepullyou_mlf1c.png" />
</div>


<p>The previous section discussed having 'Live IT' servers that use the very-fast git pull to
get updates that the server will react to.  Updating central git repositories uses an atomic 'push'
operation, so the obvious name for this pattern of pushing changes from one place (say 'me')
to a hundred servers who are listening (let us call them 'you') is... 'Push-Me-Pull-You'...
which even has a handy mascot.</p>

<p>Some great things about the PushMePullYou were identified above:</p>

<ul>
<li> Extremely low overhead and very simple model</li>
<li> All changes to a server are caused by one or more git repositories changing.  Servers can even publish there status by showing the git revision they are on.</li>
<li> Rolling back changes is simply reversing a commit</li>
<li> The only centralized activity is the <code>git fetch</code> which is very simple and fast.</li>
<li> So far the only constraint is the time is every minute, and that could be sub-minute but needing that is rare</li>
<li> Delays are at most a minute, and again that could easily become less (but not sub-second)</li>
<li> The behavior is actually the same as bootstrapping a server.  A bootstrap is just the first minute of work.</li>
</ul>


<p>A few additional ones are:</p>

<ul>
<li> You can 'push' from anywhere you want... so you can test a change on the target IT environment and then push from there if it works</li>
<li> It is very easy to organize many different kinds of machines, kinds of deployments, etc. within a single repository</li>
<li> It is very easy to detect whether a change happens at all, whether it is potentially relevant, and with a few easy patterns, whether it would have an impact that requires real action</li>
</ul>


<p>The Example-1 above had the naive 'pull and do something no matter what'.  We need to get a bit beyond that
to have a truly useful approach.</p>

<h3>Checking commit version</h3>

<p>The simplest sanity check is to see whether we have a new commit.  This can be done in a few ways, including:</p>

<ul>
<li> Do a 'fetch' and check whether the remote branch is different from the local branch</li>
<li> After each action, store the commit version that was acted upon.  On each pull compare the old to the new</li>
<li> Have a local acted-upon branch separate from the main branch</li>
</ul>


<p>The last one has a nice feature of showing 'history-according-to-this-machine' where the other two are purely
'what is now', but it is simpler to avoid having a per-machine branch and multiple versions (the differing
merges depending on local history) occurring
everywhere.</p>

<h4>Fetch based</h4>

<p>The simplest check is just to see if there are any differences between the 'origin' and the local branch.  This
would look something like <code>detectOriginFetchDiff.sh</code>:</p>

<p>```bash</p>

<h1>=================================================</h1>

<h1>=== Detect whether the version of the origin</h1>

<h1>=== is the same as the local version.</h1>

<h1>=== Return the ORIGIN_VERSION if different</h1>

<h1>=================================================</h1>

<p>git fetch</p>

<p>export ORIGIN_VERSION=<code>git rev-parse origin/master</code>
export LOCAL_VERSION=<code>git rev-parse HEAD</code></p>

<p>if [ "${ORIGIN_VERSION}" = "${LOCAL_VERSION}" ]; then</p>

<pre><code>: #Don't do anything
</code></pre>

<p>else</p>

<pre><code>echo ${ORIGIN_VERSION}
</code></pre>

<p>fi
```</p>

<p>Where you can use this script with:</p>

<p><code>bash
if [[ -n "`./bin/pushmepullyou/detectOriginDiff.sh`" ]] ; then
   : #React to the change
else
   : #Do nothing / exit
fi
</code></p>

<p>This should be lightning fast.  The only drawback is:</p>

<ul>
<li> If you don't pull (merge) until after executing the script... you might not be able to easily change the code determining whether to executing the script</li>
</ul>


<p>You can redo an action by forcing the local branch back a version.  For example:</p>

<p><code>bash
git reset --hard master~1
</code></p>

<h4>Last-action based</h4>

<p>An alternative to the fetch-based model is to record the last action performed by the local machine.  This
deals with the drawback above: you already have the latest version of code no matter what.  Also it starts
down the path of 'mid-action' protection (to avoid doing a change or sequence of changes on top of each
other).  For example <code>detectLastActionDiff.sh</code>:</p>

<p>```bash</p>

<h1>=================================================</h1>

<h1>=== Detect whether the last action version</h1>

<h1>=== is the same as the current pulled version.</h1>

<h1>=== Return the LOCAL_VERSION if different</h1>

<h1>=================================================</h1>

<p>export LOCAL_VERSION=<code>git rev-parse HEAD</code>
export LASTACTION_FILE=./.temp/nodeinfo/lastaction.txt</p>

<p>if [[ -e ${LASTACTION_FILE} ]]; then</p>

<pre><code>export LAST_ACTION=`cat ${LASTACTION_FILE}`
if [ "${LAST_ACTION}" = "${LOCAL_VERSION}" ]; then
    : #Don't do anything
else
    echo ${LOCAL_VERSION}
fi
</code></pre>

<p>else</p>

<pre><code>echo ${LOCAL_VERSION}
</code></pre>

<p>fi
```</p>

<p>And after completing any action you write the version into the lastaction version file:</p>

<p>```bash</p>

<p>export LOCAL_VERSION=<code>git rev-parse HEAD</code>
export LASTACTION_FILE=./.temp/nodeinfo/lastaction.txt
export LASTACTION_DIR="$( cd -P "$( dirname "${LASTACTION_FILE}" )" &amp;&amp; pwd )/"</p>

<p>mkdir -p LASTACTION_DIR
echo ${LOCAL_VERSION} > ${LASTACTION_FILE}
```</p>

<h3>Avoiding mid-action collision</h3>

<p>The main advantage of the Last-action approach is it starts down the path of
making sure you are not executing things twice.  Alternatively, you could
have an <code>flock</code> associated with the calling script (say the <code>cron_1m.sh</code>),
but writing to a <code>currentaction</code> file is a bit more
OS independent, can document what version is being acted upon,
and supports multiple paths hitting the same file.</p>

<p>Ideally if anything is currently working, the automatic <code>merge</code> would stop,
so the test for <code>currentaction</code> should be very early.  We should go
back to our initial worker example</p>

<p>```bash</p>

<h1>! /bin/bash</h1>

<h1>================================</h1>

<h1>=== Simple worker example</h1>

<h1>================================</h1>

<p>export ME=<code>basename $0</code>
export LOG=/root/log/${ME}<em>log.txt
export ERROR=/root/log/${ME}</em>error.txt
export START_TSS="<code>date +%Y%m%d-%H%M%S</code>"</p>

<p>export CURRENTACTION_FILE=./.temp/nodeinfo/currentaction.txt</p>

<p>mkdir -p /root/log/</p>

<p>exec 1>> ${LOG}
exec 2>> ${ERROR}</p>

<p>echo "${ME}: Start  ${START_TSS}" >> ${LOG}</p>

<p>export REPOS=<code>find /root/gitrepo/ -maxdepth 1 -mindepth 1</code>
for REPO in ${REPOS}; do</p>

<pre><code>pushd ${REPO}
    if [[ -e ${CURRENTACTION_FILE} ]]; then
        : #Don't do anything until the current action completes
    else
        git pull

        source bin/work/common/work.sh
    fi
popd
</code></pre>

<p>done</p>

<p>export FINISH_TSS="<code>date +%Y%m%d-%H%M%S</code>"
echo "${ME}: Finish ${FINISH_TSS}" >> ${LOG}</p>

<p>```</p>

<p>While the action is running you can either touch or copy
the current version into <code>currentaction</code>, and when
finished, delete or rename it.  Since everything
is running so fast, you probably don't need to do
an <code>flock</code> or even a second <code>[[ -e ]]</code> check.  Acquiring
the lock is simply:</p>

<p><code>``bash
export LOCAL_VERSION=</code>git rev-parse HEAD`
export CURRENTACTION_FILE=./.temp/nodeinfo/currentaction.txt
export CURRENTACTION_DIR="$( cd -P "$( dirname "${CURRENTACTION_FILE}" )" &amp;&amp; pwd )/"</p>

<p>mkdir -p CURRENTACTION_DIR
echo ${LOCAL_VERSION} > ${CURRENTACTION_FILE}</p>

<h1>Now officially own the current action</h1>

<p>```</p>

<h3>Dealing with different types and collections of servers</h3>

<p>All the above had a single type of machine watching any changes in the
repository and doing the same thing.  Although this does work if you
have either very few servers or are happy with a plethora of repositories
or branches, it would be good to have a way for many different types
and collections of servers to share a single repository.  We
will deal with that in the next blog.</p>

<h2>Summary</h2>

<p>Git can help automate IT with a very fast and effective PushMePullYou
model.  Hundreds of servers can be continually polling one or more
central Git servers to see if anything has changed and then act
upon those changes.  This provides history for any activity, a
very fast response time, and almost no load when nothing has changed.
Also, this approach enables bootstrapping and updating to use exactly
the same paths.</p>

<p>In all, it provides a superior foundation for IT automation in spite of
being so simple.</p>

<h2>References</h2>

<ul>
<li> Chef-Server Scalability

<ul>
<li><a href="http://lists.opscode.com/sympa/arc/chef/2012-01/msg00422.html">http://lists.opscode.com/sympa/arc/chef/2012-01/msg00422.html</a></li>
<li><a href="http://www.opscode.com/hosted-chef/">http://www.opscode.com/hosted-chef/</a></li>
</ul>
</li>
</ul>


<h2>Credit</h2>

<ul>
<li> Initial Llama for Push-Me-Pull-You Image credit: <a href='http://www.123rf.com/photo_11398752_llama-llove--two-llamas-kiss-their-necks-forming-a-heart-shape.html'>fiftyfootelvis / 123RF Stock Photo</a></li>
</ul>


<h2>Next</h2>

<p>Our next problem will be...</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Being a git about everything (IT)]]></title>
    <link href="http://markfussell.emenar.com/blog/git-about-everything-it/"/>
    <updated>2013-02-13T18:16:00-08:00</updated>
    <id>http://markfussell.emenar.com/blog/git-about-everything-it</id>
    <content type="html"><![CDATA[<p>This is the third in a series of using git as part of interesting solutions to problems.</p>

<p>The first is here: <a href="/blog/git-about-everything-intro/">Intro</a></p>

<h2>Leveraging git to help with IT tasks</h2>

<p>Doing IT for computers involves installing software, configuring things, doing backups, updates, etc.
Git can help out with a lot of this, with several big benefits.  Some examples:</p>

<ul>
<li> IT changes should always be version controlled in case of mistake.  Git and a couple patterns/tools makes that easy</li>
<li> Installing software can frequently be done with 'yum' and similar commands, but some software needs a package, and some times you want more direct 'this version' control.  An annexed git repository helps with that</li>
<li> Doing backups or other kinds of off-lining with an annexed repo is very simple and flexible</li>
<li> Git enables a PushMePullYou model that is very flexible and capable of driving hundreds of machines to do either (or both) very light and very heavy-weight operation</li>
</ul>


<p>Some of these things can be done with other tools, but I have found an annexed git repository to make doing these things easier and better</p>

<!-- more -->


<h2>IT Changes</h2>

<p>Any time you have a running computer you should be careful about making changes to it that you can't
easily revert.  The classic <code>rm -fr /</code> is amongst the most painful, but even a few simple tweaks of a
configuration file could cause you minutes or hours of pain if you do them wrong and can't just
start over.</p>

<p>There are many solutions to this problem:</p>

<ul>
<li> Create a '.bak' file</li>
<li> Create a '.datestamp' file of the file before you touch it</li>
<li> Copy the file into a backup folder</li>
<li> Rsync the directory somewhere else</li>
<li> Version control the whole directory structure</li>
<li> Have frequent backups of the whole machine</li>
</ul>


<p>Git can help with a number of different approaches, but
I will start with one approach that I find to be among the simplest, most-general, and most-beneficial.</p>

<h3>Copy 'changing' files into an IT repo</h3>

<p>To get a huge step up on the problem, we are going to allocate an annexed git repository to 'being for IT'.
And further we will have every computer out there have its own directory within the repository.  A computer's
directory is simply it's name (e.g. <code>hostname</code> in unix systems).  This give us a structure like this:</p>

<ul>
<li> IT-Repository (e.g. giteveryrepo2)

<ul>
<li>it/comp/

<ul>
<li>{computer1}

<ul>
<li>Any files you want with their full path as if under '/'</li>
</ul>
</li>
<li>{computer2}</li>
<li>{computer3}</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>As long as computers have unique names, we can freely work with any computer's files without collision
with another computer.  And because Git is distributed, we can work on files <em>on</em> any computer and
merging is very unlikely to be an issue.</p>

<p>An example of an IT repository is at:</p>

<ul>
<li> <a href="https://github.com/markfussell/giteveryrepo2">https://github.com/markfussell/giteveryrepo2</a></li>
</ul>


<p>and more specifically, the directory for computers is at:</p>

<ul>
<li> <a href="https://github.com/markfussell/giteveryrepo2/tree/master/it/comp">https://github.com/markfussell/giteveryrepo2/tree/master/it/comp</a></li>
</ul>


<p>As of this writing, there are two computers listed.  Both are EC2 machines, so they have the standard
patterns for AWS machine naming.</p>

<ul>
<li> domu-12-31-39-0e-e2-53</li>
<li> ip-10-28-81-39</li>
</ul>


<p>On each machine, I ran a script to copy from system files to the repository.  The scripts are in repo2/bin/copy
so a sequence looks like this</p>

<p><code>bash
./bin/copy/copyFromSystemFile.sh /var/www/html/index.html
git pull; git add it; git commit -m "Server2"; git push
</code></p>

<p>Again, because the machines have their own directories, merging should be automatic no matter what order
or how much time-overlap occurs.  Two notes:</p>

<ul>
<li> The machine names are lower-cased to avoid confusing behavior.  I strongly recommend keeping directories all lowercase</li>
<li> If you have any binary files you are copying from the machine, you need to remember to 'deflateAll' the repository before the git line.</li>
</ul>


<h4>Flattening Computers next to each other</h4>

<p>An advantage of flattening out computers next to each other is you can compare them:</p>

<p>```bash
diff -r it/comp/*
diff -r it/comp/domu-12-31-39-0e-e2-53/var/www/html/index.html it/comp/ip-10-28-81-39/var/www/html/index.html
1c1</p>

<h2>&lt; domU-12-31-39-0E-E2-53</h2>

<blockquote><p>ip-10-28-81-39
```</p></blockquote>

<p>In this case it should be apparent that the files have the <code>hostname</code> inside them, so that is what is changing.</p>

<p>So you can compare the history of a particular computer by looking through git history, and you can compare two
different computers to each other by comparing the different directories that each computer owns.</p>

<h3>Copy files from the IT repo</h3>

<p>There is also a script to copy from the IT repo to a particular system location.  This is a little more
risky, but can be useful at times.  Effectively this would roll back your local change if you didn't touch
the repo from anywhere else, and it can be used to document a proposed change before applying it.  This
leads to more of 'git for IT automation' though, which I will cover later.</p>

<h2>Software installation</h2>

<p>Software installation is the major part of machine provisioning.  Unless you have a fully-baked AMI or
really are happy with the factory install, you need to add more software to make your computer useful.
Installing software can frequently be done with 'yum' and similar commands, but some software needs a
package install (e.g. with 'rpm') and some times you want more direct control of the version installed.</p>

<p>Fetching RPMs from the web at install time is a major source of automation failure and annoyance.  Sites
go down and if you are fetching from many different sites, you could have a computer with 9 out of 10
things installed on it.  Generally I try to make machine provisioning <em>all or nothing</em> : Either everything
gets installed properly or I just wipe the machine and try again.  Primordial scripted installs are
easy to write (no human interaction), but trying to recover from a partial install is
a way to become very unproductive (potentially very angry too).  Additionally, some sites can be slow
and you don't want to be penalized by that for every computer you provision.</p>

<p>A solution is to store RPMs and the like in an Annexed Git repository.  S3 is both very fast and very reliable,
with an SLA that 'guarantees' four nines (99.99%) of uptime a year, which is &lt; 1-hour of downtime.  If
all install packages are stored in S3, provisioning would be almost solely dependent on S3, so it would
always be an <em>all (99.99%) or nothing (00.01%)</em> provisioning model.</p>

<h3>Example and idioms</h3>

<p>You could put RPMs and the like anywhere you want since the annexing is orthogonal to file location,
but I conventionally put things in 'it/resource'</p>

<ul>
<li> <a href="https://github.com/markfussell/giteveryrepo2/tree/master/it/resource">https://github.com/markfussell/giteveryrepo2/tree/master/it/resource</a></li>
</ul>


<p>An annexed repo will also tend to have a '.temp' directory in the '.gitignore', so that is a handy place
to do temporary work without risk of colliding with changes to the repository itself.  When you inflate a file,
it is 'dirty' / changed so it could conflict with a subsequent merge and you would have to either deflate
or 'git checkout' the file/s to clean up.  Worse would be an archive that expanded into a directory structure,
so it is generally nice to work in '.temp' where these aspects don't cause issues.</p>

<p>In giteveryrepo2 there is a Java-7 rpm, which can be installed as follows:</p>

<p><code>bash
pushd /root/gitrepo/giteveryrepo2
git pull
mkdir -p .temp
cp it/resource/jdk-7u13-linux-x64.rpm .temp
./bin/inflatePaths.sh .temp/
rpm -i .temp/jdk-7u13-linux-x64.rpm
/usr/java/default/bin/java -version
</code></p>

<p>Similarly for installing Apache Tomcat:</p>

<p>```bash
pushd /root/gitrepo/giteveryrepo2
git pull
mkdir -p .temp</p>

<p>cp it/resource/apache-tomcat-7.0.35.tar.gz .temp
./bin/inflatePaths.sh .temp/</p>

<p>pushd .temp
tar -xvf apache-tomcat-7.0.35.tar.gz
mv apache-tomcat-7.0.35 /usr/share
ln -s /usr/share/apache-tomcat-7.0.35 /usr/share/tomcat7</p>

<p>popd
```</p>

<p>And just to show the install actually works.</p>

<p>```bash
cat > /etc/httpd/conf.d/proxy_ajp.conf &lt;&lt;EOS
ProxyPass /tomcat/ ajp://localhost:8009/
EOS</p>

<p>service httpd restart</p>

<p>/usr/share/tomcat7/bin/startup.sh</p>

<p>```</p>

<p>should bring up a standard Tomcat welcome screen.</p>

<p><img width="743" height="352" src="http://markfussell.emenar.com/images/git-about-everything-it/TomcatScreenshot1.png" /></p>

<h2>Backups</h2>

<p>Backing up database dumps, logs, etc. is as easy as copying them into an appropriate folder within the repo and deflating.
This is similar to IT changes: you just put things in a reasonable location on the machine and then</p>

<p><code>bash
./bin/copy/copyFromSystemFile.sh {reasonable-location}
./deflateAll.sh
git pull; git add it; git commit -m "Backup of `hostname` as of `date`"; git push
</code></p>

<p>This bascially works fine, with two minor issues:</p>

<ul>
<li> You probably want the backup repo to be different from the IT repo even though their layout would be similar.  You wouldn't want a bunch of 'backup' activity happening on something you are using for IT maintenance or even machine provisioning.  The rhythm and types of commits would be inconsistent.</li>
<li> At some point, you are going to have <em>too many backups</em> and you need to get rid of some noise.  This is 'cleaning the annex'</li>
</ul>


<h3>Cleaning the Annex</h3>

<p>By default the Annex has a copy of every 'big' file you have ever annexed in the history of the repository.  Given S3 doesn't
cost very much, this may never be an issue for you.  But if it is an issue, you need to throw out the garbage.  There
are fancier versions of this process but the simplest involves:</p>

<ul>
<li> Removing any files you no longer want to keep around from the current HEAD of git.  So if you have 1000 backups in the filesystem and want less than that, delete and commit the ones to get rid of.  Ideally this is completely automated as part of normal activities (e.g. new backups cause some older backups to be deleted)</li>
<li> Inflating the whole repository from the current Annex (Annex2a)</li>
<li> Changing the current Annex to point to a new empty Annex (Annex2b)</li>
<li> Deflating the whole repository to the new Annex (Annex2b)</li>
<li> Committing the change of Annex into git</li>
</ul>


<p>Note that at this point you have the option to delete the old annex or not.  If you leave it around you have a 'shiny new'
cleaner Annex2b which can make some things go faster, but if you ever want to go back to an old version it will work too.
Because the annex information is in the version, checking out an old version will automatically switch the annex to use
with it too.</p>

<p>The 'moveToNewAnnex.sh' script looks like this:</p>

<p>```bash
die () {</p>

<pre><code>echo &gt;&amp;2 "$@"
exit 1
</code></pre>

<p>}</p>

<p>[ "$#" -eq 1 ] || die "1 argument required, $# provided"
echo $1 | grep -E -q '<sup>s3://'</sup> || die "S3 bucket URL required, $1 provided"</p>

<p>export NEW_ANNEX="$1"</p>

<p>./bin/inflatePaths.sh .</p>

<p>echo $NEW_ANNEX > s3info/s3annex_config.txt</p>

<p>./deflateAll.sh
```</p>

<p>An example using the contents of 'giteveryrepo2':</p>

<p>```bash</p>

<h1>Remove the temp directory since we don't want to annex it</h1>

<p>rm -fr .temp</p>

<p>./bin/moveToNewAnnex.sh s3://emenar.com/gitevery/giteveryrepo2v2/
git status
git diff s3info/s3annex_config.txt</p>

<h1>Go back to previous annex again</h1>

<p>git checkout -- s3info/s3annex_config.txt
git status
```</p>

<p>It would be 'more optimal' to go directly from S3 -> S3, but the above works well enough if you have decent bandwidth
and especially if you are within EC2.</p>

<h2>Summary</h2>

<p>Annexed git repositories can make certain IT activities much easier, more reliable, or more powerful (diff two machines' configurations).
The above wasn't showing IT automation, but it shows a valuable tool for IT automation as well as more manual interactions.</p>

<h2>Next</h2>

<p>Our next problem will be...</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Being a git about everything (Annexing)]]></title>
    <link href="http://markfussell.emenar.com/blog/git-about-everything-annex/"/>
    <updated>2013-01-13T18:16:00-08:00</updated>
    <id>http://markfussell.emenar.com/blog/git-about-everything-annex</id>
    <content type="html"><![CDATA[<p>This is the second in a series of using git as part of interesting solutions to problems.</p>

<p>The first is here: <a href="/blog/git-about-everything-intro/">Intro</a></p>

<h2>Dealing with Binary Files</h2>

<p>As mentioned in the first posting, git and similar DVCS have issues with binary files.  Adding 100s of 10MB files, or 100s of
versions of 10MB files will produce gigabytes worth of data that must be cloned by everyone using the repository.  How do we avoid this?</p>

<p>There are a number of solutions in this space out there with differing characteristics, but the core approach is usually similar:
"Don't store it in git".  Instead we want to record enough information to retrieve the binary files from somewhere else.</p>

<!-- more -->


<h2>Record Enough of the Binary File</h2>

<p>What is
enough information?  How about 160 bits!  Using SHA1 or any similar hash, we can identify the contents of any file.  To add
a bit of consistency and readability, we will make this hex-based, so we get 40 characters.  And to make it a little clearer what
hash was used and what this string is, we add 'sha1_' to the front and '.blob' to the end.  Now our 10MB file has become 50 to 52 bytes depending on
human-entered white spaces.  For example:</p>

<ul>
<li> sha1_8ac02ee34b94461fed19320d789f251e6a2a6796.blob</li>
<li> SHA1-Hash = 8ac02ee34b94461fed19320d789f251e6a2a6796</li>
<li> Google test: <a href="http://www.google.com/search?q=8ac02ee34b94461fed19320d789f251e6a2a6796">http://www.google.com/search?q=8ac02ee34b94461fed19320d789f251e6a2a6796</a></li>
</ul>


<p>And we have confirmed that the hash is enough to identify that file's content (and the file itself if it has a unique name).</p>

<p>Storing tens of thousands of 50Byte files is still under a few megabytes, so that part is good.</p>

<h2>Put the content into the Cloud (or similar)</h2>

<p>Where should we store the actual content?  Pretty much anywhere we want if things are simple (only a few files at a time to store and retrieve, need
only local access), but
if we want to store and retrieve thousands of files from anywhere rapidly things get nastier.  For example, there are projects that try to
hide the process of file-contents being moved elsewhere using git smudge/clean filters.  Unfortunately this make the process all of: sequential, heavy, and
perpetual.  We would have similar major issues if we moved the content via some other sequential approach that was not super-fast.  And
git is meant to be highly distributed, so our approach needs to work for all the 'clones' of a repository.  And finally, we don't
want to break our bank over this.</p>

<p>The solution?  Amazon S3 or similar.  Amazon S3 is:</p>

<ul>
<li> Relatively inexpensive for what it does (about a dime a month for 1GB)</li>
<li> Highly distributed</li>
<li> Reasonably fast in network transfers</li>
<li> Incredibly fast as you throw more workers at the network transfers</li>
<li> Blazingly fast if you throw many workers from EC2 at it</li>
</ul>


<p>So if we want to move files from our Git repository into S3, how do we do that?  This isn't really a git issue at all if we do
it before 'add' and after 'pull'.  It becomes
a simple filesystem-to-S3 issue and s3cmd <a href="http://s3tools.org/s3cmd">http://s3tools.org/s3cmd</a> is a very well trusted tool for this.
Add in the parallel patch <a href="https://github.com/pcorliss/s3cmd-modification">https://github.com/pcorliss/s3cmd-modification</a> and
you can have any numbers of workers running.  We want to be a 'git' about everything, but we can solve this issue independently
of git and combine the two.  The only large remaining issues are the actual processes of:</p>

<ul>
<li> 'deflating': moving the content of a file somewhere and leaving a content-hash in its place.</li>
<li> 'inflating': replacing a content-hash with the actual content</li>
</ul>


<p>A highly annex-augmented version of s3cmd is here <a href="https://github.com/markfussell/s3annex">https://github.com/markfussell/s3annex</a>
and was done while working at <a href="http://www.rumblegames.com">Rumble</a> where we needed to move gigabytes of high-quality art assets around
as part of the build-deploy pipeline.</p>

<h2>Working with S3 Annexed Git Repositories</h2>

<p>The three things you need for an S3-Annexed repository are:</p>

<ul>
<li> A git repository</li>
<li> An S3 bucket to put content into</li>
<li> The augmented s3cmd from here <a href="https://github.com/markfussell/s3annex">https://github.com/markfussell/s3annex</a></li>
</ul>


<p>A repository that is paired with an S3 bucket is located here:</p>

<ul>
<li> <a href="https://github.com/markfussell/giteveryrepo1">https://github.com/markfussell/giteveryrepo1</a></li>
</ul>


<p>You can clone that repository and get a working annexed-repository and some example content (without write permission).  The repository is tiny but
grants access to several megabytes worth of images.</p>

<h3>Configuration</h3>

<p>The configuration of the Annex is located in 's3info':</p>

<ul>
<li> blob_includes.txt &mdash; The file extensions you want annexed</li>
<li> s3annex_config.txt &mdash; The location of the annex</li>
<li> s3cmd_config.txt &mdash; Some s3cmd configuration, but most importantly the access/secret</li>
<li> s3worker_config.txt &mdash; The number of workers used for annexing</li>
</ul>


<h4>blob_includes</h4>

<p>The blob_includes should be updated with any new file types you want annexed.  To make things fast and simple, annexing is
based on file extensions not size or other properties.  It would be nice if this was more automatic, but being explicit
was simple and very visible.</p>

<h4>s3annex_config</h4>

<p>This is simply the location of the annex.  This can change over time as a quick way to do 'garbage collection'
but normally it stays the same.  Because of the content-based approach, you can share annexes across many
repositories.</p>

<h4>s3cmd_config</h4>

<p>The s3cmd configuration including access credentials.  If you don't want access credentials in the repository itself, you could take out the s3cmd_config file and it should use your defaults (you may need to tweak a couple scripts).</p>

<h4>s3worker_config</h4>

<p>The number of s3workers to run at a time.  More workers will make the S3 combined throughput faster: this should be 100 or more on an EC2 instance.</p>

<h3>Working commands</h3>

<p>All the commands expect to be run from the root of the git repository.  The main working commands are:</p>

<ul>
<li> bin/deflatePaths.sh &mdash; Move the contents of files into the Annex and replace them with a hash stub/reference</li>
<li> bin/inflatePaths.sh &mdash; Put the proper contents of the files into the filesystem based on their hash stub/reference</li>
</ul>


<p>Because annexed repositories should always be fully deflated before committing, there is a command in the root of the directory to
remind people of this:</p>

<ul>
<li> deflateAll.sh &mdash; Visible reminder and simple equivalent to 'bin/deflatePaths.sh .'</li>
</ul>


<p>These commands are all you need for annexing proper.  To make it easier to see the Annex itself, there is also an 'lsAnnex.sh'
script which is used below.</p>

<h3>Annex (S3) Layout</h3>

<p>The layout of the Annex within the S3 bucket is either:</p>

<ul>
<li> Flat in a single bucket plus path prefix</li>
<li> Hierarchical based on some amount of leading hash digits</li>
</ul>


<p>The completely flat version is the simpler and truer representation.
The hierarchy simply allows multiple threads to get listings of the annex files in parallel, which
matters for performance when you have thousands of files within the annex.</p>

<h4>Annex listing</h4>

<p>You can see the contents of the annex with:</p>

<p><code>bash
./bin/lsAnnex.sh
</code></p>

<p>For the example repository, this gives:</p>

<p><code>bash
2013-01-22 00:04   4742594   s3://emenar.com/gitevery/giteveryrepo1/sha1_02bf4b647b623dac68e1913b8d3494856041047c.blob
2013-01-22 00:11   4742594   s3://emenar.com/gitevery/giteveryrepo1/sha1_02bf4b647b623dac68e1913b8d3494856041047c.blob__.jpg
2013-01-22 00:10   2679517   s3://emenar.com/gitevery/giteveryrepo1/sha1_2abd18dfa4510e1dfc72f643bff3639b42f2aa32.blob
2013-01-22 00:15   2679517   s3://emenar.com/gitevery/giteveryrepo1/sha1_2abd18dfa4510e1dfc72f643bff3639b42f2aa32.blob__.jpg
</code></p>

<p>As you can see, in the annex are files that start with 'sha1_' and end in '.blob' or '.blob__.xxx' where '.xxx' is a proper MIME extension.
The reason for the MIME extension is just that it can be useful to see or directly retrieve the content
with proper interpretation.  The normal Annex behavior only uses the '.blob' version.</p>

<p>The names of the files in the Annex match the 'sha1' hash of the contents of the file.  So 'sha1_55b15eb3ac72351249125a3de7a81aee2bda6a2a.blob__.jpg'
has a sha1 hash of '55b15eb3ac72351249125a3de7a81aee2bda6a2a'.  It is impossible for files to collide unless they are exactly the same
content, so a completely flat representation is correct and simple.</p>

<h3>Example Walk-through</h3>

<p>If you cloned the example repository:</p>

<ul>
<li> <a href="https://github.com/markfussell/giteveryrepo1">https://github.com/markfussell/giteveryrepo1</a></li>
</ul>


<p>you should have something less than a megabyte, but it represents more than 100MB of image files (30 images of 3MB each).  But all the image
files are stubbed out with just the content hash inside.</p>

<p>To see example details of these stubbed/annexed files, look inside any of the jpg files in <code>image/album</code></p>

<p><code>bash
echo -n "content: "; cat image/album/GitEverythingAlbum_01.jpg ; echo
</code></p>

<p>This returns the blob identifier:
<code>bash
content: sha1_55b15eb3ac72351249125a3de7a81aee2bda6a2a.blob
</code></p>

<h4>Inflating a file</h4>

<p>To inflate a file, run <code>./bin/inflatePaths.sh</code> with the specific file path:</p>

<p><code>bash
./bin/inflatePaths.sh image/album/GitEverythingAlbum_01.jpg
</code></p>

<p>And you will get some feedback:
<code>bash
INFO: Compiling list of local files for 'file://image/album/GitEverythingAlbum_01.jpg', 'GitEverythingAlbum_01.jpg', 'image/album/GitEverythingAlbum_01.jpg'
INFO: Applying --exclude/--include
INFO: Retrieving list of remote files for s3://emenar.com/gitevery/giteveryrepo1/ ...
INFO: Summary: 1 local files to fetch, 60 remote files present
INFO: Inflating[1] from S3 (1) (sha1_55b15eb3ac72351249125a3de7a81aee2bda6a2a.blob): image/album/GitEverythingAlbum_01.jpg &lt;- s3://emenar.com/gitevery/giteveryrepo1/sha1_55b15eb3ac72351249125a3de7a81aee2bda6a2a.blob
s3://emenar.com/gitevery/giteveryrepo1/sha1_55b15eb3ac72351249125a3de7a81aee2bda6a2a.blob -&gt; image/album/GitEverythingAlbum_01.jpg  [1 of 1]
 4960530 of 4960530   100% in    7s   633.43 kB/s  done
</code></p>

<p>Now you should have a normal file and can view a picture of Nob Hill.</p>

<p>Note that <code>git status</code> will now show a change.  The approach here is not to hide or conflate annexing within git (e.g. being part of smudge),
but to create something that when combined with normal git makes git even more useful.</p>

<h4>Deflating a file</h4>

<p>Since we are in a git repository, we could simply do a reset to get the file back to it's original content
(it is already annexed) but it is much safer to always
'deflate' in case the contents changed vs. being the same as the original commit.</p>

<p>To deflate a single file you run <code>./bin/deflatePaths.shs</code> with the specific file path:</p>

<p><code>bash
./bin/deflatePaths.sh image/album/GitEverythingAlbum_01.jpg
</code></p>

<p>Because the contents are the same, you should see this:</p>

<p><code>bash
INFO: Compiling list of local files for 'file://image/album/GitEverythingAlbum_01.jpg', 'GitEverythingAlbum_01.jpg', 'image/album/GitEverythingAlbum_01.jpg'
INFO: Applying --exclude/--include
INFO: Retrieving list of remote files for s3://emenar.com/gitevery/giteveryrepo1/ ...
INFO: Summary: 1 local files to upload, 60 remote files already present
INFO: Skipped    (1) (sha1_55b15eb3ac72351249125a3de7a81aee2bda6a2a.blob): image/album/GitEverythingAlbum_01.jpg -&gt; s3://emenar.com/gitevery/giteveryrepo1/sha1_55b15eb3ac72351249125a3de7a81aee2bda6a2a.blob
INFO: Skipped    (1) (sha1_55b15eb3ac72351249125a3de7a81aee2bda6a2a.blob__.jpg): image/album/GitEverythingAlbum_01.jpg -&gt; s3://emenar.com/gitevery/giteveryrepo1/sha1_55b15eb3ac72351249125a3de7a81aee2bda6a2a.blob__.jpg
</code></p>

<p>The original annexing of the file did upload two files, and looked like this:</p>

<p><code>bash
INFO: Upload     (1): ./image/album/GitEverythingAlbum_01.jpg -&gt; s3://emenar.com/gitevery/giteveryrepo1/sha1_55b15eb3ac72351249125a3de7a81aee2bda6a2a.blob
File './image/album/GitEverythingAlbum_01.jpg' started [1 of 30]
...
File './image/album/GitEverythingAlbum_01.jpg' stored as 's3://emenar.com/gitevery/giteveryrepo1/sha1_55b15eb3ac72351249125a3de7a81aee2bda6a2a.blob' (4960530 bytes ...) [1 of 30]
</code></p>

<h4>Inflating Many Files</h4>

<p>The <code>inflate</code> and <code>deflate</code> scripts accept paths so you can inflate the whole album with:</p>

<p><code>bash
./bin/inflatePaths.sh image/album
</code></p>

<p>or even inflate the whole repository:</p>

<p><code>bash
./bin/inflatePaths.sh .
</code></p>

<p>With the default settings, this will launch 10 workers doing 10 files at a time in total.  How long
the whole 100MB download takes will almost likely depend on your maximum bandwidth, but if not,
just change the number of workers to a larger number.  S3 is very supportive of many requests by
different workers.</p>

<h3>EC2 Walkthrough</h3>

<p>On EC2 the performance numbers are pretty amazing, so I created a simple CloudFormation so people
can test it out.  The CloudFormation is here:</p>

<ul>
<li> <a href="https://s3.amazonaws.com/emenar.com/gitevery/aws/cloudformation/GitEverythingServer1.template">https://s3.amazonaws.com/emenar.com/gitevery/aws/cloudformation/GitEverythingServer1.template</a></li>
</ul>


<p>and it can be run by going to AWS CloudFormation and just giving it one of your keypairs:</p>

<ul>
<li> <a href="https://console.aws.amazon.com/cloudformation/">https://console.aws.amazon.com/cloudformation/</a></li>
</ul>


<h4>EC2 Performance</h4>

<p>After the instance launches, SSH in, 'sudo su -', and change to the repository:</p>

<ul>
<li> /root/gitrepo/giteveryrepo1</li>
</ul>


<p>run <code>inflatePaths</code> with the standard 10 workers</p>

<p><code>bash
time ./bin/inflatePaths.sh image
</code></p>

<p>```bash
INFO: Compiling list of local files for 'file://image', 'image', 'image'
INFO: Applying --exclude/--include
INFO: Retrieving list of remote files for s3://emenar.com/gitevery/giteveryrepo1/ ...
INFO: Summary: 30 local files to fetch, 60 remote files present
INFO: Inflating[1] from S3 (1) (sha1_55b15eb3ac72351249125a3de7a81aee2bda6a2a.blob): image/album/GitEverythingAlbum_01.jpg &lt;- s3://emenar.com/gitevery/giteveryrepo1/sha1_55b15eb3ac72351249125a3de7a81aee2bda6a2a.blob
File s3://emenar.com/gitevery/giteveryrepo1/sha1_55b15eb3ac72351249125a3de7a81aee2bda6a2a.blob started [1 of 30]
INFO: Receiving file 'image/album/GitEverythingAlbum_01.jpg', please
...
File s3://emenar.com/gitevery/giteveryrepo1/sha1_801a329248a9cbe48b512f2a75179437382dba02.blob saved as 'image/album/GitEverythingAlbum_21.jpg' (4194668 bytes in 3.1 seconds, 1330.45 kB/s)
File s3://emenar.com/gitevery/giteveryrepo1/sha1_7d5d055068a9b7e268e21279770f03a5e8c6c9d3.blob saved as 'image/album/GitEverythingAlbum_22.jpg' (4576223 bytes in 3.0 seconds, 1477.96 kB/s)
File s3://emenar.com/gitevery/giteveryrepo1/sha1_02bf4b647b623dac68e1913b8d3494856041047c.blob saved as 'image/album/GitEverythingAlbum_25.jpg' (4742594 bytes in 2.6 seconds, 1777.42 kB/s)
File s3://emenar.com/gitevery/giteveryrepo1/sha1_856984959467b2427c2a4e9ade642a3d3c26b0fd.blob saved as 'image/album/GitEverythingAlbum_20.jpg' (4208680 bytes in 4.2 seconds, 971.63 kB/s)
File s3://emenar.com/gitevery/giteveryrepo1/sha1_7cb2e0318459e276dc4b65987bbe8bd6021357df.blob saved as 'image/album/GitEverythingAlbum_28.jpg' (3559585 bytes in 2.2 seconds, 1559.41 kB/s)
File s3://emenar.com/gitevery/giteveryrepo1/sha1_b855966214caa10638f76a3aba6b6df7b0caffca.blob saved as 'image/album/GitEverythingAlbum_29.jpg' (3820335 bytes in 2.2 seconds, 1718.09 kB/s)
File s3://emenar.com/gitevery/giteveryrepo1/sha1_2abd18dfa4510e1dfc72f643bff3639b42f2aa32.blob saved as 'image/album/GitEverythingAlbum_30.jpg' (2679517 bytes in 2.1 seconds, 1229.88 kB/s)
File s3://emenar.com/gitevery/giteveryrepo1/sha1_fc322e938362e0c40ccf5e09789a1cb6b6995882.blob saved as 'image/album/GitEverythingAlbum_26.jpg' (3909187 bytes in 3.0 seconds, 1285.67 kB/s)
File s3://emenar.com/gitevery/giteveryrepo1/sha1_fd63032739c810743521ad1de0546d67b13b4357.blob saved as 'image/album/GitEverythingAlbum_27.jpg' (3465440 bytes in 2.9 seconds, 1157.55 kB/s)
File s3://emenar.com/gitevery/giteveryrepo1/sha1_798788e370f243a2a38f91bf8979fd46070a4ae2.blob saved as 'image/album/GitEverythingAlbum_24.jpg' (3461363 bytes in 5.3 seconds, 634.12 kB/s)</p>

<p>real    0m10.372s
user    0m4.311s
sys 0m2.948s
```</p>

<p>So it took 10 seconds to download 100MB.  That is 80Mb/s on an 'm1.small' which is a pretty nice number to work with.  Changing to 100 workers doesn't make much difference (about 9 seconds),
so this is pretty much the saturation of 'm1.small' to S3 performance.</p>

<p>Larger instance types can perform even better.  During a particularly grueling annexing, the performance numbers looked like this:</p>

<p><img width="743" height="352" src="http://markfussell.emenar.com/images/git-about-everything-annex/ec2performance2.png" /></p>

<p>That is getting 200MB/s <em>minimum</em> and passing 1GB/s at peaks.</p>

<h2>Summary -- Annexing makes Git good at binary files</h2>

<p>By using S3 as an Annex for binary files, we can make 'git' be exceptionally good at dealing with large amount of binary content.  Git simply
manages the history of 50-byte annex-stub text files, and git is very fast and efficient at doing that.  The annex-enhanced s3cmd can run many
workers to upload/deflate and download/inflate the binary files to and from S3.  Especially on EC2 the performance numbers can be super-fast:</p>

<ul>
<li> Clone repository &lt; 5 seconds</li>
<li> Inflate 100MB of files &lt; 10 seconds</li>
</ul>


<p>By combining git with a powerful annex solution, working with lots of version-controlled information
of any size has become all of: very simple, very fast, very distributable, and very inexpensive.</p>

<h3>Alternatives</h3>

<p>There are some alternative approaches out there, which I should mention.  Some I tried and they didn't perform well
enough.  Others didn't match the needs we had at Rumble or my needs outside of Rumble.  But they are interesting
projects:</p>

<ul>
<li> <a href="http://git-annex.branchable.com/">http://git-annex.branchable.com/</a></li>
<li> <a href="https://github.com/schacon/git-media">https://github.com/schacon/git-media</a></li>
</ul>


<h3>Next</h3>

<p>Our next problem will be <a href="/blog/git-about-everything-it/">IT</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Being a git about everything (Intro)]]></title>
    <link href="http://markfussell.emenar.com/blog/git-about-everything-intro/"/>
    <updated>2013-01-11T18:16:00-08:00</updated>
    <id>http://markfussell.emenar.com/blog/git-about-everything-intro</id>
    <content type="html"><![CDATA[<p>There are times when a new technology comes along, that at first appears to be pretty similar to
existing technology, but certain characteristics make for radically different or just nicely new solutions.
A recent example of this is 'git' and similar distributed version control systems (DVCS).  They may
at first appear to be an interesting version of centralized version/content management systems, but
they are really much more... a core piece of technology useful for many things.</p>

<p>This is a series about how to use git to solve many different problems, some obvious and some more unusual.
I hope a few of them are interesting to readers.</p>

<!-- more -->


<p>There are alternative DVCSs but I am not going to compare or translate examples... at least not on a first pass.
Git was created by Linus Torvalds in 2005.  It was initially quite 'raw' and still maintains much of that rawness,
but other tools (e.g. github) and general improvements have made it more accessible.  For source-code control, git has some big wins
in collaboration and offline work compared to SVN, Perforce, and the like.  Whether these are worth some trade-offs depends on your team
and company... but that is a different topic.</p>

<h2>Git Syntax / Overview</h2>

<p>You can learn more about git somewhere like github:</p>

<ul>
<li> <a href="http://learn.github.com/p/intro.html">http://learn.github.com/p/intro.html</a></li>
</ul>


<p>Git has a lot of commands, but around seven are core to standard git flows:</p>

<ul>
<li> clone &mdash; copy a repository from a remote location</li>
<li> fetch &mdash; get updates from a remote repository</li>
<li> merge &mdash; merge changes from one branch into the current branch</li>
<li> pull &mdash; 'fetch' and then 'merge'</li>
<li> add &mdash; add changes to the commit stage of the current branch</li>
<li> commit &mdash; commit the changes into the current branch</li>
<li> push &mdash; try to make a remote branch look like the current branch</li>
</ul>


<p>Actually of those seven 'pull' basically replaces/combines two of them, so you get
down to about five with a core loop like this:</p>

<ul>
<li> clone

<ul>
<li>pull</li>
<li>make changes (if needed)

<ul>
<li>add</li>
<li>commit</li>
<li>push</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>Of these commands only 'push' can fail due to timing.  'push' is transactional
and you can be contending with other people or machines that pushed to the same branch
as you between when you pulled and you pushed.  Merging can fail, but that
represents some actual file-level conflict vs. a timing issue (someone beat you to the 'push').
A proper 'commit' can't fail because it is local.</p>

<h3>Repositories and branches</h3>

<p>At least to begin, all these solutions will use a standard centralized repository approach
and generally not use branches.  So you don't have to worry about git's ability to
deal with many remotes and which branch a given git repository is using.  By default
at any current moment there is only</p>

<ul>
<li> local: 'master' <-> remote: origin/master</li>
</ul>


<p>where things get interesting because there could be 100 different machines each with their own 'local'.</p>

<h3>Problems with git / DVCS</h3>

<p>The biggest issue with git and similar DVCSs is that their model doesn't work well for large amounts of
binary assets.  Large amounts of binary assets could occur either because there are a large number of
assets available at any time but only a subset are needed (so with Perforce or SVN many people would not
check out those directories) or more commonly, a modest number of binary assets are frequently
changing.  Because a git repository contains all assets throughout time and to work with a git repository
you clone the whole thing, having a large amount of binary assets punishes everyone.</p>

<p>What is a large amount?  That depends on the circumstances, but generally passing 100MB can start to
become painful depending on the purpose git is being used for.  Having 1GB of textual files in
a single git repository (even over time) is an unusual thing.  GBs of images is common.</p>

<p>So our first problems and solution is going to have to deal with this critical issue.</p>

<p>Enter the annex: <a href="/blog/git-about-everything-annex/">Annex</a></p>
]]></content>
  </entry>
  
</feed>
