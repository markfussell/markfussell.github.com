<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Git | Polyglot]]></title>
  <link href="http://markfussell.emenar.com/blog/categories/git/atom.xml" rel="self"/>
  <link href="http://markfussell.emenar.com/"/>
  <updated>2015-10-15T12:22:44-07:00</updated>
  <id>http://markfussell.emenar.com/</id>
  <author>
    <name><![CDATA[Mark Fussell]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[ADD Stack [Part-3]]]></title>
    <link href="http://markfussell.emenar.com/blog/addstack-3/"/>
    <updated>2015-10-15T02:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/addstack-3</id>
    <content type="html"><![CDATA[<p>This is the second series describing a radically more productive development and delivery environment.  The
first article is here: <a href="/blog/addstack-1/">Intro</a> and described the truth and lies about developing software.
The second article dealt with 'Testing'.</p>

<h2>Grails, Groovy, and Java</h2>

<p>The primary stack I believe is a "Best Practice" is Grails.  It is in it's third full generation, and with
each generation it gets better, easier, more powerful, and more 'aligned'.  This last part is not that
common with frameworks.  A lot of people start writing a framework and it does more and more.  With more and more
code.  The Grails team has been great at 'pruning' and 'aligning' with Spring.</p>

<p>Using Spring alone is certainly a reasonable practice.  The problem is people tend to use Spring wrong.  I don't
know why.  Either they don't read the tutorials... or they get confused and a deadline is approaching... or they
are cowboys (and cowgirls) that wander off into new territory of abuse to the tools that are in front of them.
I used Grails at a company that committed to Spring.  So I simply switched to Spring and my code was simple,
functional, well-tested, and clean.  But the rest of the code base was a complete mess.  So there is nothing
wrong with Spring but it is harder to use properly than Grails.</p>

<p>Grails is 'opinionated' and has plenty of examples to show you these opinions.  We can see it in the 'petclinic'
example.</p>

<h3>Build with Gradle</h3>

<p>Grails uses Gradle to build the project.  It used to have it's own system, but it pruned that away when Gradle
became stable and capable.  Gradle won a war against other build systems so Grails honored the winner.</p>

<p><img src="http://markfussell.emenar.com/images/addstack-3/addstack3_grails1.png" /></p>

<h3>Align with Spring Boot</h3>

<!--more-->


<p>Spring Boot (<a href="http://projects.spring.io/spring-boot/">http://projects.spring.io/spring-boot/</a>)
is a relatively recent effort to 'default' a lot of the flexibility within Spring.  So Grails
is now leveraging that effort.  Developing with Grails vs. Spring is becoming just a 'small' step up
conceptually so it should be easier for people to 'level up' and also 'wander down' depending on the needs
of the project.</p>

<p><img src="http://markfussell.emenar.com/images/addstack-3/addstack3_grails2.png" /></p>

<h3>Align with Hibernate, but allow others (include NoSQL)</h3>

<p>Grails has always used Hibernate as the primary database mapping system but it is actually capable of mapping
through GORM to other systems.  Some capabilities go away but basic schema and query capability (CRUD)
is always there.  And depending on the product, some higher level query capabilities may also be present.</p>

<h3>Use modern testing frameworks</h3>

<p>As mentioned before: 'geb' and 'spock' are the default testing frameworks included, along with the default
phantomjsdriver and selenium-htmlunit-driver (headless).  This is out-of-the-box, and other testing frameworks
could be used instead or in addition, but there would have to be a compelling reason for it.</p>

<h3>Modern asset pipeline</h3>

<p>Asset management is a big deal for performant websites.  Although the internet is pretty quick, the behavior
of mobile devices is a bit different from (and back a few years from) desktops.  Grails has its own asset
pipeline system that leverages the well-defined layout of a Grails project.</p>

<h3>Good IDE integration</h3>

<p>Grails integrates with IDEA and Eclipse, which are the best (IMO) and most-pervasive (I believe) IDEs
for Java development.</p>

<h3>Plugins!</h3>

<p>Grails has a very simple and powerful plugin system that adds lots of great capabilities.
With the move to 3.x some plugins may not yet be ready, but every month several more should be migrated.</p>

<h3>Standard Layout: assets, controllers, services, views, ...</h3>

<p>Grails has a very clean layout that is mostly aligned with Spring Boot (I believe) and has been mostly
the same over all three generations.  The 'src' directory is for things outside the Grails world.  And
the 'grails-app' directory is for things inside the Grails world.  The grails layout is very intuitive
for a modern web application:</p>

<ul>
<li>assets – Assets to go through the pipeline</li>
<li>conf - Configuration of the application</li>
<li>controllers – The UI (or web-api) interaction layer of the application</li>
<li>domain – The business model layer of the application, and the persistent state model</li>
<li>i18n – Internationalization</li>
<li>init – Things to do at startup</li>
<li>services – One or more 'services' layers to pull logic from controllers and domain into</li>
<li>views – GSP to be used by controllers for rendering (if desired)</li>
</ul>


<p>For the petclinic this layout looks like this when expanded:</p>

<p><img src="http://markfussell.emenar.com/images/addstack-3/addstack3_grails3.png" /></p>

<h3>Modern logging with 'logback'</h3>

<p>Logging is one of the easiest things to swap out, but Grails defaults to the relatively modern 'logback'
<a href="http://logback.qos.ch">http://logback.qos.ch</a> framework.</p>

<p><img src="http://markfussell.emenar.com/images/addstack-3/addstack3_grails4.png" /></p>

<h3>Incredibly terse</h3>

<p>One of the horrors of moving from Smalltalk to Java was about 4-8x the number of words were required to
accomplish the same task.  Writing more is painful.  Painful to write.  Painful to read.  Painful to edit.</p>

<p>With Groovy and Grails, the power of meaning actually leap-frogged both Smalltalk and (amazingly) LISP.
Or at least LISP without a really powerful set of macros.</p>

<p>As an example, the 'PetController' is the main UI functionality of the 'petclinic'.  But it has only 53 lines,
a third of them are blank.  And a total of 150 'words'.</p>

<p><img src="http://markfussell.emenar.com/images/addstack-3/addstack3_grails5.png" />
```groovy
package org.grails.samples</p>

<p>class PetController {</p>

<pre><code>def petclinicService

def add() {
    if (request.method == 'GET') {
        return [pet: new Pet(owner: Owner.get(params.owner?.id)), types: PetType.list()]
    }

    def pet = petclinicService.createPet(params.pet_name, params.pet?.birthDate,
        (params.pet?.type?.id ?: 0) as Long, (params.pet_owner_id ?: 0) as Long)

    if (pet.hasErrors()) {
        return [pet: pet, types: PetType.list()]
    }

    redirect controller: 'owner', action: 'show', id: pet.owner.id
}

def edit() {
    if (request.method == 'GET') {
        render view: 'add', model: [pet: Pet.get(params.id), types: PetType.list()]
        return
    }

    def pet = Pet.get(params.id)

    petclinicService.updatePet(pet, params.pet_name, params.pet?.birthDate,
        (params.pet?.type?.id ?: 0) as Long, (params.pet_owner_id ?: 0) as Long)

    if (pet.hasErrors()) {
        render view: 'add', model: [pet: pet, types: PetType.list()]
    }
    else {
        redirect controller: 'owner', action: 'show', id: pet.owner.id
    }
}

def addVisit() {
    if (request.method == 'GET') {
        return [visit: new Visit(pet: Pet.get(params.id))]
    }

    def visit = petclinicService.createVisit((params.visit?.pet?.id ?: 0) as Long, params.visit?.description, params.visit?.date)
    if (visit.hasErrors()) {
        return [visit: visit]
    }

    redirect controller: 'owner', action: 'show', id: visit.pet.owner.id
}
</code></pre>

<p>}
```</p>

<p>It's functionality is not amazing.  But most things people need to do on the web <em>are not amazing</em>.  They are
basically CRUD (Create, Read, Update, Delete).  Include dealing with all kinds of media (documents, images, videos, etc.)
and 99%+ of the web is just doing CRUD.  90%+ is just Read.</p>

<h4>Automatic Wiring</h4>

<p>A lot of things are happening with this controller.  The simple statement</p>

<p><code>groovy
def petclinicService
</code></p>

<p>gets automatically wired to PetclinicService in the services folder.</p>

<h4>Automatic Rendering</h4>

<p>The line</p>

<p>```groovy</p>

<pre><code>        return [pet: new Pet(owner: Owner.get(params.owner?.id)), types: PetType.list()]
</code></pre>

<p>```</p>

<p>causes the view 'pet/add.gsp' to render with that 'pet' and 'types' property set.  So the HTML can also be
quite terse:</p>

<p>```html
<html></p>

<pre><code>&lt;head&gt;
    &lt;meta name="layout" content="main"&gt;
    &lt;title&gt;${ pet.id ? 'Update' : 'Add'} Pet&lt;/title&gt;
&lt;/head&gt;

&lt;body id="add"&gt;
    &lt;h2&gt;&lt;g:if test="${!pet.id}"&gt;New &lt;/g:if&gt;Pet&lt;/h2&gt;

    &lt;b&gt;Owner:&lt;/b&gt; ${pet.owner?.firstName} ${pet.owner?.lastName}
    &lt;br/&gt;
</code></pre>

<p>```</p>

<h4>Powerful redirect and delegation</h4>

<p>Grails can control the client with redirects or delegate to other controllers behind the scenes.</p>

<p>```groovy</p>

<pre><code>    redirect controller: 'owner', action: 'show', id: pet.owner.id
</code></pre>

<p>```</p>

<h4>Very flexibly Services</h4>

<p>The PetclinicService is transactional so it can save objects within an automatic transaction.  But this
is optional and can be controlled.</p>

<p>```groovy
class PetclinicService {</p>

<pre><code>// PetController

Pet createPet(String name, Date birthDate, long petTypeId, long ownerId) {
    def pet = new Pet(name: name, birthDate: birthDate, type: PetType.load(petTypeId), owner: Owner.load(ownerId))
    pet.save()
    pet
}
</code></pre>

<p>```</p>

<h4>Super-clean domain classes</h4>

<p>A modern tendency is to have domain classes mostly represent the data side of the Domain object and pull the
higher level logic out into other classes.  Although I dislike this (why have two classes), it does work
better with automatic schema generation and migration.  You are less likely to have to restart the container
and make the system check for data migration issues.</p>

<p>Grails supports a very simple, rich, mapping system called GORM.  And with it, the Domain class is very terse
but also very powerful.  And GORM works on multiple database <em>kinds</em> let alone products.</p>

<p>```groovy
package org.grails.samples</p>

<p>/<em>*
 * Simple domain object representing a pet.
 *
 * @author Graeme Rocher
 </em>/
class Pet {</p>

<pre><code>String name
Date birthDate
PetType type
Owner owner

static hasMany = [visits: Visit]

static constraints = {
    name blank: false, validator: { name, pet -&gt;
        if (!pet.id &amp;&amp; pet.owner?.pets?.find { it.name == name })  {
            return 'pet.duplicate'
        }
    }
}
</code></pre>

<p>}
```</p>

<h2>Summary</h2>

<p>There is much more functionality to Grails than described above, but that is a good, quick, walk-through.  There
are also certain conventions I believe are best practices on top of the Grails framework (e.g. 'RepoService' classes
for Domain objects so functionality is easily and consistently located), but again that is an augmentation vs. being
a requirement to getting a good picture of the system.</p>

<p>Next I will go into the UI portion and options for that.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ADD Stack [Part-2]]]></title>
    <link href="http://markfussell.emenar.com/blog/addstack-2/"/>
    <updated>2015-10-15T01:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/addstack-2</id>
    <content type="html"><![CDATA[<p>This is the second series describing a radically more productive development and delivery environment.  The
first article is here: <a href="/blog/addstack-1/">Intro</a> and described the truth and lies about developing software.
Basically, assuming you are doing everything in the first series, and are using relatively modern technologies
and techniques, the 'language', 'framework', 'stack', and 'methodology' does not matter very much.
And how it matters has a lot to do with the development team and the customer.
Customer happiness is the ultimate goal, so if you are not making your customer happy, you should make incremental
adjustments (Educated Guesses) to do so.  And if you can not "continuously" deliver valuable products to the
customer, you need to figure out what is the bottleneck and get rid of it (e.g. google Throughput Accounting).</p>

<h2>Testing</h2>

<blockquote><p>"Hey... you said testing wasn't important"</p></blockquote>

<p>No, I said automated testing was not <em>necessary</em>.  At times it is not even <em>valuable</em>, but at other times it is.</p>

<p>There is no way to avoid 'Testing' unless no one is going to use your product.  'Using' is 'Testing', so everything
useful is continuously tested.  You have one user, you have one tester.  A million users, a million testers.
So you can't avoid testing you software.  You can only augment/change <em>who</em> or <em>what</em> tests your software.  There
 are a number of options:</p>

<!--more-->


<ul>
<li>Users – Guaranteed unless you have none</li>
<li>Developers – Should, unless they are crazy.  Not 'testing' your software is equivalent to not reading (and editing) what you wrote.</li>
<li>Product Management – Should, unless they are crazy or really confident in the development team.  Product management is the Customer and represents the Users.  They are supposed to know what the users want and need to verify the product is doing the right thing</li>
<li>Quality Assurance – Useful if they are really good at being a 'Black Hat'.  This is a different skill from 'Developing' and some people are very good at it

<ul>
<li>QA should report to Product Management in spite of having technical skills similar to software developers</li>
</ul>
</li>
</ul>


<p>You could have a few more kinds of people that are stake holders, but the above is the core sources.  These are
people who could know what the software is supposed to do, and then test whether it is doing it.  They have
different talents, different tolerance for failure, and different price-points (or time restrictions).  So choosing
the right balance is again a 'Make Customer Happy' (given a particular budget) ratio of utilization.</p>

<h3>Automation</h3>

<p>The above lists people.  This is because people have to do the hard work of figuring out 'What should it do?' and
'How do I tell it is doing it?'.  It is true there are some automated test generators... but they are a rare
and limited breed that do a little bit more than a 'static typed' language would do.  The benefits for most project
are minimal.</p>

<p>So 'automated testing' is rarely about generation, but more about repetition.  Doing a manual test over and over
by a human is much more expensive, time consuming, and failure prone to getting a computer to run a similar test
against software.  The problem is someone has to 'spend time' writing the test.  And time is both money and delay.</p>

<p>But maybe the tests are (magically) free.  Are they worth it?  Are they valuable?  Possibly "No!".
Testing does not make software better.
Testing just proves software does something that passes the test.  Just like in academics a student could
prep for the SAT but be bad at math, software can pass the test and be horrible.  Fragile.  Complex.  Incoherent.
And the tests themselves could be Fragile.  Complex.  Incoherent.  And Obsolete.</p>

<p>Automated tests are neither good nor bad.  Only good tests are good and bad tests are bad.  I have had products with
tens of thousands of great tests.  And very successful products with basically no tests.  And been on teams with
hundreds of horrible tests that made them go slower and produce a worse product than if they just threw all the
tests away.</p>

<h3>Automation with Frameworks</h3>

<p>Because testing can be useful, your frameworks should support it.  And should support it as <em>easily</em> as possible.
They should be easy to read.  Easy to write.  Easy to maintain.  Powerful.  And as much as possible be "from the outside".</p>

<p>Strangely this last truth has been replaced by a lie:</p>

<blockquote><p>"You should test each unit, each module, each integration, etc."</p></blockquote>

<p>It is a variant of the XP 'test first' mentality.  And it is completely idiotic.  Because it makes you focus on
the 'how' instead of the 'what'.  Your customer does not care you wrote something in Java or C.  With Spring
or Netty.  MySQL or Mongo.  Object-Oriented or functional.  They want the food.
And they want it to taste good.  Everything about making that food <em>you</em> can care about, but <em>you can't be</em> continuously
testing it or you are wasting your customers time and money.  Testing the knife before each cut.  Testing the pan to see if it is solid.  If something breaks you
might investigate why it broke and do a periodic check in the future.  But you <em>do</em> by <em>doing</em>, and not with a
fear of failing on each step of the way.  You are paid to <em>do</em> not to <em>be afraid</em>.  <em>Do</em>, <em>Fail</em>, <em>Learn</em>, <em>Do</em></p>

<p>I am not saying you can't test a few things on the inside.  Sometimes I do that as I bring a system up so it is
obvious (a) whether it is working, and (b) how it is supposed to be working.  A little 'extra documentation' beyond
the code itself.  But these are a limited set that covers a slice of the system, not the whole thing.</p>

<h3>Automation with Grails</h3>

<p>The team behind Grails leverages other technologies as much as possible when they work.  Among the best technologies
for automated testing in Spring / Java are:</p>

<ul>
<li> Spock – https://code.google.com/p/spock/</li>
<li> Geb – https://github.com/geb/geb</li>
</ul>


<p>So Grails leverages these and has testing built-in to the framework.  It can automatically generate test stubs for
Controllers (the UI interaction classes) and Domain classes (Business Logic).  And it generates both integration
and unit tests. I prefer integration tests as they are the most "from the outside", but all the different tests
are useful in different amounts (some are faster to run).</p>

<p>For our 'petclinic', it has a number of tests of two different kinds:</p>

<p><img src="http://markfussell.emenar.com/images/addstack-2/addstack2_grails1.png" /></p>

<h4>Spock / Unit-ty tests</h4>

<p><img src="http://markfussell.emenar.com/images/addstack-2/addstack2_grails2.png" /></p>

<h4>Grub / Integration tests</h4>

<p><img src="http://markfussell.emenar.com/images/addstack-2/addstack2_grails3.png" /></p>

<p><img src="http://markfussell.emenar.com/images/addstack-2/addstack2_grails4.png" /></p>

<p><img src="http://markfussell.emenar.com/images/addstack-2/addstack2_grails5.png" /></p>

<p>The separation of Unit and Integration is somewhat more complex than this because Spock especially has
the ability to be lots of different kinds of tests.  But the basic idea is above.</p>

<h4>When to run?</h4>

<p>So we have some tests in a nice testing framework.  Now the critical questions:</p>

<ul>
<li>When should we run them?  As much as possible without wasting people's time.</li>
<li>Should they be run before deploying a new version?  If they are fast enough, sure.  If not, then something should 'go live' <em>before</em> the tests.

<ul>
<li>I don't mean 'go live' to end users, but go live to a server where others can see it.  Just like the 'fed1_app1' deployment.</li>
<li>And you can have another server that is 'post-test' so it is gated by tests being successful.  It is more stable but slightly behind in time and version.  Say 'fed2'.</li>
</ul>
</li>
<li>Should developers run tests before checking in?  If they want to and are nervous, sure.  Especially tests in the area they are touching.  But breaking 'fed1' is not a big deal unless you walk away afterwards.  Break it.  Fix it.  Carry on.</li>
</ul>


<p>The tests for the 'petclinic' are quite fast, so we might as well run them before deploying and let people know the status.  Returning to our deployment script:</p>

<p>```bash
pushd $APP_PATH
./gradlew test</p>

<h3>if success then continue with 'run', otherwise leave the current version running.</h3>

<p>popd
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ADD Stack [Part-1]]]></title>
    <link href="http://markfussell.emenar.com/blog/addstack-1/"/>
    <updated>2015-10-14T01:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/addstack-1</id>
    <content type="html"><![CDATA[<p>This is the second series describing a radically more productive development and delivery environment.</p>

<h2>Review of the first series</h2>

<p>The first series starts here: <a href="/blog/add-1/">Intro</a> and ends here: <a href="/blog/add-11/">Voilà!</a>.  The bulk of the
series was describing how to combine four ingredients:</p>

<ul>
<li>Amazon EC2 <a href="http://aws.amazon.com/">http://aws.amazon.com/</a></li>
<li>Amazon S3  <a href="http://aws.amazon.com/">http://aws.amazon.com/</a></li>
<li>GitHub.com <a href="http://github.com/">http://github.com/</a></li>
<li>HipChat    <a href="http://HipChat.com/">http://HipChat.com/</a></li>
</ul>


<p>in a way that gives individuals and teams 4x or more productivity.  These resulting environment solves problems for
all three roles in a development team:</p>

<ul>
<li>Changers</li>
<li>Watchers</li>
<li>Machines</li>
</ul>


<p>And can be envisioned as so:</p>

<p><img width="432" height="414" src="http://markfussell.emenar.com/images/add-1/ADD_FourIngredients_ThreeRoles_mlf1a1.png" /></p>

<p>Along the way, the ingredients combined to make:</p>

<ul>
<li>Machines automatically launch and configure themselves, including hooking them up to each other (auto-wire)</li>
<li>Changes to IT roll out automatically to all the running machines.</li>
<li>Changes to the application roll out automatically to all the running machines.</li>
<li>Creating new environments to be as easy providing a simple parameter (e.g. "fed2") to a CloudFormation</li>
<li>The whole thing working on EC2 or inside a Vagrant container... or even on bare metal (but that was not shown)</li>
<li>Incredible visibility into everything that happens within HipChat... for the whole team.</li>
</ul>


<p>By the end, we had machines tell us when they launched, when they were operational, when they were deploying a new
version, and whether that new version deployed successfully.</p>

<p><img src="http://markfussell.emenar.com/images/add-11/add11_autodeploy1.png" /></p>

<p>and finally, we had a running application at a human-usable URL:</p>

<p><img src="http://markfussell.emenar.com/images/add-11/add11_dns1.png" /></p>

<p>That could be scaled to any number of servers as load demands.</p>

<h2>Second series</h2>

<p>The above is the core of the ADD and enabling team productivity and customer visibility (even happiness)
to shoot through the roof.  It by itself is basically 'complete'.  It is missing some details, nuances,
augmentation, and such.  But it alone is the core of a 4x productivity.</p>

<blockquote><p>"Complete! What nonsense!  Where are your automated tests!  How do you check for code quality!
What kind of programming language do you use!  And where is your description of Scrum, XP, Waterfall, etc. methodology
to use with this ADD?!!"</p></blockquote>

<p>Ah... yes.  Over my <em>thirty five years</em> and estimated <em>thirty five thousand hours</em> of developing software,
I have encountered all these things.  I do have opinions
on a lot of these things.  Preferences.  Best practices.  You may be interested.  They matter.  A little.</p>

<p>How little?  Maybe... maybe... 2x.</p>

<!--more-->


<p>I mentioned in the first series that when I shifted to Java from Smalltalk, I lost
a lot of productivity.  Part of it was in going from a hyper-productive language (Smalltalk) to a mediocre language.
Most of it was going from a hyper-productive development model (tweak and clone) to a horrible development model (build
most of it from scratch... because there are no libraries).  I bet I lost 4x in productivity.  And then another 2x
when people started mucking with my code.  4x is a lot.  2x is a little.
And if you try to get the '2x' but lose the '4x' you just did a win-LOSE bad tradeoff.</p>

<h3>Lies</h3>

<p>There are a lot of lies or confusions in the software industry.  During the nineties we tried to get rid of a lot of them
through actual research-based observations (e.g. "The Mythical Man-Month" by Fred Brooks) and experience-based
 observations (e.g. "The Agile Manifesto" by Kent, Ken, Ward, etc.).  The <em>truth</em> of software
is available... but the lies hide it.  And people can't tell the difference.  And don't believe the science.  Or are
too scared of losing their jobs (or having a more boring job) if they believe and speak the truth vs. the lie.</p>

<h4>Truth: One Brain</h4>

<p>I will first start with among the best researched truth there is:</p>

<ul>
<li>The best software is created by one primary, talented, and skilled 'brain'

<ul>
<li>Potentially accompanied by some number of 'assistants'</li>
</ul>
</li>
</ul>


<p>This is the 'surgical team' model.  You want one primary, talented, and skilled 'Surgeon'.  A 'Surgeon' with many thousands
if not many tens-of-thousands of hours of experience.  Nurses can help.  Support doctors can help.  Even other surgeons can
help.  If the operation is long enough, you might need multiple primary surgeons who pass the baton along to each other.
But if someone is opening your body... you want that person to be fully responsible, fully accountable, insanely competent,
and insanely skilled.  An insanely great surgeon.  She will charge you a lot, and you will get a lot in return.</p>

<p>Software is the same.  The best software is create by one primary, talented, and skilled 'Engineer'. An 'Engineer' with
many thousands if not many tens-of-thousands of hours of experience.  QA can help.  Support programmers can help.  Software is
 never needed so quickly as to require a hourly baton, but that can happen if the 'Engineer' moves on to other things.</p>

<p>If someone is writing an application that you want to be valuable to you... you want that person to be fully responsible,
fully accountable, insanely competent, and insanely skilled.  An insanely great engineer.
She will charge you a lot, and you will get a lot in return.</p>

<h4>Lie: All together now!</h4>

<p>But software developers don't like this truth.  They think they (after less than 5000 hours) can be the point person
for building an application.  I don't mean 5000 hours <em>of work</em> I mean 5000 hours <em>in surgery</em> and that is after say
2x the amount of hours of <em>studying surgery</em>.  So 10,000 hours of studying how to build software applications and 5,000
hours of writing code that builds actual production-worthy software applications.  If you work 2000 hours a year and
spend half of it actually on either of those tasks... it would take you about 15 years to be 'an Engineer'.  Ten years
of reading and five years of writing.  Done concurrently, where reading dominates early and writing dominates later.</p>

<p>But again, developers don't like this truth and instead they say: "If a bunch of us get together we can reproduce that
surgeon with each of our individual skills."</p>

<p>But this is saying: "Nine people who can dissect frogs can perform brain surgery", or
"Nine club tennis players can beat Roger Federer if they tag-team".  Or "Nine people who are good in the
kitchen can together produce a better meal than Gordon Ramsey in a throw down".  Or...</p>

<p>Hopefully by shifting the context you can see the impossibility of it.  It makes absolutely no sense, yet the lie is
propagated so much as to be systemically the status quo.  Even the smallest XP version of the lie "You need two people to write better software" is
ludicrous.  Better to have <em>two surgeons</em> perform <em>two different operations</em> at the same time... and save <em>two</em> lives.
There is nothing wrong with having an apprentice watch / help.  But that is <em>training</em> (somewhat to both parties) not
<em>doing</em></p>

<h4>Truth: Customer Rules</h4>

<p>The second truth is that the <em>customer</em> rules.  If the customer is happy... WIN!  If they are not happy... :-(</p>

<p>Given this is <em>sort of</em> in the Agile Manifesto you would think it would be well known now that everyone is 'Going Agile'.
But reviewing the <a href="http://www.agilemanifesto.org">Agile Manifesto</a>, I see that it "backed off" saying what it should say.
The lines that it should have within it are:</p>

<ul>
<li>Working software over ...everything...</li>
<li>Customer happiness over ...everything...</li>
</ul>


<p>But because software developers think they are special... unusually talented in a way that is beyond the customer... like 'artists'...
they think these two lines do not apply to them.  Again, the insanity of it is evident when you shift industry.  Say 'Food':</p>

<ul>
<li> Working food (e.g. good produce, a cooked meal, etc.) over ???</li>
<li> Customer happiness over ???</li>
</ul>


<p>Pretty sure if you nail the second bullet, you nailed the first one (and maybe some more).  And not nailing both these
bullets is a path to failure.  The question marks
are replaceable with 'everything not mentioned'.  So the actual pyramid is this:</p>

<ul>
<li>Happy Customer (100%)

<ul>
<li>Delivered good product (80%)

<ul>
<li>All the stuff needed to do the above</li>
</ul>
</li>
<li>Other aspects of delivering (20%) –  E.g. customer interaction, appearance of product or delivery person, etc.

<ul>
<li>All the stuff needed to do the above</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>You are allowed to have less than totally happy customers.  But you should try to fix that.  You should care.
Otherwise you are resigning yourself to  being content with the quality of your product being sub-par.  And likely
headed downward.</p>

<p>You are allowed to hate making your customers happy.  You can change industries.  Change role.  Continually grumble about how
bad your job is (but google "Worst Jobs" to see you don't have it so bad).  But you are not allowed to say that
<em>your customer</em> shouldn't be 100% happy... or at least as happy as you can make them.</p>

<h4>Lie: Automated Testing and Quality Verification</h4>

<p>The second lie is that <em>instead of</em> trying to make the customer 100% happy, we are going to make our automated
tests and our QA verification servers happy.  We are going to make them <em>more</em> happy than our customer and
<em>before</em> we make our customer happy.</p>

<p>This actually pops up as a 'truth' in education too.  We try to make the tests happy as opposed to the students,
except in exceptional schools that Think Different.  But
in most any <em>normal</em> industry, making a 'test' happy instead of a customer is a crazy and effective way to
go out of business.  Say we inspected and tested <em>every</em> hamburger before it went out the door.  Fast food would
take on a whole new meaning as it took minutes per-person to do the inspection.  And things like 'salad' would
be unservable.  Salad almost always has something weird in it, and finding it takes about the same amount of time
as eating it.  And it would basically destroy / consume the salad as you checked each leaf, fruit, and nut.</p>

<p>Again, automated testing is <em>useful</em>.  But it is not <em>necessary</em>.  And it doesn't trump releasing to a customer
so they can see the 'Delivered good product'.</p>

<h3>Conclusion : Customer's One Powerful Right and Responsibility</h3>

<p>The ADD described so far, efficiently, effectively, and production-worthy scalably enabled one simple thing:</p>

<p><img src="http://markfussell.emenar.com/images/add-11/add11_dns1.png" /></p>

<p>That is a delivered good product (if you want to run a Pet Clinic).  What the application is beneath that, and
what processes were used to develop it, are all "of little importance".  There are many ways to develop
an identical web site.  The stack and method I will describe is just one of them.  It is a reference
that I consider a "Best Practice" among alternative "Best Practice" possibilities.  And all of these alternatives
get measured by exactly one macro criteria: "Customer Satisfaction".  For example:</p>

<ul>
<li>Buggy in production, customer is less happy.</li>
<li>Take longer to build, customer is less happy.</li>
</ul>


<p>Neither of the above trumps the other.  The ratio between them 'depends'.  And that ratio is totally under
the customer's control.  They may not know the ratio before the fact, but while 'driving' the system they
<em>have every right and even responsibility</em> to complain.  The development team can make an 'Educated Guess'
at the start of the project
but if the customer complains, they need to address that complaint and <em>change</em> the process, tools, people,
or office space <em>as much as possible and reasonable</em> to make the customer more happy.  Maybe just a little
change to become a little more happy: we aren't prescient and you don't know if the change will cause havoc
(everyone quits) or even address the issue.  But you address the complaint with an another 'Educated Guess'.
Over and over.</p>

<p>What should your development process be?  It depends... on your customer and your product and your team and
... The reason you need an Engineer is they make great 'Educated Guesses'.  And can iterate incredibly
quickly to fine tune the process and tools to what will work for the customer and the development team.</p>

<p>But the following parts of this series will be a good start.  It is my best recommendation in 2015 for a modern,
JVM-based, web-delivered, set of development methods, tools, frameworks, and other components.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-11]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-11/"/>
    <updated>2015-10-14T01:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-11</id>
    <content type="html"><![CDATA[<p>This is the elevent installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>.  In the previous parts I described the big picture, Vagrant and EC2,
node initialization, the one minute configuration HeartBeat, HipChat integration, presence, EC2, configuring nodes, inter-machine presence,
and configuring an application server and its' stack.</p>

<h2>It's Alive!</h2>

<p>We are close to having a viable semi-production environment running the latest version of our code, and with an incredible
capability of configuring, controlling, and being informed of that environment.  With the simple line:</p>

<p>```bash
mkdir -p /root/app/
cd /root/app</p>

<p>git clone git@github.com:grails-samples/grails-petclinic.git
cd grails-petclinic
./gradlew run
```</p>

<p>We got:</p>

<p><img src="http://markfussell.emenar.com/images/add-10/add10_grails2.png" /></p>

<!-- more -->


<p>Which is almost what we want... except.</p>

<ul>
<li> We need that IP address to be a stable domain name</li>
<li> We need the 'clone' to be augmented with subsequent 'git pull' in case there are subsequent updates</li>
</ul>


<p>In reverse order</p>

<h2>Pulling the Application</h2>

<p>Going back to the one-minute heartbeat, we had this 'cron' job:</p>

<p>```bash
mkdir -p /root/bin
cp ${RESOURCE}/cron_1m.sh /root/bin/cron_1m.sh
chmod +x /root/bin/cron_1m.sh</p>

<p>cat &lt;<EOS > /var/spool/cron/root
MAILTO=""</p>

<ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li>/root/bin/cron_1m.sh
EOS
```</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>Where part of the cron was described as:</p>

<p>```bash</p>

<pre><code>        bash_ifexist bin/nodework/common/work.sh
        bash_ifexist bin/nodework/part/`cat /root/nodeinfo/nodepart.txt`/work.sh
        bash_ifexist bin/nodework/stacktype/`cat /root/nodeinfo/stacktype.txt`/work.sh
        bash_ifexist bin/nodework/stacktype/`cat /root/nodeinfo/stacktype.txt`/part/`cat /root/nodeinfo/nodepart.txt`/work.sh
</code></pre>

<p>```</p>

<p>That is both too much code and too little to understand what is going on.  It is too much because we don't need to put anything but
the first line in the 'cron_1b.sh'.  It is "fine" to do so, but if we add some new concept we have to rebuild machines (or
dynamically copy in new 'cron_1m.sh') vs. the simpler option of just having the first 'work' cal the rest of the 'work'.</p>

<p>The part that is too little is the part where a bunch of git repositories are getting updated.  The full 'cron_1m.sh' looks
like this:</p>

<h4>cron_1m.sh</h4>

<p>```bash</p>

<h1>! /bin/bash</h1>

<h1>================================</h1>

<h1>=== Simple worker example</h1>

<h1>================================</h1>

<p>export ME=<code>basename $0</code>
export LOG=/root/log/${ME}<em>log.txt
export ERROR=/root/log/${ME}</em>error.txt
export START_TSS="<code>date +%Y%m%d-%H%M%S</code>"</p>

<p>export CURRENT_ACTION_FILE=./.temp/nodeinfo/currentaction.txt</p>

<p>bash_ifexist () { if [[ -f "$1" ]]; then bash "$1"; else echo "Skipped missing: $1"; fi }</p>

<p>mkdir -p /root/log/</p>

<p>exec 1>> ${LOG}
exec 2>> ${ERROR}</p>

<p>echo "${ME}: Start  ${START_TSS}" >> ${LOG}</p>

<p>export REPOS=<code>find /root/gitrepo/ -maxdepth 1 -mindepth 1</code>
for REPO in ${REPOS}; do</p>

<pre><code>pushd ${REPO}
    git pull
popd
</code></pre>

<p>done</p>

<p>for REPO in ${REPOS}; do</p>

<pre><code>pushd ${REPO}
    if [[ -e ${CURRENT_ACTION_FILE} ]]; then
        : #Don't do anything until the current action completes
    else
        bash_ifexist bin/nodework/common/work.sh
    fi
popd
</code></pre>

<p>done</p>

<p>export FINISH_TSS="<code>date +%Y%m%d-%H%M%S</code>"
echo "${ME}: Finish ${FINISH_TSS}" >> ${LOG}
```</p>

<p>The reason for the double loop is to make sure inter-repository interactions are all "at the same time".  After the set
of 'pulls', everything is stable.  We could even slow down the tempo of the system by simply pulling less often and
could jitter before the set of 'pulls' which would jitter the whole system.</p>

<h3>Where to work?</h3>

<p>So if we have our 'IT' repo as 'repo2_petulant-cyril' and our application repo as 'repo3_miniature-ironman', where should
we do our 'work.sh'?  The answer is unfortunatly simple: "where ever you want".  The more you do work in 'repo2' the
more 'functional' your system.  Like classic functional / imperative programming.  The more you do work in 'repo3' the
more 'object-oriented' your system.  The more 'repo3' is alive vs. being acted-upon.  On the other hand, the more you
do work in 'repo2', the more you can leverage similarities in the work (like knowing directory structures, git versions,
etc.) and the less 'repo3' is complicated by these things.  Ultimately it would be ideal to be 'aspect-oriented' and
add a 'trait' to 'repo3'.  But for the moment, I will keep things simple and do all the work in 'repo2' where depending
on the kind of node you are, you <em>know about</em> 'repo3' vs. 'repo3' knowing it could be alive.</p>

<h3>'repo3:miniature-ironman'</h3>

<p>Why does 'repo3' have two names?  The first is a simple identifier: it is a 'repo' and it is the third of its kind.
This simplicity makes sure things like 'find' produce a simple in-order answer.  If we have a lot of repositories, we
might start with '101' to make sure sorting works for a few hundred repositories.  The second part is a human-memorable
name, and to help associate the repository with its purpose.  You shouldn't name a repository <em>after</em> its purpose, because
it's purpose could change.  Outright change or simply grow.  Or another repo could have the same purpose and now we
can't figure out which is right.  It would be like naming a kid 'toddler'.  At first it might
not even be a toddler yet, but more importantly, eventually it will clearly no longer be a toddler.  And we would have
  a lot of 'toddler's so we can't figure out which is what.</p>

<p>So instead, each repo just has a unique name.  We have to look at it and interact with it to figure out what it is.  Or
each has a README that we can try to keep current.</p>

<h3>app1/work.sh</h3>

<p>We can fetch 'repo3' from 'repo2' within the work of an application server:</p>

<p>```bash</p>

<h1>====================================================</h1>

<h1>=== lad1/app1</h1>

<h1>====================================================</h1>

<p>echo "Doing App1 work"</p>

<h1>====================================================</h1>

<h1>=== checkout the application repository</h1>

<h1>=== if it doesn't exist</h1>

<h1>====================================================</h1>

<p>export APP_REPO=repo3_miniature-ironman
export APP_NAME=app2
export APP_PATH=$REPO_ROOT/$APP_NAME</p>

<p>if [ ! -d "$APP_PATH" ]; then
  echo "git clone git@github.com:shaklee/${APP_REPO}.git $APP_PATH"
  git clone git@github.com:shaklee/${APP_REPO}.git $APP_PATH
fi</p>

<p>```</p>

<p>The rest of the script looks identical to our main 'work.sh' except our directory has switched from 'common' to
something more specific.</p>

<p>```bash</p>

<h1>====================================================</h1>

<h1>=== Temporary files location</h1>

<h1>=== Override the 'TEMP' directory to be for this part</h1>

<h1>====================================================</h1>

<p>export ADD_TEMP=${GIT_ROOT}/.temp/add/lad1_app1b/
export MY_DIR="$( cd -P "$( dirname "${BASH_SOURCE[0]}" )" &amp;&amp; pwd )"</p>

<p>```</p>

<p>And we have to push/pop to get in the right directory for the 'GIT' commands to work.</p>

<p>```bash</p>

<pre><code>pushd $APP_PATH
export GIT_VERSION=`git rev-parse HEAD`
export DETECT_GIT_CHANGE=`git log --pretty=oneline ${PREV_WORK_VERSION}..  -- ${WORK_WATCH_VALUE} | awk '{print $1}'`
popd
</code></pre>

<p>```</p>

<h3>Start / restart</h3>

<p>Ultimately we will run from a Tomcat or similar container, but currently we just have to:</p>

<p><code>bash
pushd $APP_PATH
./gradlew run &amp;
popd
</code></p>

<p>Except for a couple issues</p>

<ul>
<li>We would like the error and standard output to go somewhere in case something goes wrong</li>
<li>We need to not collide with an previous 'run' because they hold onto the same port</li>
</ul>


<p>Solving these is pretty trivial if we are allowed to be 'brutal' and accept downtime.  Ultimately this won't be
a problem because we will first 'shutdown' and let the load balancer pull us out of rotation before we actually
upgrade versions.</p>

<h4>Launching</h4>

<p>Making something into a daemon is a common approach, but an even simpler approach is just redirect stdout and stderr
on a detached process.  Trying this from a terminal appears to not work, because everything is a child
of the terminal login process.  But to get out of that problem, you can just use 'screen' and
create a non-terminal-connected screen.</p>

<p>```bash
screen
./gradlew run &amp;</p>

<h1>Control-D</h1>

<p>ps aux | grep java
```</p>

<p>This is just for testing with a manual launch.  Our cron job doesn't have this issue and can start and restart
things without the extra effort.</p>

<h4>Logging</h4>

<p>The heartbeat cron job has its own logs, so everything would appear there by default.  We can change the destination to
something more informative / isolated by redirecting stdout and stderr.  Ultimately we would use a logging
framework and infrastructure (e.g. Graylog) to get everything off the machine and into a central logger.</p>

<h4>Killing</h4>

<p>Again, because we will ultimate decommission ourselves, we can be quite brutal to the running application.  Whenever
we are going offline, we mark ourselves as decommissioned and drain out all 'clients'.  For a web server this is
very quick.  For other servers (like a game server with shared game state), this can take a while.  So the states are:</p>

<ul>
<li>Running

<ul>
<li>Add Users</li>
<li>Keep Steady</li>
<li>Drain Users</li>
</ul>
</li>
<li>Decommissioning

<ul>
<li>Drain Users</li>
<li>Drained</li>
</ul>
</li>
<li>Decommission

<ul>
<li>Graceful shutdown</li>
<li>Kill application</li>
</ul>
</li>
</ul>


<p>The last one is the critical one.  Although 'tomcat' and others have a graceful shutdown, <em>it does not always work</em>.
We must be able to decommission properly, or we have to kill the application.  If we can't decommission a node
successfully, we can not bring it back to 'running' and so our instance pool has shrunk.  Ideally our infrastructure
recognizes this and either (a) kills the node or (b) marks the node as permanently unavailable and bumps the count
of expected nodes in the array (say from 4 to 5 to include one being permanently unavailable).
The benefit of 'b' is we can investigate the node later.  But either is operationally reasonable.</p>

<p>So for the moment, lets just do the last one to decommission:</p>

<p>```bash</p>

<h1>Log what we are killing... could also test whether 'kill' is needed</h1>

<p>ps aux | grep java | grep -v grep
ps aux | grep java | grep -v grep | awk '{print $2}' | xargs kill
```</p>

<h4>Are we alive?</h4>

<p>Finally, we should at least reasonably check whether we are alive with the new version.  The load balancer is
checking our URL, so it knows.  But the load balancer can't tell the world (HipChat) whether the deploy was
successful.</p>

<p>A simple 'curl' to our local port can though.</p>

<p><code>``
curl localhost:8080 &gt; $ADD_TEMP/curl_test.html
export CURL_TEST=</code>cat  $ADD_TEMP/curl_test.html`</p>

<p>if [[ -z "$CURL_TEST" ]] ;
then</p>

<pre><code>echo 'Deployment failed!';
</code></pre>

<p>else</p>

<pre><code>echo 'Success!';
</code></pre>

<p>fi
```</p>

<h2>DNS Stability</h2>

<p>The DNS stability is a trivial thing on EC2 and basically any modern DNS system.  Trivial as long as we understand
one fatal flaw:</p>

<ul>
<li>Everyone caches</li>
</ul>


<p>We can have stability so long as we accept that everyone is trying to cache the DNS to IP address for as long as
possible.  This can be minutes or even longer <em>in spite</em> of our DNS records saying otherwise.  This is actually
 one of the reason load balancers are so important.  Load balancing is a very simple concept.  Your load balancers
 should be simple enough to 'keep running' and be rarely flipped.  So their IP addresses will be stable: like the person
 who is always hanging out on their front porch.  You can always find them there and ask them to ask someone else
 a question.  They will take care of the rest.</p>

<p>So ultimately we don't want the application server obtaining a domain-name, we want it to register with a load
balancer.  But for the moment (and for some useful internal capabilities) we will have each application server
take-a-name.</p>

<h3>How to take a name?</h3>

<p>Before the cloud, the common ways to take a name were:</p>

<ul>
<li>To assign a name via the console of the domain name server</li>
<li>To dynamically attach an IP address to a name via various protocols</li>
</ul>


<p>Pre-cloud, these work fairly well.  If you DN server is smart enough, it can take failed servers out of rotation
with a health check.  And even early versions of cloud computing used a similar model where servers took
possession of a pre-allocated IP address that the DN server knew about.  But these models don't scale as
well as a much simpler model.  Route-53.</p>

<p>Route-53 takes the 'dynamically' to a whole new level.  You can dynamically add / change / delete most anything
and the common latency for the change to take affect is very short.  The rest of the world <em>still caches</em> but
at least you know the DN servers are up to date.  In the following, you can see the 'gaps2c' project I created
a while ago as a demonstration of this ability.  The main domain registry for 'gaps2c.com' is elsewhere, but
the subdomain of 'aws.gaps2c.com' is managed by route53.  The last entry works and says <a href="google.aws.gaps2c.com">google.aws.gaps2c.com</a>
knows where the google servers are.</p>

<p><img src="http://markfussell.emenar.com/images/add-11/add11_route53_1.png" /></p>

<h3>Registering with Route-53</h3>

<p>There are a lot of ways to register with Route-53:</p>

<ul>
<li> Via the console</li>
<li> Via an XML-based payload update</li>
<li> Via adding some elements to a CloudFormation</li>
</ul>


<p>The first is easy, but not scalable.  The second is not hard, but a bit 'peculiar' in how the payload works.  The
third is trivial and scalable.</p>

<p>So lets start with the third</p>

<h4>Registering via a CloudFormation template</h4>

<p>The good news is it is simple in a CloudFormation.  The bad news is it requires a bit more infrastructure to appear, so
it isn't a one-liner.  The issue with CloudFormations is that they must be <em>complete</em>.  They can only auto-wire things
they know about.  So they are relatively monolithic.  It is possible to compose them somewhat, but that is really
just embedding one formation into another vs. wiring two independent formations up.  Ultimately to wire up independent
formations, you need to 'know what you said' and tell two or more formations enough information that they can find
each other.  That is not hard but it isn't free.</p>

<p>But for free, we can add three things to our application stack and get a DNS registration for our application tier.</p>

<ul>
<li> A HostedZone entry – Pointing into the Route 53 database</li>
<li> An ElasticLoadBalancer – Which enables the CloudFormation to know about IP addresses</li>
<li> A PrimaryDnsZone – Which defines the FQDN of the application tier</li>
</ul>


<p>And we need to augment our AutoScalingGroup to know about the ELB.  Adding all this gives:</p>

<p>```json</p>

<pre><code>"HostedZone" : {
  "Type" : "String",
  "Default" : "aws.gaps2c.com",
  "Description" : "The DNS name of an existing Amazon Route 53 hosted zone"
},
</code></pre>

<p>```</p>

<p>```json</p>

<pre><code>"PrimaryElb" : {
  "Type" : "AWS::ElasticLoadBalancing::LoadBalancer",
  "Metadata" : {
    "Comment" : "Configure the Load Balancer with a simple health check and cookie-based stickiness"
  },
  "Properties" : {
    "AvailabilityZones" : { "Fn::GetAZs" : "" },
    "LBCookieStickinessPolicy" : [ {
      "PolicyName" : "CookieBasedPolicy",
      "CookieExpirationPeriod" : "30"
    } ],
    "Listeners" : [ {
      "LoadBalancerPort" : "8080",
      "InstancePort" : "8080",
      "Protocol" : "HTTP",
      "PolicyNames" : [ "CookieBasedPolicy" ]
    } ]
  }
},

"PrimaryDnsZone" : {
  "Type" : "AWS::Route53::RecordSet",
  "Properties" : {
    "HostedZoneName" : { "Fn::Join" : [ "", [{"Ref" : "HostedZone"}, "." ]]},
    "Comment" : "CNAME redirect to aws.amazon.com.",
    "Name" : { "Fn::Join" : [ "", [{ "Fn::FindInMap" : [ "TemplateConstant", "deployment", "value" ] }, "-", { "Fn::FindInMap" : [ "TemplateConstant", "nodepart", "value" ] }, ".", {"Ref" : "HostedZone"}, "."]]},
    "Type" : "CNAME",
    "TTL" : "900",
    "ResourceRecords" : [{ "Fn::GetAtt" : ["PrimaryElb","CanonicalHostedZoneName"] }]
  }
},

"PrimaryServerGroup" : {
  "Type" : "AWS::AutoScaling::AutoScalingGroup",
  "Properties" : {
    "Tags": [
      { "Key": "add:deployment", "Value": { "Fn::FindInMap" : [ "TemplateConstant", "deployment", "value" ] }, "PropagateAtLaunch" : "true" },
      { "Key": "add:nodepart", "Value": { "Fn::FindInMap" : [ "TemplateConstant", "nodepart", "value" ] }, "PropagateAtLaunch" : "true" },
      { "Key": "add:stacktype", "Value": { "Fn::FindInMap" : [ "TemplateConstant", "stacktype", "value" ] }, "PropagateAtLaunch" : "true" },
      { "Key": "add:statelog", "Value": { "Fn::FindInMap" : [ "TemplateConstant", "statelog", "value" ] }, "PropagateAtLaunch" : "true" },
      { "Key": "add:state", "Value": { "Fn::FindInMap" : [ "TemplateConstant", "state", "value" ] }, "PropagateAtLaunch" : "true" },
      { "Key": "add:statetss", "Value": { "Fn::FindInMap" : [ "TemplateConstant", "statetss", "value" ] }, "PropagateAtLaunch" : "true" }
    ],
    "AvailabilityZones" : { "Fn::GetAZs" : "" },
    "LaunchConfigurationName" : { "Ref" : "PrimaryLaunchConfig" },
    "MinSize" : "1",
    "MaxSize" : "1",
    "DesiredCapacity" : "1",
    "LoadBalancerNames" : [ { "Ref" : "PrimaryElb" } ]
  }
},
</code></pre>

<p>```</p>

<p>After adding all that, we get a new Route53 entry</p>

<p><img src="http://markfussell.emenar.com/images/add-11/add11_route53_2.png" /></p>

<p>And can also look at our load balancer:</p>

<p><img src="http://markfussell.emenar.com/images/add-11/add11_route53_3.png" /></p>

<p>And the instance associated with it:</p>

<p><img src="http://markfussell.emenar.com/images/add-11/add11_route53_4.png" /></p>

<p>And the rule we use to figure out whether the instance is alive</p>

<p><img src="http://markfussell.emenar.com/images/add-11/add11_route53_5.png" /></p>

<h2>Voilà!</h2>

<h3>Domain-Based, Auto-Scaling, IT and Application Stack</h3>

<p>We now have a stable domain name for our stack: 'fed1-app1.aws.gapcom.com'</p>

<p><img src="http://markfussell.emenar.com/images/add-11/add11_dns1.png" /></p>

<p>which can scale up and down machines automatically.  Ideally there are at least four in production to make sure
all the availability zones under 'US East' are covered.  But for anything else, one or two is sufficient depending
on what you are testing.</p>

<h3>Autodeploying IT and Application</h3>

<p>We now have a completely automated upgrade system for both our IT (servers) and
for the applications those servers are running.  Whenever we touch the code base
and 'push' the changes so the servers can see/pull them (PushMePullYou), they automatically
decommission, deploy and confirm the deploy was successful.</p>

<p><img src="http://markfussell.emenar.com/images/add-11/add11_autodeploy1.png" /></p>

<h3>What Next?</h3>

<p>Basically nothing is left for the core of the ADD.  This series has described how the four ingredients
are cooked/hooked together and shown what that looks like on EC2.  Other topics are just filling in details
that are really just nuances and flavors of the ADD. For example:</p>

<ul>
<li>What is the Grails application stack like?</li>
<li>How does Vagrant handle the one minute crons and other more advanced features?</li>
<li>Should you <em>develop in Vagrant</em> or on the host operating system?

<ul>
<li>Answer: Either depending on what you are doing and how productive you are, but the closer you are to production, the more likely you are to catch issues</li>
</ul>
</li>
<li>How does HAProxy and the database configuration using presence work?</li>
<li>What about NewRelic and other monitoring tools</li>
<li>Where is Angular in all this?</li>
</ul>


<p>The first and last topic I will cover in the next multi-part series called the 'AddStack'.  The others are more specific topics that I may get to eventually, but
are not critical to understanding and using the ADD.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-10]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-10/"/>
    <updated>2015-10-10T02:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-10</id>
    <content type="html"><![CDATA[<p>This is the tenth installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>.  In the previous parts I described the big picture, Vagrant and EC2,
node initialization, the one minute configuration HeartBeat, HipChat integration, presence, EC2, configuring nodes, and inter-machine presence.</p>

<h2>A good, opinionated, framework</h2>

<p>Back in 1972, Smalltalk became the first Object-Oriented Programming Language (Simula was Object-Based but there is a difference).
For decades this kind of language was 'esoteric'.  It was like LISP or Prolog or APL: somehow exotic and inaccessible.
I was lucky: I had access to Smalltalk at Caltech.  I had access to lots of crazy expensive things at Caltech and that
made the exotic (e.g. making your own chips) into the mundane (e.g. made lots of chips, they were commonly broken,
had to be re-fabricated, and I eventually got bored with all that and moved a level up).</p>

<!-- more -->


<p>But back to the language of Smalltalk.  The problem with Smalltalk is that it appears to be a language when it is
actually a computer. 'C' was a language.  It made programs. 'Pascal', 'Lisp' (sans Machine), 'Fortan', and so on...
they were all languages.  Smalltalk <em>contains</em> a language.  It is named Smalltalk (darn).  But Smalltalk-80 was not
<em>just</em> a language, it was an <em>entire running operating system with applications and full source</em>  It could boot on
most any machine that you made the 'bootstrap' code work on.  To make a new Smalltalk-80 machine, you cloned either
the primordial Smalltalk-80 'image' from PARC, or you cloned your own modified 'image'.  And by 'image' this is
basically the same concept as cloning a disk... bit by bit identical copy that happens to be on a different disk / computer.</p>

<p>Eventually OOP became mainstream with Java, C++, Objective-C, Ruby, Python, and the like.  So people thought they
were getting the "Smalltalk" (or LISP Machine) benefits.  But they left out the 'computer' that went with the language.</p>

<h3>Why opinionated?</h3>

<p>The Smalltalk computer was quite functional and thoroughly opinionated.  It <em>already did</em> a bunch of things and showed
you how it did them.  It wasn't opinionated like a human usually uses the term: "You should build that house out of bricks, not straw".
It was opinionated like the planet is: "I have already created lots of flora and fauna... please use them wisely".  Even
how humans on the planet are: "We have already created plenty of roads... please use them instead of driving through yards"</p>

<p>Opinionated is basically a synonym for "Working".  Smalltalk computers "worked" so don't break it.  They work, so you
should probably copy them for anything similar.  And they work, so you might want to study how they work even if you
are going to be creative later.</p>

<h3>Modern 'working' frameworks</h3>

<p>Early frameworks (say for Java) 'kind-of-worked'.  They didn't fully work, but you could 'configure' them to work.
That is like getting all 'IKEA' furniture for your house.  You could easily build it wrong.  It could not work
together.  Yes, you get to 'tweak' it, but if someone simply offered "a furnished house" you would save a lot of
time and leverage their full sense of design.  Or you could go to a different furnished house that more closely matched your tastes.</p>

<p>The later fully-working / opinionated frameworks (like Ruby/Rails) truly worked out of the box.  They would come up with a UI, Business/Domain
layer, and a Database layer.  You could add things to the UI and it would go down the whole stack.  Add things to the database
or Domain, and it would bubble up/down.  For the framework to do these things it had to have a model for what software (in its full form)
looks like.  These frameworks had patterns/templates/rules for building things at command.  This isn't quite as good as Smalltalk ("it already exists")
but it is getting close, especially with sample applications available.
It also gets rid of the one problem / hurdle with Smalltalk full-computers: you had to strip them
of things you didn't want customers to see / use / clone.</p>

<h3>Languages</h3>

<p>There are many modern languages.  They are mostly quite similar and boring in the language themselves.  The community around the
language makes much more of a difference, and the libraries / frameworks that exist based on that community's interest.</p>

<p>I mentioned that I switched to Java pretty early on, which cost me productivity.  But I wanted the community and their
libraries.  Java was popular enough that it had multiple communities associated with it.  Some were crazy stupid and
created things (even tried to mandate use of things) that were completely stupid.  But other communities continued
to plug along and evolve libraries and frameworks that are better than you can get in other languages.  On the whole,
I believe the Java ecosystem is by far the best 'hub' to build most custom development and to pair
with other tools/components in other languages.  And by Java, I mean Java, Groovy, and potentially other JVM-targeting
languages.  The less Java-like the language, the less likely I would consider it acceptably an 'other'.</p>

<h3>Framework</h3>

<p>I believe the best (general) application framework in Java is Grails, which lives on top of the Spring stack.  It is
very mature and has good minds in the drivers seat.  It gets simpler and more powerful every generation.  If Spring
does something right, Grails simply uses it.  If not, Grails augments it.  Very rational.  Very powerful.</p>

<h2>The first application</h2>

<p>The first application will simply be the default applications with a grails 'create-app'.  To get the application we
need to get grails on the 'app1' nodes, create the application, and then run it.</p>

<p>Grails needs Java, but ec2 instances automatically have that.  In other environments we would use something like:</p>

<h3>installJava.sh</h3>

<p>```bash</p>

<h1>!/bin/bash</h1>

<h1>================================================</h1>

<h1>=== Java</h1>

<h1>================================================</h1>

<p>export JAVA_VERSION=7u79
export JAVA_FULL_VERSION=jdk-${JAVA_VERSION}-linux-x64</p>

<p>mkdir -p .temp
cp it/resource/${JAVA_FULL_VERSION}.rpm .temp/</p>

<p>./bin/inflatePaths.sh .temp/${JAVA_FULL_VERSION}.rpm</p>

<p>rpm -i .temp/${JAVA_FULL_VERSION}.rpm</p>

<p>```</p>

<p>The advantage of storing the RPM within our own system is speed of access and reliability.  EC2 to S3 communication
is very fast.  And S3 has never been down (AFAIK) at all, let alone when EC2 is running.  We also lock down on the
version we want vs. using 'yum' without an explicit version.</p>

<h3>installGrails3_x.sh</h3>

<p>For grails we will get the latest version</p>

<p>```bash</p>

<h1>!/bin/bash</h1>

<h1>==========================================================</h1>

<h1>=== Install Grails 3.x</h1>

<h1>==========================================================</h1>

<p>curl -s get.sdkman.io | bash
source "$HOME/.sdkman/bin/sdkman-init.sh"
yes | sdk install grails</p>

<p>source ~/.bashrc
grails -version</p>

<p>```</p>

<p>We need to source '~/.bashrc' so we get the additions to our path.</p>

<h3>Create 'test' application</h3>

<p>At this point we have grails on the machine and can simply
```bash
mkdir -p /root/app/
cd /root/app</p>

<p>grails create-app test
cd test
grails run-app
```</p>

<p>The application will come up at 'localhost:8080' and if you wget/curl it, it returns the generated 'index.html' file:</p>

<p>```html
&lt;!doctype html>
<html lang="en" class="no-js"></p>

<pre><code>&lt;head&gt;
    &lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt;
    &lt;meta http-equiv="X-UA-Compatible" content="IE=edge"&gt;
    &lt;title&gt;Welcome to Grails&lt;/title&gt;
    &lt;meta name="viewport" content="width=device-width, initial-scale=1"&gt;
</code></pre>

<p>...</p>

<pre><code>    &lt;div id="page-body" role="main"&gt;
        &lt;h1&gt;Welcome to Grails&lt;/h1&gt;
        &lt;p&gt;Congratulations, you have successfully started your first Grails application! At the moment
           this is the default page, feel free to modify it to either redirect to a controller or display whatever
           content you may choose. Below is a list of controllers that are currently deployed in this application,
           click on each to execute its default action:&lt;/p&gt;

        &lt;div id="controller-list" role="navigation"&gt;
            &lt;h2&gt;Available Controllers:&lt;/h2&gt;
            &lt;ul&gt;

            &lt;/ul&gt;
        &lt;/div&gt;
    &lt;/div&gt;

    &lt;div class="footer" role="contentinfo"&gt;&lt;/div&gt;
    &lt;div id="spinner" class="spinner" style="display:none;"&gt;Loading&amp;hellip;&lt;/div&gt;
&lt;/body&gt;
</code></pre>

<p></html>
```</p>

<h3>Firewalls</h3>

<p>Unfortunately, you won't be able to test this app from the outside unless you open the '8080' port.</p>

<p>If we open that port either initially or through commands:
```json
{</p>

<pre><code>"IpProtocol":"tcp",
"FromPort":"8080",
"ToPort":"8080",
"CidrIp":"0.0.0.0/0"
</code></pre>

<p>}
```</p>

<p>We get the latest and greatest 'Grails' application (3.0.8 as of this writing)</p>

<p><img src="http://markfussell.emenar.com/images/add-10/add10_grails1.png" /></p>

<h3>Demo2: PetClinic</h3>

<p>There is a PetClinic demo at: https://github.com/grails-samples/grails-petclinic .  Doing the same simple launch procedure you would get something like this:</p>

<p>```bash
mkdir -p /root/app/
cd /root/app</p>

<p>git clone git@github.com:grails-samples/grails-petclinic.git
cd grails-petclinic
./gradlew run
```</p>

<p>And you get:</p>

<p><img src="http://markfussell.emenar.com/images/add-10/add10_grails2.png" /></p>

<p>But an important difference between the two: the second is now <em>wired</em> into the git repository and can 'pull' or 'push' to it as needed.
We now have a live server with both alive 'IT' and alive 'APP'!  It will happily and automatically bend to our will and needs.</p>
]]></content>
  </entry>
  
</feed>
