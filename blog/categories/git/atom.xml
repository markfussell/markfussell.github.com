<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Git | Polyglot]]></title>
  <link href="http://markfussell.emenar.com/blog/categories/git/atom.xml" rel="self"/>
  <link href="http://markfussell.emenar.com/"/>
  <updated>2015-10-08T14:22:17-07:00</updated>
  <id>http://markfussell.emenar.com/</id>
  <author>
    <name><![CDATA[Mark Fussell]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-9]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-9/"/>
    <updated>2015-10-08T02:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-9</id>
    <content type="html"><![CDATA[<p>This is the ninth installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>.  In the previous parts I described the big picture, Vagrant and EC2,
node initialization, the one minute configuration HeartBeat, HipChat integration, presence, EC2, and configuring nodes.</p>

<h2>Using Presence for configuration</h2>

<p>So far, presence has been just information that 'humans' consume.  It shows up on dashboards, in chat rooms, and so on,
but nothing has acted upon it.  Until now!</p>

<p>We have 'app' and 'db' nodes.  Clearly the 'app' nodes need to find the 'db' nodes or the app is not going to be
able to persist much.  The 'db' here happens to be 'Maria' but it could be anything from a single DB node to a
cluster of Riak nodes.  At the moment, I just want to get the information that a 'db' node knows ("I exist!", "My IP is this!")
over to the 'app' nodes so they can process it.</p>

<h3>It is already there?</h3>

<p>But wait?  All nodes have 'repo4' for writing?  Don't they already have everything in 'repo4' for reading as well?</p>

<p>And so they do.  The presence information is already there waiting patiently for somebody or someboty to read it.</p>

<p>As of this writing, repo4 looks like this:</p>

<p><img src="http://markfussell.emenar.com/images/add-9/add9_all.png" /></p>

<p>And the live hipchat still looks like this:</p>

<p><img src="http://markfussell.emenar.com/images/add-8/add8_demo1.png" /></p>

<p>So the DB server is definitely there:</p>

<p><code>
i-4e7cb59a:fed1/db1: Launched!
</code></p>

<p>And the information is there:
```json
{</p>

<pre><code>"filetype": "nodepresence",
"value": {
    "a": "a",
    "deployment": "fed1",
    "initgitrepo": "repo2_petulant-cyril",
    "instance-id": "i-4e7cb59a",
    "nodepart": "db1",
    "stacktype": "lad1",
    "state": "done",
    "statelog": "setup:20151008-174646;initdone:20151008-174830;",
    "statetss": "20151008-174830"
}
</code></pre>

<p>}
```</p>

<p>So the only issue is for nodes to 'find their partners'</p>

<h3>Finding the partners</h3>

<p>By the HipChat there are only three nodes alive, so problem one is finding live nodes vs. dead nodes.  There are
two levels to that:</p>

<ul>
<li> Finding plausibly live nodes</li>
<li> Finding truly awake nodes</li>
</ul>


<p>The simplest approach to the first is to make sure there is some kind of heartbeat within 'statetss'.  Every-minute
is clearly possible, but a bit noisy if done in the main part of repo4.  It would be nice to not to see the heartbeat
block out the actual state change information that is already there. An interesting alternative is to 'flatten time'
and have an alternative branch that stores information as 'it/presence/flattime/timestamp'.  Or given we are storing
the information differently, we could use the main branch and just change the comment to mention 'flattime'.
Yes, the information in the files would not change very often.  But Git stores files separate from paths,
so there is very little overhead to adding new paths to identical files.</p>

<h3>Time flattened</h3>

<p>repo4 has folder under 'presence' called 'flattime' which contains the presence information where every checkin is a new path.
To keep things manageable the timestamp is complete 'YYYYMMDDHHMM', but year/month/day/hour/ is used to organize it.
Seconds are not used because we want everything to be together and there is no guarantee the seconds would match
between machines.</p>

<p>Looking at a few minutes of time, we get something like this:</p>

<p><img src="http://markfussell.emenar.com/images/add-9/add9_flattime1.png" /></p>

<p>The files change a bit because the machines are switching states and at the 'capture' moment could be in almost any state
of their 'state machine'.</p>

<h3>How precise?</h3>

<p>So this is quite precise in time and you could certainly slow it down.  Sometimes providers get annoyed if you use
git this way, but the software itself is thoroughly comfortable with it.  You can get some impressive merge graphs
as the machine count goes up.</p>

<p><img src="http://markfussell.emenar.com/images/add-9/add9_merge1.png" /></p>

<p>So making things less often and more jittered will help alleviate some stress.  Since
this is only the first stage of presence (what exists and is plausibly alive), we will deal with stale data in the
second stage.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-8]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-8/"/>
    <updated>2015-10-08T01:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-8</id>
    <content type="html"><![CDATA[<p>This is the eighth installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>.  In the previous parts I described the big picture, Vagrant and EC2,
node initialization, the one minute configuration HeartBeat, HipChat integration, presence, and EC2 integration.</p>

<h2>Configuring Node Parts and even fuller Presence and visibility</h2>

<p>So far our node was a generic 'ControlNode'.  A ControlNode is a node that is in your data center
(to be near the other nodes), similar to your actual nodes, and configured to be able to do interesting tasks.
It is not a critical part of anything, so you can fiddle with it and just throw it away.  Say you want to
start configuring a database node.  You launch and login to a ControlNode and then try to install a database
(for example Maria).</p>

<!-- more -->


<p>```bash
cat > /etc/yum.repos.d/mariadb10.repo &lt;&lt;EOS</p>

<h1>MariaDB 10.0 CentOS repository list - created 2015-10-08 15:16 UTC</h1>

<h1>http://mariadb.org/mariadb/repositories/</h1>

<p>[mariadb]
name = MariaDB
baseurl = http://yum.mariadb.org/10.0/centos7-amd64
gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB
gpgcheck=1
EOS</p>

<p>yum install MariaDB-server MariaDB-client
```</p>

<p>Assuming you changed to root for the instance:</p>

<p><code>bash
sudo bash
</code></p>

<p>This works!  After confirming some things along the way...</p>

<p>OK, so now we want to have our database servers install MariaDB.  We first need an 'installMaria' script with the above
in it.  Say</p>

<h3>installMaria10.sh</h3>

<p>```bash</p>

<h1>!/bin/bash</h1>

<h1>================================================</h1>

<h1>=== Java</h1>

<h1>================================================</h1>

<p>cat > /etc/yum.repos.d/mariadb10.repo &lt;&lt;EOS</p>

<h1>MariaDB 10.0 CentOS repository list - created 2015-10-08 15:16 UTC</h1>

<h1>http://mariadb.org/mariadb/repositories/</h1>

<p>[mariadb]
name = MariaDB
baseurl = http://yum.mariadb.org/10.0/centos7-amd64
gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB
gpgcheck=1
EOS</p>

<p>yum install MariaDB-server MariaDB-client
```</p>

<h3>adding init</h3>

<p>And unless you want every node to install Maria, you need to add an 'init' for the type of node we are dealing with.
In our case, the StackType is 'lad1' (Load,App,Db,1) and the node type / part is 'db1'.</p>

<p>So we get this hierarchy:</p>

<p><img src="http://markfussell.emenar.com/images/add-8/add8_hierarchy1.png" /></p>

<p>And the content of the 'init.sh' file is simply:
```bash</p>

<h1>!/bin/bash</h1>

<h1>====================================================</h1>

<h1>=== fed1/db1</h1>

<h1>====================================================</h1>

<p>bash ${COMMON}/installMaria10.sh</p>

<p>```</p>

<h3>CloudFormation</h3>

<p>We have a new CloudFormation named 'awscf4_Fed1Db1' with the simple change of the nodepart being 'db1'</p>

<p>```json</p>

<pre><code>"TemplateConstant" : {
  "stacktype" : { "value" : "lad1" },
  "initgitrepo" : { "value" : "repo2_petulant-cyril" },
  "nodepart" : { "value" : "db1" },
  "state" : { "value" : "presetup" },
  "statetss" : { "value" : "" },
  "statelog" : { "value" : "" },
  "deployment" : { "value" : "fed1" }
},
</code></pre>

<p>```</p>

<h3>Demo or Die!</h3>

<p>Since the previous presence description, the presence system has upped itself and now nodes check-in with
more information than before.  After launching two 'app' stack and one 'db' stack we get this:</p>

<p><img src="http://markfussell.emenar.com/images/add-8/add8_demo1.png" /></p>

<p>So clearly our nodes know who they are, what deployment they are in, what the 'part' in that deployment is, and how to
"do work". But are they actually what they say they are?</p>

<h2>Being the 'user', fixing the 'user'</h2>

<p>If you use the installMaria script above, it will not work.  Because I said: "This works!  After confirming some things along the way..."
During 'boot' there is no user to confirm anything.  So the 'yum' part fails although the yum repository is there
(the user is 'root' so it has permission).  It ran the script but it didn't work under 'automation'.
The three annoying issues in automation:</p>

<ul>
<li> The user could be different from the user you think it is (say 'ec2' vs. 'root')</li>
<li> The user is not interactive</li>
<li> The user did not "launch a login shell" and so some launch things that happen for you did not happen for them.</li>
</ul>


<p>There are trivial and super-effective solutions to all of these, but until you get them right, you can bang your head
quite a bit.</p>

<h3>Who is the user?  'root'</h3>

<p>The user provisioning a machine should always be 'root'.  Yes, 'root' is dangerous.  Because 'root' is powerful.  And
given (a) if you mess up you simply kill the machine, and (b) everything is from version-controlled source files...
you can handle that power.  So don't add silly hoops to jump through.  On a ControlNode, immediately 'sudo bash'.
And if for some reason the default user isn't 'root' in a launch or cron script, 'su root' or 'sudo bash' to fix that.</p>

<h3>Is the user interactive? 'no'</h3>

<p>We are doing production automation that is designed to scale into thousands of machines.  No one is going to answer
questions for thousands of machines.  That is not 'scalable' or at all valuable.  You should always know all the
answers when a machine launches.  So script any UI that really requires some value put in, or use the variant of
a command that has default answers.</p>

<p>The true script for 'installMaria' has this line.</p>

<p><code>bash
yes | yum -y install MariaDB-server MariaDB-client
</code></p>

<p>The 'yum -y' <em>should</em> never ask any questions.  But just in case it does, I give it a 'y' for every answer.  If you
are testing something and get a question, check if it has a flag like '-y'</p>

<p><img src="http://markfussell.emenar.com/images/add-8/add8_yes.png" /></p>

<h3>Has the user launch as a login shel? 'yes'</h3>

<p>Since getting on a machine without 'logging in' is quite a bit more painful than 'ssh' into the machine, just make sure
any 'init' and 'work' script has read the files a login shell would automatically read.</p>

<p>A simple 'source /root/.bashrc' fixes the problem immediately</p>

<p>```bash</p>

<h1>========================================</h1>

<h1>=== Do the work for this repository</h1>

<h1>========================================</h1>

<p>source /root/.bashrc
...
```</p>

<h2>Node Work</h2>

<p>Just like the 'init' hierarchy, the 'db1' nodes can have their own work items to do regularly by simply adding the 'work.sh'
into the hierarchy:</p>

<p><img src="http://markfussell.emenar.com/images/add-8/add8_hierarchy2.png" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-7]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-7/"/>
    <updated>2015-10-04T01:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-7</id>
    <content type="html"><![CDATA[<p>This is the seventh installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>.  In the previous parts I described the big picture, Vagrant and EC2,
node initialization, the one minute configuration HeartBeat, HipChat integration, and presence.</p>

<h2>Fuller Presence and EC2 Integration</h2>

<p>In the previous part, I went through a very simple but powerful model of 'presence' using simply GitHub repositories.
The content of those presence statements was enough to figure out what nodes exist, but not much more about them.
The second level of presence is to update the state of the node as it changes.  For example, a node goes through
a few bootstrap steps:</p>

<ul>
<li> presetup – The node before any updates are possible (no ability to change status)</li>
<li> setup – The beginning of the 'setup' phase where the node is alive enough to change it's status</li>
<li> initdone – The time a node is done are initialization and can start doing 'work' as the 'nodepart' it is</li>
</ul>


<p>A node getting to 'setup' is pretty important: before that it may be a zombie!  And we don't want zombie's in
our federation!</p>

<!-- more -->


<p>So far for the ADD we now have four resources within which track node states:</p>

<ul>
<li> On the node (say '/root/log' or '/root/nodeinfo/state.txt)</li>
<li> Within the presence system</li>
<li> Within HipChat</li>
<li> On EC2 itself</li>
</ul>


<p>I recommend using <em>all</em> of them.</p>

<h3>On node</h3>

<p> On the node is very helpful in that it is isolated from any other failures.  You
 can 'tail' the logs or 'cat' the state file.  This tangibility helps understand things and debug if there is failure.</p>

<h3>Within Presence</h3>

<p> Within the presence system is the most powerful and flexible.  It is easy to see history and all the activity of your
 nodegrid.  And the nodegrid can use the presence system to figure out what nodes are present and in full 'working'
 mode.</p>

<h3>HipChat</h3>

<p> Within HipChat lets everyone see and talk about the changes.  It can get noisy though, so you need
 to separate the 'chatty' state changes from the 'critical' ones.  An example of 'critical' is when a machine realizes
 it is broken.  It is running the cron job, but something is wrong and it can tell that the 'work' is not completable.
 I call this being 'wedged'.  If a machine is 'wedged', it should tell people and then we can work on improving its
 DNA so it can unwedge itself in the future.  And then kill the machine.</p>

<h3>EC2</h3>

<p> By using EC2 tags you can leverage the EC2 dashboard.  I view 'tags' as read-only because the ADD should not get
 attached (be dependent) on EC2, but it is helpful for visibility.</p>

<h3>Examples</h3>

<p>The following show two machines initializing through Presence, HipChat, and EC2.  The only trigger for this was
killing the existing two instances: the AutoScalingGroup automatically replaced them.</p>

<h4>Launching viewed within EC2 Dashboard</h4>

<p>Nicely the 'add:' prefix makes all the properties that are most important appear on the left.  Some of the names
of concepts are intentionally alphabetically 'sorted' so they appear in the correct column.</p>

<p><img src="http://markfussell.emenar.com/images/add-7/add7_ec2_cv1b.png" /></p>

<p>Click here: <a target="add7_ec2_cv1b" href="http://markfussell.emenar.com/images/add-7/add7_ec2_cv1b.png" >add7_ec2_cv1b</a> to expand.</p>

<p>The ‘stacktype’ of ‘lad1’ in the capture is short for a stack of</p>

<ul>
<li>Load Balancer</li>
<li>Application</li>
<li>Database</li>
</ul>


<h4>Working viewed within HipChat</h4>

<p><img src="http://markfussell.emenar.com/images/add-7/add7_hipchat_cv1.png" /></p>

<h4>Working viewed within Presence / SourceTree</h4>

<p><img src="http://markfussell.emenar.com/images/add-7/add7_presence_cv1.png" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-6]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-6/"/>
    <updated>2015-10-02T01:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-6</id>
    <content type="html"><![CDATA[<p>This is the sixth installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>.  In the previous parts I described the big picture, Vagrant and EC2,
node initialization, the one minute configuration HeartBeat, and HipChat integration.</p>

<h2>'Part' Provisioning</h2>

<p>Each node plays a singular 'Part'.  A 'part' is a unique combination of roles (in the chef sense) that identifies
exactly how the node should be provisioned, usually globally, but at least for each stacktype.  A standard array
of parts would be the LAMP stack:</p>

<ul>
<li> Load Balancer (lb)</li>
<li> Application Server (app)</li>
<li> Database Server  (db)</li>
</ul>


<!-- more -->


<p>The most interesting thing about parts is hooking them together.  Load balancers need to know about application servers.
Application servers need to know where the databases are.  This feature I call 'presence'.  There are a lot
of fancy ways to solve 'presence'.  There could be 'presence' servers that servers register with.  Or 'presence' servers
that poll AWS registries.  Certain products keep their 'CI' (Configuration Item) information in databases: both SQL and
other kinds.</p>

<p>All of this is stupidly complex and treats the nodes as if they are idiots.  Pretty sure these nodes can be made about
as smart as a young student (say elementary school or even younger).  A young person is perfectly capable of putting
their name on a list.  And then listing some other interesting things about them.  A node can do the same.  So all
we need is a list.  The classic list for a computer?  A folder.  A folder containing files.  A file named after a node's
unique name.  And a file containing information about the node.  Voila.  No SPOF (can have two folders stored differently),
and no additional nodes doing something stupidly simple.</p>

<h2>Demo or Die!</h2>

<p>So we already have a 'HeartBeat' server, all we need are for it to write somewhere what it's state is.  That is
quite simple:</p>

<h3>updatePresence.sh</h3>

<p>This simply writes information in ~/nodeinfo into a JSON file.  To make that JSON file a little nicer we python
format it.  What gets written into the JSON file?  Basically anything we want!  When?  Every minute!  Ta Da...
the trick is done.</p>

<p>```bash</p>

<h1>!/bin/bash</h1>

<p>...</p>

<h1>====================================================</h1>

<h1>=== Generate file</h1>

<h1>====================================================</h1>

<p>export INSTANCE_ID="<code>cat /root/nodeinfo/instance-id.txt</code>"
echo $INSTANCE_ID</p>

<p>cat &lt;<EOS >>$PRESENCE_TEMP
{
"filetype":"nodepresence",
"value": {</p>

<pre><code>"a":"a"
</code></pre>

<p>EOS</p>

<p>FILES=( "initgitrepo" "instance-id" "nodepart" "stacktype" )</p>

<p>for i in "${FILES[@]}"
do</p>

<pre><code>cat &lt;&lt;EOS &gt;&gt;$PRESENCE_TEMP
,
"$i":"`cat /root/nodeinfo/${i}.txt`"
</code></pre>

<p>EOS
done</p>

<p>cat &lt;<EOS >>$PRESENCE_TEMP
}
}
EOS</p>

<p>cat $PRESENCE_TEMP | python -mjson.tool > $PRESENCE_TEMP2
cat $PRESENCE_TEMP2</p>

<h1>| bash ${COMMON}/send_hipchat.sh -c green</h1>

<h1>====================================================</h1>

<h1>=== Switch to the presence repository and copy file</h1>

<h1>====================================================</h1>

<p>pushd $REPO_ROOT</p>

<p>if [ ! -d "$PRESENCE_REPO" ]; then
  echo "git clone git@github.com:shaklee/${PRESENCE_REPO}.git"
  git clone git@github.com:shaklee/${PRESENCE_REPO}.git
fi</p>

<p>if [ -d "$PRESENCE_REPO" ]; then
  cd $PRESENCE_REPO
  git pull</p>

<p>  #Now splat it out to all the proper places
  TARGETS=( "it/presence/all" )
  for i in "${TARGETS[@]}"
  do</p>

<pre><code>mkdir -p $i
cp -f $PRESENCE_TEMP2 $i/${INSTANCE_ID}.json
</code></pre>

<p>  done</p>

<p>  git add .
  git commit -m "Updated by $INSTANCE_ID"; git push
  #Now need to see if this works... but the following is an easy trick in the small
  git pull; git push; git pull; git push</p>

<p>  #Repeat until push succeeds
else
  echo "Not working!"
fi</p>

<p>popd</p>

<p>```</p>

<p>And with the above script we get this simple and beautiful view:</p>

<p><img src="http://markfussell.emenar.com/images/add-6/add6_sourceTree_cv1.png" /></p>

<h2>Collisions</h2>

<p>OK, updating a GitHub repository every minute is not the smartest thing to do at scale... but: if the file
is the same, Git won't do anything.  And if we want, we can always turn down the noise.</p>

<h2>Death</h2>

<p>A machine can die or be killed, so presence information could be out of date.  The solution is just to
broadcast a 'HeartBeat' in either the presence repository or another repository.  Or to make sure to
check if the machine is actually responsive (e.g. HAProxy will interact with the machine to make sure it
is actually alive) vs. being present.  This final interaction is pretty critical (no Zombies in my data center),
so that is the best way to figure out who is alive and alert vs. just being present.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-5]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-5/"/>
    <updated>2015-10-02T01:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-5</id>
    <content type="html"><![CDATA[<p>This is the fifth installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>.  In the previous parts I described the big picture, Vagrant and EC2,
node initialization, and the one minute configuration HeartBeat.</p>

<h2>HipChat</h2>

<p>The fourth ingredient to the ADD is HipChat.  HipChat is meant to help people communicate and see the state of the world.
It is an excellent communication program with a plethora of integrations.  But for ADD the critical capability is
to mix notifications of machines with communication between people.  You can also have it be used to drive the ADD
system (kind of like a command line) but that isn't very important since it would just make changes in GitHub
which can be made in lots of different ways.</p>

<p>The demo of this is very simple:</p>

<p><img src="http://markfussell.emenar.com/images/add-5/add5_hipchat_cv1.png" /></p>

<!-- more -->


<p>The GitHub part is an integration that works out of the box, and the AddBot1 notification is a minor addition
to the working script:</p>

<p><code>bash
echo "Doing the work for ${GIT_VERSION}"
echo "Doing the work for ${GIT_VERSION}" | bash ${COMMON}/send_hipchat.sh
sleep 121
echo "Done the work for ${GIT_VERSION}"
echo "Done the work for ${GIT_VERSION}" | bash ${COMMON}/send_hipchat.sh -c green
</code></p>

<p>So now we have 'Action in GitHub' and visibility to all the activities of the machines that reacted to
that action.  Finally we can have human augmentation like "@Team New version is up on the development server"
to transform something technical to something more meaningful.  With proper use of colors, the state
of the world is fairly well communicated at a glance (e.g. Reds, Yellows, Greens, and Blues).</p>
]]></content>
  </entry>
  
</feed>
