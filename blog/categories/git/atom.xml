<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Git | Polyglot]]></title>
  <link href="http://markfussell.emenar.com/blog/categories/git/atom.xml" rel="self"/>
  <link href="http://markfussell.emenar.com/"/>
  <updated>2015-10-02T15:49:17-07:00</updated>
  <id>http://markfussell.emenar.com/</id>
  <author>
    <name><![CDATA[Mark Fussell]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-6]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-6/"/>
    <updated>2015-10-02T01:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-6</id>
    <content type="html"><![CDATA[<p>This is the sixth installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>.  In the previous parts I described the big picture, Vagrant and EC2,
node initialization, the one minute configuration HeartBeat, and HipChat integration.</p>

<h2>'Part' Provisioning</h2>

<p>Each node plays a singular 'Part'.  A 'part' is a unique combination of roles (in the chef sense) that identifies
exactly how the node should be provisioned, usually globally, but at least for each stacktype.  A standard array
of parts would be the LAMP stack:</p>

<ul>
<li> Load Balancer (lb)</li>
<li> Application Server (app)</li>
<li> Database Server  (db)</li>
</ul>


<!-- more -->


<p>The most interesting thing about parts is hooking them together.  Load balancers need to know about application servers.
Application servers need to know where the databases are.  This feature I call 'presence'.  There are a lot
of fancy ways to solve 'presence'.  There could be 'presence' servers that servers register with.  Or 'presence' servers
that poll AWS registries.  Certain products keep their 'CI' (Configuration Item) information in databases: both SQL and
other kinds.</p>

<p>All of this is stupidly complex and treats the nodes as if they are idiots.  Pretty sure these nodes can be made about
as smart as a young student (say elementary school or even younger).  A young person is perfectly capable of putting
their name on a list.  And then listing some other interesting things about them.  A node can do the same.  So all
we need is a list.  The classic list for a computer?  A folder.  A folder containing files.  A file named after a node's
unique name.  And a file containing information about the node.  Voila.  No SPOF (can have two folders stored differently),
and no additional nodes doing something stupidly simple.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-5]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-5/"/>
    <updated>2015-10-02T01:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-5</id>
    <content type="html"><![CDATA[<p>This is the fifth installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>.  In the previous parts I described the big picture, Vagrant and EC2,
node initialization, and the one minute configuration HeartBeat.</p>

<h2>HipChat</h2>

<p>The fourth ingredient to the ADD is HipChat.  HipChat is meant to help people communicate and see the state of the world.
It is an excellent communication program with a plethora of integrations.  But for ADD the critical capability is
to mix notifications of machines with communication between people.  You can also have it be used to drive the ADD
system (kind of like a command line) but that isn't very important since it would just make changes in GitHub
which can be made in lots of different ways.</p>

<p>The demo of this is very simple:</p>

<p><img src="http://markfussell.emenar.com/images/add-5/add5_hipchat_cv1.png" /></p>

<!-- more -->


<p>The GitHub part is an integration that works out of the box, and the AddBot1 notification is a minor addition
to the working script:</p>

<p><code>bash
echo "Doing the work for ${GIT_VERSION}"
echo "Doing the work for ${GIT_VERSION}" | bash ${COMMON}/send_hipchat.sh
sleep 121
echo "Done the work for ${GIT_VERSION}"
echo "Done the work for ${GIT_VERSION}" | bash ${COMMON}/send_hipchat.sh -c green
</code></p>

<p>So now we have 'Action in GitHub' and visibility to all the activities of the machines that reacted to
that action.  Finally we can have human augmentation like "@Team New version is up on the development server"
to transform something technical to something more meaningful.  With proper use of colors, the state
of the world is fairly well communicated at a glance (e.g. Reds, Yellows, Greens, and Blues).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-4]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-4/"/>
    <updated>2015-10-01T03:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-4</id>
    <content type="html"><![CDATA[<p>This is the fourth installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>.  In the previous parts I described the big picture and the Vagrant and EC2 bootstrap.</p>

<h2>Node initialization</h2>

<p>The previous parts described getting Vagrant and EC2 to have an operational node.  For Vagrant it leverages 'host' virtual
disk access to configure and bootstrap itself.  For EC2, it leverages CloudFormation to configure and bootstrap itself.
In both cases the very last thing the node does in the bootstrap is:</p>

<p><code>bash
cd /root/gitrepo/`cat /root/nodeinfo/initgitrepo.txt`
include () { if [[ -f \"$1\" ]]; then source \"$1\"; else echo \"Skipped missing: $1\"; fi }
include it/nodeinit/common/init.sh
</code></p>

<!-- more -->


<p>It is an 'include/source' to make sure it is at the same level as the initial bootstrap script.  For EC2 this affects
logging, so continual sourcing is preferred.  In other cases, the 'source' enables sub-scripts to set values for subsequent
scripts where subshells are more isolated.</p>

<h3>init.sh</h3>

<p>The init script first figures out where it is and sets up some important paths.</p>

<p>```bash</p>

<h1>!/bin/bash</h1>

<h1>===================================================</h1>

<h1>=== Want DIR to be root of the 'nodeinit' directory</h1>

<h1>===================================================</h1>

<p>export DIR="$( cd -P "$( dirname "${BASH_SOURCE[0]}" )" &amp;&amp; pwd )/../"
export RESOURCE=${DIR}/resource
export COMMON=${DIR}/common
```</p>

<h3>cron_1m.sh</h3>

<p>It then gets some AWS resources, sets up a shared 'cron', and so on.  I like a single 'cron' job running every minute
so it is easy to understand what is going on.  This is the 'heartbeat' of the server configuration infrastructure: a
server can want to change any 'minute'.  They look every minute for something that makes them want to change and
then they launch an activity.  The look need to be fast: take about a second or two per 'look' and not cause much load.
But the 'change' does not have to be fast: it could take minutes to reconfigure based on the change.  So while
changing, the 'looking' is disabled.  For example, deploying a new WAR can take a while.  The server stops looking
for new WARs when deploying a WAR.  Then starts looking again when it is back online.</p>

<p>At scale (say 100 servers) with servers all on NTP this one-minute rhythm can cause resource rushing.  To
counter that we need to 'jitter' the servers so they work on a different
second of the minute, or even as much as minutes later at super-scale (1000 servers).
That is done within the cron_1m.sh script after the look has established something needs to be done.</p>

<p>```bash
mkdir -p /root/bin
cp ${RESOURCE}/cron_1m.sh /root/bin/cron_1m.sh
chmod +x /root/bin/cron_1m.sh</p>

<p>cat &lt;<EOS > /var/spool/cron/root
MAILTO=""</p>

<ul>
<li><ul>
<li><ul>
<li><ul>
<li><ul>
<li>/root/bin/cron_1m.sh
EOS
```</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>


<h3>More specific initialization</h3>

<p>The above activities are done for any node.  They all need to have heartbeats and some other common resources.
But beyond that, it depends on the type of node and the type of stack what should be put on a particular node.
This is done by simple 'includes' with the 'nodeinfo' that came from the configuration.</p>

<p>```bash</p>

<p>include ${DIR}part/<code>cat /root/nodeinfo/nodepart.txt</code>/init.sh
include ${DIR}stacktype/<code>cat /root/nodeinfo/stacktype.txt</code>/init.sh
include ${DIR}stacktype/<code>cat /root/nodeinfo/stacktype.txt</code>/part/<code>cat /root/nodeinfo/nodepart.txt</code>/init.sh</p>

<p>```</p>

<p>You can see the layout in the directory picture.</p>

<p><img src="http://markfussell.emenar.com/images/add-2/vag1_20151001b.png" /></p>

<p>As of that picture, no 'part' or 'stacktype' exists.  So a machine that is brought up is simply a heart-beating server,
but a heart-beating server that can mutate on command every minute.</p>

<h2>What are nodes doing every minute?</h2>

<p>The next cool feature of ADD is that nodes do work based on the state of git repositories.  For any given repository, they
look for a 'work.sh' file within a 'nodework' directory for either all types of nodes (i.e. common again), or the specific
type of node they are.  So just like the other 'include' we get:</p>

<p>```bash</p>

<pre><code>        bash_ifexist bin/nodework/common/work.sh
        bash_ifexist bin/nodework/part/`cat /root/nodeinfo/nodepart.txt`/work.sh
        bash_ifexist bin/nodework/stacktype/`cat /root/nodeinfo/stacktype.txt`/work.sh
        bash_ifexist bin/nodework/stacktype/`cat /root/nodeinfo/stacktype.txt`/part/`cat /root/nodeinfo/nodepart.txt`/work.sh
</code></pre>

<p>```</p>

<p>where the only change is these are not 'sourced' but executed within a sub-shell since they could do weird things to each other, and
also this enables them not to block each other (if desired).</p>

<p>All of these 'work' scripts should quickly determine if anything has changed and then release themselves.  While 'work' is going on,
 the main cron script is locked out.</p>

<p>```bash</p>

<pre><code>     if [[ -e ${CURRENT_ACTION_FILE} ]]; then
         : #Don't do anything until the current action completes
</code></pre>

<p>```</p>

<h3>work.sh</h3>

<p>The main purpose of 'work.sh' is to detect changes.  Any actual work will be in 'work_ActualWork.sh'.  In reverse, the ActualWork is
simply:</p>

<p><code>bash
echo "Doing the work for ${GIT_VERSION}"
</code></p>

<p>So a one-liner appears in the log for the cron job just to prove the 'ActualWork' was done.</p>

<p>But 'work.sh' has to do a few things (very quickly) to detect if there are changes of relevance.  It stores files in the 'repo/.temp/add'
directory that keeps track of state.  The example 'work.sh' will detect changes to the repository based on a watched 'path'.  This
allows multiple things to use the same git repository but be looking at different parts.  By default they look at the root, but it
can be changed.  No matter what 'path' is watched, the version of the 'work' is always the version of the git repository... not the path itself.
In total, there are four 'outer' states possible:</p>

<ul>
<li>The version of the work previously done is identical to the version of git now</li>
<li>The version of the work previously done is different from the version of git now, but the version of the watched path is the same</li>
<li>The version of the work previously done is different from the version of git now, and the watched path has changed</li>
<li>There is no work previously done (the first run of the work)</li>
</ul>


<p>Of the above, only the last two should trigger work.  You can branch differently based on the first run or subsequent runs, but
generally it is best to be 'idempotent' with the work: you change the state of the server to a new state without caring
what the previous state is/was.</p>

<p>The 'inner' state issue is the server could already be doing 'ActualWork', so you have to wait until that is done.</p>

<p>The core of the work.sh script is</p>

<p>```bash
export WORK_VERSION=$ADD_TEMP/work.sh_VERSION
export WORK_DOING_VERSION=$ADD_TEMP/work.sh_DOING_VERSION
export WORK_WATCH_PATH=$ADD_TEMP/work.sh_WATCH_PATH.txt</p>

<p>export WORK_WATCH_VALUE=<code>cat $WORK_WATCH_PATH</code>
export PREV_WORK_VERSION=<code>cat $WORK_VERSION</code></p>

<h1>====================================================</h1>

<h1>=== Now do comparison</h1>

<h1>====================================================</h1>

<p>export GIT_VERSION=<code>git rev-parse HEAD</code></p>

<p>export DETECT_GIT_CHANGE=<code>git log --pretty=oneline ${PREV_WORK_VERSION}..  -- ${WORK_WATCH_VALUE} | awk '{print $1}'</code></p>

<p>echo "Compared ${PREV_WORK_VERSION} to ${GIT_VERSION} for ${WORK_WATCH_VALUE} and got ${DETECT_GIT_CHANGE}"</p>

<p>mkdir -p ${ADD_TEMP}</p>

<p>if [[ -n "${DETECT_GIT_CHANGE}" ]] ;
then</p>

<pre><code>echo "Detected Change in Git Version!";

if [[ -e ${WORK_DOING_VERSION} ]] ;
then
   echo "Already doing `cat ${WORK_DOING_VERSION}`"
else
   echo $GIT_VERSION &gt; ${WORK_DOING_VERSION}
   source ${COMMON}/work_ActualWork.sh

   #Update the state.  This also does a clean startup on first run

   echo $GIT_VERSION &gt; ${WORK_VERSION}
   echo $WORK_WATCH_VALUE &gt; ${WORK_WATCH_PATH}

   rm -fr ${WORK_DOING_VERSION}
fi
</code></pre>

<p>else</p>

<pre><code>echo "No change, move along";
</code></pre>

<p>fi</p>

<p>```</p>

<h3>Speed!</h3>

<p>How fast does this detection take? Basically one second for it to figure out which of the variations it is in, plus the time of the 'git pull'.  With a
'small' server and a small change, this a single second and basically no load:</p>

<h4>Difference detected: start the work</h4>

<p>```bash
cron_1m.sh: Start  20151002-013401
~/gitrepo/repo2_petulant-cyril ~</p>

<p>==> /root/log/cron_1m.sh_error.txt &lt;==
From github.com:shaklee/repo2_petulant-cyril
   57f20ff..6f85e76  master     -> origin/master</p>

<p>==> /root/log/cron_1m.sh_log.txt &lt;==
Updating 57f20ff..6f85e76
Fast-forward
 bin/nodework/common/work.sh | 2 ++
 1 file changed, 2 insertions(+)
Compared 57f20ff734c8836fa34f938bcc540a89bad9215c to 6f85e76a507ce599f42762ad7bf4ae639884ae12 for  and got 6f85e76a507ce599f42762ad7bf4ae639884ae12
Detected Change in Git Version!
Starting ActualWork at 20151002-013402
Doing the work for 6f85e76a507ce599f42762ad7bf4ae639884ae12
```</p>

<h4>Difference detected (but already doing something)</h4>

<p><code>bash
cron_1m.sh: Start  20151002-012301
~/gitrepo/repo2_petulant-cyril ~
Already up-to-date.
Compared 7916053bb9f8bc3d952588a87a48da96dda7abe6 to 57f20ff734c8836fa34f938bcc540a89bad9215c for  and got 57f20ff734c8836fa34f938bcc540a89bad9215c
Detected Change in Git Version!
Already doing 57f20ff734c8836fa34f938bcc540a89bad9215c
Skipped missing: bin/nodework/part/controlnode/work.sh
Skipped missing: bin/nodework/stacktype/ControlServer1/work.sh
Skipped missing: bin/nodework/stacktype/ControlServer1/part/controlnode/work.sh
~
cron_1m.sh: Finish 20151002-012302
</code></p>

<h4>No Difference</h4>

<p><code>bash
cron_1m.sh: Start  20151002-012801
~/gitrepo/repo2_petulant-cyril ~
Already up-to-date.
Compared 57f20ff734c8836fa34f938bcc540a89bad9215c to 57f20ff734c8836fa34f938bcc540a89bad9215c for  and got
No change, move along
Skipped missing: bin/nodework/part/controlnode/work.sh
Skipped missing: bin/nodework/stacktype/ControlServer1/work.sh
Skipped missing: bin/nodework/stacktype/ControlServer1/part/controlnode/work.sh
~
cron_1m.sh: Finish 20151002-012802
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-3]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-3/"/>
    <updated>2015-10-01T02:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-3</id>
    <content type="html"><![CDATA[<p>This is the third installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>.  In the previous parts I described the big picture and the
first part of the Vagrant bootstrap.</p>

<h2>EC2</h2>

<p>The Vagrant bootstrap occurred through 'bash' files that shaped (put shape information into files) and
the 'init' itself to get access to the repo (repo2) that contains the true configuration.  For EC2
the same thing happens within a CloudFormation template.  The code of the 'init' is almost identical, but because
it is in a JSON file there is a lot of noise as the string gets concatenated together.</p>

<!-- more -->


<h3>Shaping</h3>

<p>```json</p>

<pre><code>"TemplateConstant" : {
  "stacktype" : { "value" : "ControlServer1" },
  "initgitrepo" : { "value" : "repo2_petulant-cyril" },
  "nodepart" : { "value" : "controlnode" }
},
</code></pre>

<p>```</p>

<p>```json</p>

<pre><code>        "files" : {
          "/root/nodeinfo/stacktype.txt" : {
            "content" : { "Fn::Join" : ["", [
              { "Fn::FindInMap" : [ "TemplateConstant", "stacktype", "value" ] },
              ""
            ]]},
            "mode"  : "000700",
            "owner" : "root",
            "group" : "root"
          },
          "/root/nodeinfo/initgitrepo.txt" : {
            "content" : { "Fn::Join" : ["", [
              { "Fn::FindInMap" : [ "TemplateConstant", "initgitrepo", "value" ] },
              ""
            ]]},
            "mode"  : "000700",
            "owner" : "root",
            "group" : "root"
          },
</code></pre>

<p>```</p>

<h3>Init</h3>

<p>```json</p>

<pre><code>    "UserData"       : { "Fn::Base64" : { "Fn::Join" : ["", [
      "#!/bin/bash -v\n",
      "yum update -y aws-cfn-bootstrap\n",

      "# Helper function\n",
      "function error_exit\n",
      "{\n",
      "  /opt/aws/bin/cfn-signal -e 1 -r \"$1\" '", { "Ref" : "WaitHandle" }, "'\n",
      "  exit 1\n",
      "}\n",

      "# Install LAMP packages\n",
      "/opt/aws/bin/cfn-init -s ", { "Ref" : "AWS::StackName" }, " -r PrimaryLaunchConfig ",
      "    --access-key ",  { "Ref" : "HostKeys" },
      "    --secret-key ", {"Fn::GetAtt": ["HostKeys", "SecretAccessKey"]},
      "    --region ", { "Ref" : "AWS::Region" }, " || error_exit 'Failed to run cfn-init'\n",

      "yum -y install git \n",

      "echo 'Fetch s3cmd to get credentials' \n",
      "mkdir /root/download/ \n",
      "pushd /root/download/ \n",
      "git clone git://github.com/s3tools/s3cmd.git \n",
</code></pre>

<p>```</p>

<h3>Launching and Clusters</h3>

<p>A CloudFormation can provision a single server, but it is used more for clusters.  Instead of creating a server, we create
a server definition and then say how many servers we want.  The 'PrimaryServerGroup' defines this:</p>

<p>```json</p>

<pre><code>"PrimaryServerGroup" : {
  "Type" : "AWS::AutoScaling::AutoScalingGroup",
  "Properties" : {
    "Tags": [
      { "Key": "add:stacktype", "Value": { "Fn::FindInMap" : [ "TemplateConstant", "stacktype", "value" ] }, "PropagateAtLaunch" : "true" },
      { "Key": "add:nodepart", "Value": { "Fn::FindInMap" : [ "TemplateConstant", "nodepart", "value" ] }, "PropagateAtLaunch" : "true" }
    ],
    "AvailabilityZones" : { "Fn::GetAZs" : "" },
    "LaunchConfigurationName" : { "Ref" : "PrimaryLaunchConfig" },
    "MinSize" : "1",
    "MaxSize" : "1",
    "DesiredCapacity" : "1"
  }
},
</code></pre>

<p>```</p>

<p>Note that it ends with '1', '1', '1' meaning this is just a single server.  But those numbers can be changed at any time.</p>

<p>Given the DesiredCapacity is '1', if you ever kill a server, a new one will be spun up.</p>

<h3>EC2 Keypairs</h3>

<p>Another difference of the EC2 model is that EC2 holds onto the keypair that is used for logging into it.  So that information
doesn't need to be exposed.  And further, the EC2 version creates a special 'IAM' agent for the machine.</p>

<h3>EC2 Dashboard</h3>

<p>The dashboard shows some standard EC2 properties along with the add:stacktype and add:nodepart.  The nodepart shows the
kind of node it is (say a load-balancer vs. a game server) and is used within the bootstrap to put the right software
onto the machine.  The 'nodepart' and the 'stacktype' are the core DNA switches of a server.  Later we will add in
'federation' which primarily configures the size of the node (e.g. no 'small' in production)</p>

<p><img src="http://markfussell.emenar.com/images/add-3/add3_ec2_cv1.png" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Advanced Development and Delivery (ADD) [Part-2]]]></title>
    <link href="http://markfussell.emenar.com/blog/add-2/"/>
    <updated>2015-10-01T01:00:00-07:00</updated>
    <id>http://markfussell.emenar.com/blog/add-2</id>
    <content type="html"><![CDATA[<p>This is the second installment of describing a radically more productive development and delivery environment.</p>

<p>The first part is here: <a href="/blog/add-1/">Intro</a>, but to summarize: since the 1970s, the most productive way
to develop and deliver software was present in Smalltalk, Lisp, and other languages (Mesa/Cedar at Xerox)
by using a very simple and powerful model.  You take a computer with a fully running environment, you tweak it,
and then you clone that.  This way you: (a) minimize what could go wrong, and (b) maximize what will continue to work.
It is very tangible and very instructive (you have full source for everything that is running).  You tweak other
people's masterpieces until they do what you want, and you learn from their masterpieces to create your own.</p>

<h2>ADD: How Better?</h2>

<p>As described before, ADD has four ingredients:</p>

<ul>
<li>Amazon EC2 <a href="http://aws.amazon.com/">http://aws.amazon.com/</a></li>
<li>Amazon S3  <a href="http://aws.amazon.com/">http://aws.amazon.com/</a></li>
<li>GitHub.com <a href="http://github.com/">http://github.com/</a></li>
<li>HipChat    <a href="http://HipChat.com/">http://HipChat.com/</a></li>
</ul>


<p>And these are hooked together to enable 'Changers', 'Watchders', and 'Machines' to be super-productive.  How
is the ADD <em>more productive</em> than the tweak and clone model?  It is because it solves the core problems
of the clone model:</p>

<ul>
<li>How do we clone to <em>different</em> environments?  Different hardware or configuration changes?</li>
<li>How do we reduce the amount of information we have to clone?</li>
<li>How do we reduce the time it takes to transport the clone?</li>
<li>How do we know what version of the clone is on any machine?</li>
<li>How do we create thousands of clones?</li>
<li>How do we know what is different about the different clones?</li>
<li>...</li>
</ul>


<!-- more -->


<p>The first three ingredients are the most powerful and enables a fantastic improvement to the tweak and clone model.
The last ingredient is mainly for people to be able to enjoy the ADD more easily.  It is like the 'salt-to-taste'
and how much you integrate HipChat in with the rest of the ecosystem is up to your team.  But the more it is integrated,
the more your team will know what is going on, the more easily your team will solve problems/issues, and the more easily
you will onboard new people.</p>

<h2>ADD: GitHub</h2>

<p>GitHub will become your primary resource for <em>everything</em> related to 'information'.  Human notes... go into GitHub
in Markdown format (like this blog).  Meeting notes.  GitHub.  Images to go with Meeting notes.  GitHub.  Your first,
most important, repository will be called 'repo1' and will be all the notes you want the team to see.  No more
arguing about the best Wiki, blogger, file store, etc.  The answer will always be the same.  It is in GitHub.  Because
<em>everything</em> is in GitHub.</p>

<p>Why?  Because it is simple.  It is accessible.  It is powerful.  Keeps history.  Takes almost no space.
It works offline (on an airplane).  And it works with multiple writers.  And if GitHub dies...
you have a complete copy of everything you need to bring up your own 'Git'.  Asking 'Why?' is silly.  First
move to GitHub for all of this, and then ask 'Why?' to everything else.</p>

<h2>ADD: S3 : Annexed Repositories</h2>

<p>Git and GitHub are not good with large binary assets.  They get stored in a notably raw way and just make the
repository huge for no benefit.  So don't store large binary assets in GitHub.  Instead store a reference to
the binary object up in S3.  Retrieve it as needed.  See <a href="/blog/git-about-everything-annex">Annex</a></p>

<h2>ADD: EC2 and Vagrant and GitHub and S3</h2>

<p>Using EC2 and Vagrant with a 'PushMe-PullYou' model (see <a href="/blog/git-about-everything-it-automation-2">PushMePullYou</a>)
solves a host of development, delivery, and operations issues.  The benefits are:</p>

<ul>
<li>Complete version control of machines – both operations and developers' machines (or part of a developers' machines)</li>
<li>A very simple model that enables machines to be provisioned rapidly and to change their state every minute (if needed)</li>
<li>An impressive fan-out of activity</li>
<li>An ability to work offline (say GitHub goes down) or to have complete redundancy (use both GitHub and BitBucket to avoid SPOF)</li>
<li>Inherently no SPOF</li>
<li>Dependent on <em>nothing</em> - Not EC2, Not Vagrant, not GitHub, not S3.  These may be gold standards, but they can all be swapped out</li>
</ul>


<p>This is where the ADD just shoots through the roof.  The ADD uses particular technology to show "How it is done" and
get you doing it right.  But it is not dependent on those technologies.  No Chef.  Unless you want it (and I recommend 'Solo').
No Linux unless you want it.  No Grails or Groovy unless you want it.  Use Google Compute if you want to.  Or even your
own Big Iron.  The ADD is a set of tools and methods that work well together and is most easily seen with the Gold Standard.
But it is beyond them: like a mathematical formula (the Golden Ratio) that can be present in many forms.</p>

<h2>Demo or Die!</h2>

<p>The core demo for this article will walk through bringing up a server on EC2 <em>and</em> Vagrant.  If you are not familiar
with EC2 and Vagrant, please read some of my other articles or meeting notes, or look to the web for resources.</p>

<h3>Vagrant</h3>

<p>The demo in Vagrant is slightly simpler than in EC2 because you are dealing with a machine at a time.  On EC2 you
should be thinking 'Clusters' of machines that work together in 'Federations', and the technology to do that is
more complicated and more EC2-centric.</p>

<p>In Vagrant, you have a 'Box' definition and then an actual virtual instance.  To provision an instance you have
to 'init' it, bring it up, and then configure it.  Except you don't.  As long as the instance knows how to
bootstrap itself.  Demo:</p>

<h4>Vagrantfile</h4>

<p>```ruby
  config.vm.provision "shell", inline: &lt;&lt;-SHELL</p>

<pre><code> su root
 source /vagrant/resource/centos7a_cf2_ControlServer1.sh
 source /vagrant/resource/centos7a_cred_bot1.sh
 source /vagrant/resource/centos7a_boot1.sh
</code></pre>

<p>  SHELL
```</p>

<p>As the last step of the Vagrantfile, the (linux) server does three things:</p>

<ul>
<li>It 'shapes' itself to be a ControlServer</li>
<li>It 'shapes' itself to be 'bot1' for it's credentials</li>
<li>It configures itself with a boot script</li>
</ul>


<p>By the end of the boot script it will be fully alive and running.  Watching for changes to repositories that
indicate it should do something.  You should never have to SSH into the machine... ever.  You can to look around
(like the Magic Schoolbus) but you should treat it like it is a living creature and <em>never</em> touch anything inside
it.  If you have to touch something, fix the 'DNA' (that boot script), kill the server, and launch a new one.</p>

<p>Given this is a Vagrant file on the developer's machine, they can certainly feel free to fiddle with things.  But that is
to learn to <em>understand</em> the server.  Some EC2 servers may even be for 'fiddling'.  But QA and production servers should
never be touched and should only be looked at if they are confusing people (who already understand the fiddling and Vagrant servers).</p>

<p>Both 'ControlServer1.sh' and 'cred_bot1.sh' simply put information into files under '/root/nodeinfo/'.  This is an
amazingly flexible approach that works very simply for Vagrant and EC2.</p>

<h4>ControlServer1.sh</h4>

<p>```bash</p>

<h1>!/bin/bash -v</h1>

<p>export stacktype="ControlServer1"
export initgitrepo="repo2_petulant-cyril"
export nodepart="controlnode"</p>

<p>mkdir /root/nodeinfo
echo $stacktype > /root/nodeinfo/stacktype.txt
echo $initgitrepo > /root/nodeinfo/initgitrepo.txt
echo $nodepart > /root/nodeinfo/nodepart.txt
```</p>

<h4>cred_bot1.sh</h4>

<p>The actual version of this would contain real credential information.  The actual version would be developer-specific and not in version control.</p>

<p>```bash</p>

<h1>!/bin/bash -v</h1>

<p>export access_key="access_key"
export secret_key="secret_key"
export keyname="keyname"</p>

<p>echo $access_key > /root/nodeinfo/access_key.txt
echo $secret_key > /root/nodeinfo/secret_key.txt
echo $keyname > /root/nodeinfo/keyname.txt</p>

<p>cat >> /root/.s3cfg &lt;&lt;EOS
[default]
access_key = $access_key
secret_key = $secret_key</p>

<p>EOS
```</p>

<h4>centos7a_boot1.sh</h4>

<p>This script mirrors most of how EC2 works: we need this machine to be able to checkout a repository from GitHub but
we only have Amazon credentials.  So we put the full credentials into S3 and check them out.  Then we can clone
the 'repo2' provisioning repo and go from there.</p>

<p>```bash</p>

<h1>!/bin/bash -v</h1>

<p>yum -y install git</p>

<p>echo 'Fetch s3cmd to get credentials'
mkdir /root/download/
pushd /root/download/
git clone git://github.com/s3tools/s3cmd.git
cd s3cmd
git checkout a91c40fcd14772fa48297e676c8c6efa1aabc3c0
python --version
python setup.py install
mkdir /root/bin/
mv /root/download/s3cmd /root/bin/s3cmd
popd</p>

<p>echo 'Retrieve SSH keys for Github'
pushd /root/.ssh
/root/bin/s3cmd/s3cmd --config /root/.s3cfg get s3://gapshaklee/it/key/shakbot1key2/<em>
chmod 600 id</em>
cd /root
ssh -v -o StrictHostKeyChecking=no -T git@github.com</p>

<p>mkdir /root/gitrepo
cd /root/gitrepo
git clone git@github.com:shaklee/<code>cat /root/nodeinfo/initgitrepo.txt</code>.git</p>

<p>cd /root/gitrepo/<code>cat /root/nodeinfo/initgitrepo.txt</code>
include () { if [[ -f \"$1\" ]]; then source \"$1\"; else echo \"Skipped missing: $1\"; fi }
include it/nodeinit/common/init.sh</p>

<h1>Zzzzz....</h1>

<p>```</p>

<h4>Directory Layout</h4>

<p>An image of the directory structure is below.  The little-meaning but organized name 'repo2' is augmented with a human suffix 'petulant-cyril'
to make it unique and memorable. 'repo2' is always the first operations repo and 'repo3' is always the first development repo.
The suffix is generated by GitHub or other name generators.</p>

<p>The layout of the directory contains a few things:</p>

<ul>
<li> A 'bin' that contains scripts that can be run within this repository.  The 'deflateAll.sh' script is important enough to be put in the root, but the rest are inside 'bin'.</li>
<li> All things other than the README and deflatAll should be in consistent subdirectories.  The 's3info' is for the annex.  And 'it' is for everything related to being it.  'src' and 'test' are meaningless at the root level and should not be checked in.</li>
<li> You can see the 'node' folders.  A 'node' is a virtual server (Chef and others terminology).  'nodeaws' is for aws related node configuration.  'nodeinit' is common.  'nodevag' is for vagrant.  'resource' contains resourceds in general if under 'it' and for something more specific if lower</li>
<li> folder names are never capitalized or pluralized to avoid inter-operating-system issues.  File names can be any format, but I use augmented CamelCase (with snakes) or snake_case depending on the situation.</li>
<li> You can see the annexed files in the '/it/resource' folder.  They are all '50 bytes'</li>
</ul>


<p><img src="http://markfussell.emenar.com/images/add-2/vag1_20151001b.png" /></p>
]]></content>
  </entry>
  
</feed>
